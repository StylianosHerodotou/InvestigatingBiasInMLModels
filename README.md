# Investigating Bias in Machine Learning Models
This repository holds the code needed to recreate the results of our experiments. 

# Description
In this paper we investigate whether the biases of the annotators can be found in the predictions of the ML models. Specifically, we want to explore whether these biases can be found in a variety of disciplines (Attractiveness, Trustworthiness, Age, Gender and Race ) examining both subjective (like attractiveness) and objective (like Gender) features. 

The dataset we will be using for our experiments is the Chicago Face Dataset(CFD) . The annotations of this dataset will be our baseline since they were recorded with annotators of various backgrounds. We will also be using our own annotations of the face images of CFD. Our annotators can be categorised in multiple groups by their education levels, gender, ethnicity, and age. 

We will not provide the dataset in this repository to respect the owners copyrights. 

