{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RegressionModelArchitectureCYENS_Trustworthyness.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQz6vRdU-t6W","executionInfo":{"status":"ok","timestamp":1625135509972,"user_tz":-60,"elapsed":19768,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"a7cff138-e3ad-4933-d0d7-1275a0280760"},"source":["from google.colab import drive\n","\n","drive.mount(\"/content/drive\", force_remount=True)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8KGBeFFfTngr","executionInfo":{"status":"ok","timestamp":1625135513475,"user_tz":-60,"elapsed":3508,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["#IMPORTS\n","from keras.utils.np_utils import to_categorical\n","# import keras\n","import pandas as pd\n","import numpy as np\n","import random\n","# import brewer2mpl\n","import sys\n","import warnings\n","from google.colab import files\n","import os\n","import tensorflow as tf\n","import keras_preprocessing\n","from keras_preprocessing import image\n","from keras_preprocessing.image import ImageDataGenerator\n","import cv2\n","import numpy as np\n","import os\n","import PIL\n","import PIL.Image\n","import pathlib\n","import tensorflow_datasets as tfds\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import regularizers\n","Heinitializer = tf.keras.initializers.HeNormal()\n","import glob\n","import imageio\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import PIL\n","from tensorflow.keras import layers\n","import time\n","\n","from IPython import display\n","\n","\n","\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","%matplotlib inline\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fHt5Y4reLL3T","executionInfo":{"status":"ok","timestamp":1625135524406,"user_tz":-60,"elapsed":10934,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"8da6b447-ec77-4348-cf03-e4640c36cdcb"},"source":["!pip install deepface"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting deepface\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/e8/f574905fe01622f5e08439fa2968b899f08e4259a62dfa22bb6072e64e67/deepface-0.0.62-py3-none-any.whl (60kB)\n","\r\u001b[K     |█████▌                          | 10kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 30kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 40kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.3MB/s \n","\u001b[?25hRequirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from deepface) (7.1.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from deepface) (1.19.5)\n","Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from deepface) (2.5.0)\n","Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from deepface) (1.1.4)\n","Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.7/dist-packages (from deepface) (4.41.1)\n","Collecting gdown>=3.10.1\n","  Downloading https://files.pythonhosted.org/packages/52/b9/d426f164f35bb50d512a77d6a7c5eb70b2bea3459dc10f73f130ba732810/gdown-3.13.0.tar.gz\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: opencv-python>=3.4.4 in /usr/local/lib/python3.7/dist-packages (from deepface) (4.1.2.30)\n","Collecting retina-face>=0.0.1\n","  Downloading https://files.pythonhosted.org/packages/e7/ce/62209db0f6a9e57120fd368a771eb1c23f916e4c34c45f2d13a00c561a63/retina_face-0.0.4-py3-none-any.whl\n","Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from deepface) (1.1.5)\n","Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from deepface) (2.4.3)\n","Collecting mtcnn>=0.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 10.1MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (3.7.4.3)\n","Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.34.1)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.15.0)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (3.1.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (0.36.2)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.12.1)\n","Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (2.5.0)\n","Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (2.5.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.1.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.1.2)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (0.12.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (3.3.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n","Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (2.5.0.dev2021032900)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (1.12)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (0.4.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.9.0->deepface) (3.12.4)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->deepface) (1.0.1)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->deepface) (7.1.2)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->deepface) (2.11.3)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->deepface) (1.1.0)\n","Requirement already satisfied: requests[socks]>=2.12.0 in /usr/local/lib/python3.7/dist-packages (from gdown>=3.10.1->deepface) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown>=3.10.1->deepface) (3.0.12)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->deepface) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->deepface) (2018.9)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->deepface) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.2.0->deepface) (1.4.1)\n","Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=1.9.0->deepface) (1.5.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.9.0->deepface) (3.3.4)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.9.0->deepface) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.9.0->deepface) (1.8.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.9.0->deepface) (0.4.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.9.0->deepface) (1.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=1.9.0->deepface) (57.0.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=1.1.2->deepface) (2.0.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown>=3.10.1->deepface) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown>=3.10.1->deepface) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown>=3.10.1->deepface) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown>=3.10.1->deepface) (2021.5.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown>=3.10.1->deepface) (1.7.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=1.9.0->deepface) (4.5.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=1.9.0->deepface) (1.3.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.9.0->deepface) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.9.0->deepface) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.9.0->deepface) (4.7.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=1.9.0->deepface) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=1.9.0->deepface) (3.1.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=1.9.0->deepface) (0.4.8)\n","Building wheels for collected packages: gdown\n","  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gdown: filename=gdown-3.13.0-cp37-none-any.whl size=9046 sha256=fe2ad63eee8b746b19dd42e5ba019d6f6e3ea98f94e502d9a28a738766c56c9d\n","  Stored in directory: /root/.cache/pip/wheels/ba/fa/c5/12813d7496f34652c43a471e11a780e769889d06e34735c32e\n","Successfully built gdown\n","Installing collected packages: gdown, retina-face, mtcnn, deepface\n","  Found existing installation: gdown 3.6.4\n","    Uninstalling gdown-3.6.4:\n","      Successfully uninstalled gdown-3.6.4\n","Successfully installed deepface-0.0.62 gdown-3.13.0 mtcnn-0.1.0 retina-face-0.0.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xMdOI8Ly3pQX","executionInfo":{"status":"ok","timestamp":1625135524407,"user_tz":-60,"elapsed":11,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["#IMPORTS\n","from keras.utils.np_utils import to_categorical\n","# import keras\n","import pandas as pd\n","import numpy as np\n","import random\n","# import brewer2mpl\n","import sys\n","import warnings\n","from google.colab import files\n","import os\n","import tensorflow as tf\n","import keras_preprocessing\n","from keras_preprocessing import image\n","from keras_preprocessing.image import ImageDataGenerator\n","import cv2\n","import numpy as np\n","import os\n","import PIL\n","import PIL.Image\n","import pathlib\n","import tensorflow_datasets as tfds\n","import matplotlib.pyplot as plt\n","\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","%matplotlib inline\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"1lngeadf3pQX","executionInfo":{"status":"ok","timestamp":1625135524407,"user_tz":-60,"elapsed":10,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["import matplotlib.pyplot as plt\n","def plot_accuracy(history):\n","  acc = history.history['accuracy']\n","  val_acc = history.history['val_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  epochs = range(len(acc))\n","\n","  plt.plot(epochs, acc, 'r', label='Training accuracy')\n","  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","  plt.title('Training and validation accuracy')\n","  plt.legend(loc=0)\n","  plt.figure()\n","\n","  plt.show()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"VrsoxmKJmhg8","executionInfo":{"status":"ok","timestamp":1625135524408,"user_tz":-60,"elapsed":10,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["import os\n","from pathlib import Path\n","import gdown\n","from functools import partial\n","\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.layers import add\n","from tensorflow.keras import backend as K\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Input, concatenate\n","from tensorflow.keras.layers import Dense, Activation, Lambda, Flatten, BatchNormalization\n","from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n","from tensorflow.keras.models import load_model\n","tf_version = int(tf.__version__.split(\".\")[0])\n","from tensorflow.python.keras import backend\n","from tensorflow.python.keras.engine import training\n","from tensorflow.python.keras.utils import data_utils\n","from tensorflow.python.keras.utils import layer_utils\n","from tensorflow.python.lib.io import file_io\n","import tensorflow\n","import zipfile\n","from tensorflow.keras.layers import Convolution2D, LocallyConnected2D, Add, Dropout\n","from tensorflow.keras.layers import Concatenate\n","\n","\n","if tf_version == 1:\n","\tfrom keras.models import Model, Sequential\n","\tfrom keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n","else:\n","\tfrom tensorflow import keras\n","\tfrom tensorflow.keras.models import Model, Sequential\n","\tfrom tensorflow.keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n","\t\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"VwSDlPrmXXZe","executionInfo":{"status":"ok","timestamp":1625135524865,"user_tz":-60,"elapsed":466,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["def readEmbedings(listOfDesiredAugmentation, listOfDesiredModelNames, base_directory=\"\"):\n","  listOfAugEmbedings=list()\n","  listOfAugLabels=list()\n","  \n","  for directory in listOfDesiredAugmentation:\n","    listOfAllModelEmbedings=list()\n","\n","    for isFirst, modelName in enumerate(listOfDesiredModelNames):\n","      filename= base_directory+directory+\"/\"+ modelName+\".txt\"\n","      firstLineInDoc=True\n","      with open( filename, \"r\") as a_file:\n","        currentEmbedings= None\n","        currentlabels= None\n","        for outterIndex, line in enumerate(a_file):\n","          if (firstLineInDoc==True):\n","            firstline= line.split(\",\")\n","            numberOfEmbedings=int(firstline[0])\n","            embedingsSize    =int(firstline[1])\n","            currentEmbedings= np.empty((numberOfEmbedings,embedingsSize) )\n","            currentlabels= np.empty((numberOfEmbedings) )\n","            firstLineInDoc=False\n","            continue\n","\n","          stripped_line = line.strip()\n","          line = stripped_line.split(\"[\")[1]\n","          parts=line.split(\"]\")\n","          numbers=parts[0]\n","          numbers=numbers.split(\",\")\n","          numbers = numbers[:-1]\n","          for index, number in enumerate(numbers):\n","              numbers[index]=float(number)\n","          currentEmbedings[outterIndex-1]=numbers\n","          if(isFirst==0):\n","            label=float(parts[1])\n","            currentlabels[outterIndex-1]=label\n","        if(isFirst==0):\n","          listOfAugLabels.append(currentlabels)\n","        listOfAllModelEmbedings.append(currentEmbedings)\n","\n","    AugEmbedings= np.concatenate(listOfAllModelEmbedings, axis=1)\n","    listOfAugEmbedings.append(AugEmbedings)\n","\n","  AugEmbedings= np.concatenate(listOfAugEmbedings, axis=0)\n","  AugLabels= np.concatenate(listOfAugLabels, axis=0)\n","\n","  return AugEmbedings,AugLabels"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"n96UcMQCqtC3","executionInfo":{"status":"ok","timestamp":1625135524865,"user_tz":-60,"elapsed":5,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["# !ls"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F9fmNla2fZzT","outputId":"29a02a07-4365-4501-9255-67087093f788"},"source":["# %cd /content/drive/MyDrive/AttracivenessRegression/FaceEncodings"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/AttracivenessRegression/FaceEncodings\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1FArnPoI4j8n","outputId":"d136fbf6-587f-49d1-f911-975725afb84f"},"source":["import itertools    \n","listOfAllModelNames=[\"facenet\",\"vggface\",\"openface\", \"deepface\",\"deepID\", \"arcFace\"] \n","listOfAlCombinations= list()\n","for length in range(1, len(listOfAllModelNames)+1):\n","  temp= list(itertools.combinations(listOfAllModelNames, length))\n","  print(len(temp))\n","  listOfAlCombinations.extend(temp)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["6\n","15\n","20\n","15\n","6\n","1\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["63"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TR1QOBLozwSK","outputId":"9fc46c57-3ceb-4f2b-fec4-d601095d3a10"},"source":["len(listOfAlCombinations)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["63"]},"metadata":{"tags":[]},"execution_count":103}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9AcEWJTT97M","outputId":"d3e00d53-ff54-4330-b00e-c159be956d36"},"source":["listOfAlCombinations\n","len(listOfAlCombinations)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["63"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xb2tsMs6mdTL","outputId":"a424061c-f70d-4259-9fcb-f0f64304b6e7"},"source":["len(doneAlready)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["41"]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LdRfjXaqklw1","outputId":"399a0c02-1e0a-437c-cc57-921908e08f28"},"source":["listOfSomeCombinations= set(listOfAlCombinations) - set(doneAlready)\n","len(listOfSomeCombinations)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["22"]},"metadata":{"tags":[]},"execution_count":106}]},{"cell_type":"code","metadata":{"id":"0lFZFQkfkrDa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJp_jvgH9-QG","outputId":"800a8dbd-5dc0-4497-f180-59d0eb7333a1"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test  Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m-ofaKMx-wZ1"},"source":["from tensorflow.keras import regularizers\n","Heinitializer = tf.keras.initializers.HeNormal()\n","from tensorflow.keras.layers.experimental import preprocessing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"LaiJEXtO5bTe","outputId":"1f9a938b-2a81-435e-8695-c87899cc295d"},"source":["dic={}\n","listOfTrainingAugmentation=['HF', 'None', 'RO', 'HF-RC', 'None-RC', 'None-TR', 'HF-TR']\n","listOfTestAugmentation=['None']\n","numberOfRepeated=5\n","for combination in listOfSomeCombinations:\n","  training_embedings, training_labels= readEmbedings(listOfDesiredAugmentation=listOfTrainingAugmentation,\n","                                 listOfDesiredModelNames=combination,base_directory=\"Training/\")\n","  testing_embedings, testing_labels= readEmbedings(listOfDesiredAugmentation=listOfTestAugmentation,\n","                                 listOfDesiredModelNames=combination,base_directory=\"Test/\")\n","  normalizer = preprocessing.Normalization(axis=-1)\n","  normalizer.adapt(training_embedings)\n","\n","  dic[combination]={\"ModelName\": combination , \"ValLosses\": list()}\n","  \n","  for i in range(numberOfRepeated):\n","    dropout_posibility=0.06\n","    L2_lambda=0.0004\n","    init_learning_rate=0.001\n","    newModel = tf.keras.models.Sequential([                 \n","        # pre_trained_model, \n","        # tf.keras.layers.GlobalAveragePooling2D(),\n","        tf.keras.Input(shape=(training_embedings[0].shape[0],)),\n","        normalizer,\n","        tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","        tf.keras.layers.Dropout(dropout_posibility),\n","        tf.keras.layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","        tf.keras.layers.Dropout(dropout_posibility),\n","        tf.keras.layers.Dense(4, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","        tf.keras.layers.Dropout(dropout_posibility),\n","        tf.keras.layers.Dense(1,kernel_initializer=Heinitializer),\n","        ]) \n","\n","\n","    learning_rate=0.001\n","    adamOptimizerWithCustomLearningRate =tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    newModel.compile( optimizer=adamOptimizerWithCustomLearningRate,\n","    loss= tf.keras.losses.MeanAbsolutePercentageError())\n","\n","    history= newModel.fit(\n","      training_embedings,\n","      training_labels,\n","      validation_data=(testing_embedings, testing_labels),\n","    # train_ds,\n","    # validation_data=val_ds,\n","    epochs=1\n","    )\n","\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","\n","    numberEpochsPerIteration=50\n","    numberOfIterations=4\n","    for i in range(1, numberOfIterations+1):\n","      learning_rate=init_learning_rate/(i*10)\n","      adamOptimizerWithCustomLearningRate =tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","      newModel.compile(\n","        optimizer=adamOptimizerWithCustomLearningRate,\n","        # loss=tf.keras.losses.MeanAbsoluteError())\n","      loss= tf.keras.losses.MeanAbsolutePercentageError())\n","\n","\n","        # loss=tf.keras.losses.MeanSquaredError())\n","      history= newModel.fit(\n","        training_embedings,\n","        training_labels,\n","        validation_data=(testing_embedings, testing_labels),\n","      # train_ds,\n","      # validation_data=val_ds,\n","      epochs=numberEpochsPerIteration\n","      )\n","      loss += history.history['loss']\n","      val_loss += history.history['val_loss']\n","\n","    dic[combination][\"ValLosses\"].append(min(val_loss))\n","  print( str(dic[combination][\"ModelName\"]) + \" bestvalloss \"+ str(min( dic[combination][\"ValLosses\"])))\n","\n","\n","\n","\n","\n","\n","    \n","\n","  \n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n","157/157 [==============================] - 1s 2ms/step - loss: 39.7719 - val_loss: 25.8082\n","Epoch 1/50\n","157/157 [==============================] - 1s 2ms/step - loss: 28.0755 - val_loss: 25.2041\n","Epoch 2/50\n","157/157 [==============================] - 0s 1ms/step - loss: 28.0806 - val_loss: 24.5084\n","Epoch 3/50\n","157/157 [==============================] - 0s 1ms/step - loss: 27.3231 - val_loss: 23.6416\n","Epoch 4/50\n","157/157 [==============================] - 0s 1ms/step - loss: 25.6935 - val_loss: 23.3117\n","Epoch 5/50\n","157/157 [==============================] - 0s 1ms/step - loss: 25.2438 - val_loss: 22.8541\n","Epoch 6/50\n","157/157 [==============================] - 0s 1ms/step - loss: 24.4823 - val_loss: 22.4892\n","Epoch 7/50\n","157/157 [==============================] - 0s 1ms/step - loss: 24.3341 - val_loss: 22.1118\n","Epoch 8/50\n","157/157 [==============================] - 0s 1ms/step - loss: 23.7493 - val_loss: 21.9773\n","Epoch 9/50\n","157/157 [==============================] - 0s 1ms/step - loss: 23.1405 - val_loss: 21.6151\n","Epoch 10/50\n","157/157 [==============================] - 0s 1ms/step - loss: 22.8069 - val_loss: 21.4924\n","Epoch 11/50\n","157/157 [==============================] - 0s 1ms/step - loss: 22.6101 - val_loss: 21.2545\n","Epoch 12/50\n","157/157 [==============================] - 0s 1ms/step - loss: 22.4402 - val_loss: 21.1644\n","Epoch 13/50\n","157/157 [==============================] - 0s 1ms/step - loss: 21.5545 - val_loss: 21.3263\n","Epoch 14/50\n","157/157 [==============================] - 0s 1ms/step - loss: 21.7903 - val_loss: 20.8641\n","Epoch 15/50\n","157/157 [==============================] - 0s 1ms/step - loss: 21.1454 - val_loss: 20.6425\n","Epoch 16/50\n","157/157 [==============================] - 0s 1ms/step - loss: 20.4852 - val_loss: 20.5557\n","Epoch 17/50\n","157/157 [==============================] - 0s 1ms/step - loss: 20.9553 - val_loss: 20.5622\n","Epoch 18/50\n","157/157 [==============================] - 0s 1ms/step - loss: 20.5219 - val_loss: 20.2673\n","Epoch 19/50\n","157/157 [==============================] - 0s 1ms/step - loss: 19.8592 - val_loss: 20.3750\n","Epoch 20/50\n","157/157 [==============================] - 0s 1ms/step - loss: 19.9015 - val_loss: 20.1539\n","Epoch 21/50\n","157/157 [==============================] - 0s 2ms/step - loss: 19.8107 - val_loss: 20.3038\n","Epoch 22/50\n","157/157 [==============================] - 0s 2ms/step - loss: 19.0844 - val_loss: 20.1864\n","Epoch 23/50\n","157/157 [==============================] - 0s 1ms/step - loss: 19.6249 - val_loss: 20.0953\n","Epoch 24/50\n","157/157 [==============================] - 0s 1ms/step - loss: 18.9763 - val_loss: 20.0291\n","Epoch 25/50\n","157/157 [==============================] - 0s 1ms/step - loss: 18.9656 - val_loss: 20.0072\n","Epoch 26/50\n","157/157 [==============================] - 0s 1ms/step - loss: 18.6614 - val_loss: 19.9503\n","Epoch 27/50\n","157/157 [==============================] - 0s 1ms/step - loss: 18.5225 - val_loss: 19.9349\n","Epoch 28/50\n","157/157 [==============================] - 0s 1ms/step - loss: 18.0010 - val_loss: 19.9537\n","Epoch 29/50\n","157/157 [==============================] - 0s 1ms/step - loss: 17.8672 - val_loss: 19.9378\n","Epoch 30/50\n","157/157 [==============================] - 0s 1ms/step - loss: 18.0718 - val_loss: 19.7643\n","Epoch 31/50\n","157/157 [==============================] - 0s 1ms/step - loss: 17.6579 - val_loss: 19.6311\n","Epoch 32/50\n","157/157 [==============================] - 0s 1ms/step - loss: 17.0798 - val_loss: 19.7693\n","Epoch 33/50\n","157/157 [==============================] - 0s 1ms/step - loss: 17.2083 - val_loss: 19.7885\n","Epoch 34/50\n","157/157 [==============================] - 0s 1ms/step - loss: 17.2493 - val_loss: 19.7829\n","Epoch 35/50\n","157/157 [==============================] - 0s 1ms/step - loss: 16.9988 - val_loss: 19.8270\n","Epoch 36/50\n","157/157 [==============================] - 0s 1ms/step - loss: 17.6423 - val_loss: 19.6662\n","Epoch 37/50\n","157/157 [==============================] - 0s 1ms/step - loss: 16.4543 - val_loss: 19.6787\n","Epoch 38/50\n","157/157 [==============================] - 0s 1ms/step - loss: 16.1585 - val_loss: 19.5172\n","Epoch 39/50\n","157/157 [==============================] - 0s 1ms/step - loss: 16.7685 - val_loss: 19.4800\n","Epoch 40/50\n","157/157 [==============================] - 0s 1ms/step - loss: 16.2482 - val_loss: 19.4508\n","Epoch 41/50\n","157/157 [==============================] - 0s 1ms/step - loss: 16.5728 - val_loss: 19.3436\n","Epoch 42/50\n","157/157 [==============================] - 0s 1ms/step - loss: 15.8206 - val_loss: 19.2236\n","Epoch 43/50\n","157/157 [==============================] - 0s 1ms/step - loss: 15.8990 - val_loss: 19.3028\n","Epoch 44/50\n","157/157 [==============================] - 0s 1ms/step - loss: 15.6967 - val_loss: 19.1861\n","Epoch 45/50\n","157/157 [==============================] - 0s 1ms/step - loss: 15.4184 - val_loss: 19.2083\n","Epoch 46/50\n","157/157 [==============================] - 0s 1ms/step - loss: 15.5881 - val_loss: 19.1473\n","Epoch 47/50\n","157/157 [==============================] - 0s 1ms/step - loss: 15.7030 - val_loss: 19.1409\n","Epoch 48/50\n","157/157 [==============================] - 0s 1ms/step - loss: 15.5308 - val_loss: 19.0680\n","Epoch 49/50\n","157/157 [==============================] - 0s 1ms/step - loss: 15.2076 - val_loss: 19.0946\n","Epoch 50/50\n","157/157 [==============================] - 0s 1ms/step - loss: 15.2697 - val_loss: 19.0932\n","Epoch 1/50\n","157/157 [==============================] - 1s 2ms/step - loss: 14.6314 - val_loss: 19.1248\n","Epoch 2/50\n","157/157 [==============================] - 0s 2ms/step - loss: 14.8436 - val_loss: 19.0815\n","Epoch 3/50\n","157/157 [==============================] - 0s 2ms/step - loss: 14.7646 - val_loss: 19.0757\n","Epoch 4/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.8373 - val_loss: 19.0345\n","Epoch 5/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.3507 - val_loss: 19.0277\n","Epoch 6/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.6721 - val_loss: 18.9683\n","Epoch 7/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.3541 - val_loss: 18.9259\n","Epoch 8/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.4165 - val_loss: 19.0173\n","Epoch 9/50\n","157/157 [==============================] - 0s 2ms/step - loss: 14.6606 - val_loss: 19.0450\n","Epoch 10/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.5433 - val_loss: 19.0411\n","Epoch 11/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.4077 - val_loss: 19.0107\n","Epoch 12/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.4717 - val_loss: 18.9897\n","Epoch 13/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.2560 - val_loss: 18.9714\n","Epoch 14/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.0870 - val_loss: 19.0143\n","Epoch 15/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.3169 - val_loss: 19.0026\n","Epoch 16/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.6254 - val_loss: 18.9863\n","Epoch 17/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.5183 - val_loss: 19.0587\n","Epoch 18/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.5206 - val_loss: 19.0731\n","Epoch 19/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.6579 - val_loss: 19.0454\n","Epoch 20/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.9601 - val_loss: 18.9825\n","Epoch 21/50\n","157/157 [==============================] - 0s 1ms/step - loss: 14.0222 - val_loss: 19.0114\n","Epoch 22/50\n","157/157 [==============================] - 0s 2ms/step - loss: 13.7054 - val_loss: 18.9984\n","Epoch 23/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.7192 - val_loss: 18.9664\n","Epoch 24/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.7541 - val_loss: 18.9713\n","Epoch 25/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.7981 - val_loss: 19.0194\n","Epoch 26/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.5506 - val_loss: 19.0067\n","Epoch 27/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.7119 - val_loss: 18.9549\n","Epoch 28/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.7363 - val_loss: 18.9548\n","Epoch 29/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.8370 - val_loss: 18.9161\n","Epoch 30/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.8645 - val_loss: 18.9597\n","Epoch 31/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.8443 - val_loss: 18.9153\n","Epoch 32/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.2806 - val_loss: 18.9649\n","Epoch 33/50\n","157/157 [==============================] - 0s 2ms/step - loss: 13.3406 - val_loss: 18.8736\n","Epoch 34/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.2881 - val_loss: 18.8904\n","Epoch 35/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.0085 - val_loss: 18.8639\n","Epoch 36/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.9374 - val_loss: 18.8635\n","Epoch 37/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.0668 - val_loss: 18.8100\n","Epoch 38/50\n","157/157 [==============================] - 0s 2ms/step - loss: 13.2313 - val_loss: 18.8566\n","Epoch 39/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.7989 - val_loss: 18.8429\n","Epoch 40/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.3524 - val_loss: 18.9175\n","Epoch 41/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.7951 - val_loss: 18.9470\n","Epoch 42/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.3056 - val_loss: 18.9187\n","Epoch 43/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.0485 - val_loss: 18.8995\n","Epoch 44/50\n","157/157 [==============================] - 0s 2ms/step - loss: 12.9679 - val_loss: 18.9175\n","Epoch 45/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.5250 - val_loss: 18.8466\n","Epoch 46/50\n","157/157 [==============================] - 0s 2ms/step - loss: 13.0613 - val_loss: 18.8532\n","Epoch 47/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.2686 - val_loss: 18.7660\n","Epoch 48/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.0809 - val_loss: 18.8419\n","Epoch 49/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.9910 - val_loss: 18.7492\n","Epoch 50/50\n","157/157 [==============================] - 0s 2ms/step - loss: 13.0200 - val_loss: 18.7911\n","Epoch 1/50\n","157/157 [==============================] - 1s 2ms/step - loss: 13.1350 - val_loss: 18.8047\n","Epoch 2/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.9241 - val_loss: 18.8118\n","Epoch 3/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.3153 - val_loss: 18.8201\n","Epoch 4/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.6422 - val_loss: 18.8074\n","Epoch 5/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.5568 - val_loss: 18.7932\n","Epoch 6/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.7505 - val_loss: 18.7359\n","Epoch 7/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.7420 - val_loss: 18.7210\n","Epoch 8/50\n","157/157 [==============================] - 0s 2ms/step - loss: 12.7737 - val_loss: 18.6788\n","Epoch 9/50\n","157/157 [==============================] - 0s 2ms/step - loss: 12.6467 - val_loss: 18.7190\n","Epoch 10/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.3344 - val_loss: 18.7106\n","Epoch 11/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.5043 - val_loss: 18.7340\n","Epoch 12/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.4516 - val_loss: 18.6668\n","Epoch 13/50\n","157/157 [==============================] - 0s 1ms/step - loss: 13.0380 - val_loss: 18.7357\n","Epoch 14/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.5592 - val_loss: 18.6998\n","Epoch 15/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.5690 - val_loss: 18.7344\n","Epoch 16/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.3243 - val_loss: 18.7101\n","Epoch 17/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.6723 - val_loss: 18.6291\n","Epoch 18/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.4737 - val_loss: 18.6683\n","Epoch 19/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.4544 - val_loss: 18.6811\n","Epoch 20/50\n","157/157 [==============================] - 0s 2ms/step - loss: 12.1539 - val_loss: 18.6569\n","Epoch 21/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.3606 - val_loss: 18.6433\n","Epoch 22/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.5764 - val_loss: 18.6375\n","Epoch 23/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.1541 - val_loss: 18.6199\n","Epoch 24/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.5870 - val_loss: 18.5943\n","Epoch 25/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.3119 - val_loss: 18.6568\n","Epoch 26/50\n","157/157 [==============================] - 0s 2ms/step - loss: 12.2518 - val_loss: 18.6580\n","Epoch 27/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.2657 - val_loss: 18.6791\n","Epoch 28/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.2258 - val_loss: 18.6084\n","Epoch 29/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.0921 - val_loss: 18.6399\n","Epoch 30/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.1308 - val_loss: 18.6216\n","Epoch 31/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.3545 - val_loss: 18.5614\n","Epoch 32/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.8688 - val_loss: 18.5704\n","Epoch 33/50\n","157/157 [==============================] - 0s 2ms/step - loss: 12.1492 - val_loss: 18.5270\n","Epoch 34/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.1420 - val_loss: 18.4904\n","Epoch 35/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.1063 - val_loss: 18.5570\n","Epoch 36/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.0863 - val_loss: 18.4950\n","Epoch 37/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.1793 - val_loss: 18.5369\n","Epoch 38/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.9177 - val_loss: 18.5574\n","Epoch 39/50\n","157/157 [==============================] - 0s 2ms/step - loss: 12.3401 - val_loss: 18.5298\n","Epoch 40/50\n","157/157 [==============================] - 0s 2ms/step - loss: 12.0467 - val_loss: 18.4946\n","Epoch 41/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.0736 - val_loss: 18.5040\n","Epoch 42/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.8919 - val_loss: 18.5451\n","Epoch 43/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.7598 - val_loss: 18.5168\n","Epoch 44/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.9953 - val_loss: 18.5240\n","Epoch 45/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.0802 - val_loss: 18.5154\n","Epoch 46/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.7864 - val_loss: 18.4803\n","Epoch 47/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.5566 - val_loss: 18.4880\n","Epoch 48/50\n","157/157 [==============================] - 0s 2ms/step - loss: 12.0629 - val_loss: 18.4891\n","Epoch 49/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.8513 - val_loss: 18.5050\n","Epoch 50/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.6122 - val_loss: 18.4468\n","Epoch 1/50\n","157/157 [==============================] - 1s 2ms/step - loss: 11.6241 - val_loss: 18.4437\n","Epoch 2/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.4552 - val_loss: 18.4990\n","Epoch 3/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.6410 - val_loss: 18.5012\n","Epoch 4/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.8076 - val_loss: 18.4764\n","Epoch 5/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.1548 - val_loss: 18.4636\n","Epoch 6/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.5851 - val_loss: 18.4398\n","Epoch 7/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.1739 - val_loss: 18.4411\n","Epoch 8/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.3549 - val_loss: 18.4141\n","Epoch 9/50\n","157/157 [==============================] - 0s 2ms/step - loss: 11.7559 - val_loss: 18.4350\n","Epoch 10/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.8076 - val_loss: 18.4040\n","Epoch 11/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.0201 - val_loss: 18.4319\n","Epoch 12/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.4596 - val_loss: 18.4367\n","Epoch 13/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.6686 - val_loss: 18.4382\n","Epoch 14/50\n","157/157 [==============================] - 0s 2ms/step - loss: 11.7009 - val_loss: 18.3854\n","Epoch 15/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.3672 - val_loss: 18.4576\n","Epoch 16/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.5262 - val_loss: 18.4263\n","Epoch 17/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.6432 - val_loss: 18.3961\n","Epoch 18/50\n","157/157 [==============================] - 0s 2ms/step - loss: 11.7497 - val_loss: 18.4261\n","Epoch 19/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.5733 - val_loss: 18.4359\n","Epoch 20/50\n","157/157 [==============================] - 0s 1ms/step - loss: 12.1357 - val_loss: 18.4149\n","Epoch 21/50\n","157/157 [==============================] - 0s 2ms/step - loss: 11.3530 - val_loss: 18.4552\n","Epoch 22/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.6623 - val_loss: 18.3864\n","Epoch 23/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.9345 - val_loss: 18.3750\n","Epoch 24/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.2197 - val_loss: 18.3853\n","Epoch 25/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.3741 - val_loss: 18.3633\n","Epoch 26/50\n","157/157 [==============================] - 0s 2ms/step - loss: 11.6612 - val_loss: 18.3803\n","Epoch 27/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.2807 - val_loss: 18.3995\n","Epoch 28/50\n","157/157 [==============================] - 0s 2ms/step - loss: 11.4132 - val_loss: 18.3844\n","Epoch 29/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.8340 - val_loss: 18.3297\n","Epoch 30/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.3739 - val_loss: 18.3495\n","Epoch 31/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.3772 - val_loss: 18.3931\n","Epoch 32/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.3017 - val_loss: 18.3855\n","Epoch 33/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.5268 - val_loss: 18.3886\n","Epoch 34/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.0827 - val_loss: 18.3675\n","Epoch 35/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.4124 - val_loss: 18.3666\n","Epoch 36/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.2783 - val_loss: 18.3393\n","Epoch 37/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.6223 - val_loss: 18.3671\n","Epoch 38/50\n","157/157 [==============================] - 0s 2ms/step - loss: 11.6367 - val_loss: 18.3693\n","Epoch 39/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.3884 - val_loss: 18.3501\n","Epoch 40/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.5716 - val_loss: 18.3569\n","Epoch 41/50\n","157/157 [==============================] - 0s 1ms/step - loss: 11.4225 - val_loss: 18.4149\n","Epoch 42/50\n"," 84/157 [===============>..............] - ETA: 0s - loss: 11.3633"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-109-c340dbe573d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0;31m# train_ds,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0;31m# validation_data=val_ds,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumberEpochsPerIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m       )\n\u001b[1;32m     71\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mavailable\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mbecause\u001b[0m \u001b[0mit\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mgarbage\u001b[0m \u001b[0mcollected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \"\"\"\n\u001b[0;32m--> 565\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m       raise ValueError(\n\u001b[1;32m    567\u001b[0m           \u001b[0;34m\"Arguments and signature arguments do not match. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEFY9_uYMtfF","outputId":"c6e7aaf6-3c57-413e-9a1e-09be8301940d"},"source":["dic.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys([('openface', 'deepface'), ('facenet', 'vggface', 'deepface', 'deepID'), ('facenet', 'vggface', 'deepface', 'deepID', 'arcFace'), ('facenet', 'vggface', 'openface', 'deepface'), ('openface', 'deepface', 'deepID'), ('vggface', 'openface', 'arcFace'), ('facenet', 'vggface', 'openface', 'deepface', 'arcFace'), ('openface', 'deepface', 'arcFace'), ('facenet', 'arcFace'), ('openface', 'deepface', 'deepID', 'arcFace'), ('facenet', 'vggface', 'openface', 'deepID', 'arcFace'), ('facenet', 'vggface', 'deepface'), ('vggface', 'deepface', 'deepID', 'arcFace'), ('deepface', 'arcFace')])"]},"metadata":{"tags":[]},"execution_count":98}]},{"cell_type":"code","metadata":{"id":"vGdNG338k0T4"},"source":["# fullDic= dict()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"49Z1OHPyk4zK"},"source":["# Python code to merge dict using update() method\n","def Merge(dict1, dict2):\n","    print(\"Hi\")\n","    return  {**dict1, **dict2}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IHUl7pC8lUne","outputId":"45d6ea71-e1f1-49e5-f7f5-4169c933c4b4"},"source":["fullDic= Merge(fullDic,dic)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Hi\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bp3H4gZe4a2J","outputId":"bf4742f8-9248-44ca-8dfc-ed791d6c0cc1"},"source":["%cd .."],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/AttracivenessRegression\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KoMqGcz1JZqG","outputId":"309ec0dd-aec4-494a-958d-a06b2a78d708"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["FaceEncodings  FaceRecognisionImages  fullDic.txt  Test  Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ped_nresI0F7","outputId":"de131b1c-1215-436d-a2ea-7272708f39ae"},"source":["# ('facenet',):{'ModelName': ('facenet',), 'ValLosses': [17.784801483154297, 17.84476661682129, 19.509611129760742, 17.953367233276367, 17.68967628479004]}\n","dic=dict()\n","with open(\"fullDic.txt\", 'r') as f: \n","    for line_number, line in  enumerate(f):\n","      parts=line.split(\":\")\n","      name=parts[0]\n","      lista= parts[3].split(\"}\")[0]\n","      # print(\"1 \"+ lista)\n","      lista= lista.split(\"[\")[1]\n","      # print(\"2 \"+ lista)\n","      lista= lista.split(\"]\")[0]\n","      # print(\"3 \"+ lista)\n","\n","      lista= lista.split(\",\")\n","      numbers=list()\n","      for i, strNumber in enumerate(lista):\n","        print(line_number, i,strNumber)\n","        numbers.append(float(strNumber))\n","      dic[name]=numbers\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 0 17.784801483154297\n","0 1  17.84476661682129\n","0 2  19.509611129760742\n","0 3  17.953367233276367\n","0 4  17.68967628479004\n","1 0 17.220624923706055\n","1 1  39.986351013183594\n","1 2  17.040260314941406\n","1 3  18.050291061401367\n","1 4  18.324792861938477\n","2 0 19.604345321655273\n","2 1  19.738405227661133\n","2 2  19.550737380981445\n","2 3  19.18060874938965\n","2 4  18.83814811706543\n","3 0 22.06441307067871\n","3 1  24.90096664428711\n","3 2  30.50788116455078\n","3 3  19.78093719482422\n","3 4  21.89668083190918\n","4 0 39.98918533325195\n","4 1  18.935495376586914\n","4 2  39.97893524169922\n","4 3  18.903831481933594\n","4 4  19.1740665435791\n","5 0 18.710786819458008\n","5 1  17.27591896057129\n","5 2  18.291828155517578\n","5 3  18.40117073059082\n","5 4  17.419044494628906\n","6 0 17.808855056762695\n","6 1  16.953943252563477\n","6 2  18.084924697875977\n","6 3  18.329753875732422\n","6 4  16.913318634033203\n","7 0 16.70099639892578\n","7 1  17.670406341552734\n","7 2  18.065282821655273\n","7 3  18.365083694458008\n","7 4  17.805845260620117\n","8 0 23.127981185913086\n","8 1  27.650358200073242\n","8 2  20.87938117980957\n","8 3  21.285755157470703\n","8 4  20.58898162841797\n","9 0 18.07789421081543\n","9 1  17.585359573364258\n","9 2  16.611736297607422\n","9 3  16.858470916748047\n","9 4  17.874128341674805\n","10 0 27.805343627929688\n","10 1  24.896772384643555\n","10 2  27.175048828125\n","10 3  30.091650009155273\n","10 4  22.204511642456055\n","11 0 17.692716598510742\n","11 1  18.38829231262207\n","11 2  18.561656951904297\n","11 3  18.47364044189453\n","11 4  17.40798568725586\n","12 0 17.289703369140625\n","12 1  17.90843963623047\n","12 2  16.76219940185547\n","12 3  16.786067962646484\n","12 4  17.252418518066406\n","13 0 16.59671401977539\n","13 1  16.72817611694336\n","13 2  16.51791000366211\n","13 3  16.605609893798828\n","13 4  17.701366424560547\n","14 0 17.281455993652344\n","14 1  17.014022827148438\n","14 2  17.471227645874023\n","14 3  17.452302932739258\n","14 4  39.9953727722168\n","15 0 16.751989364624023\n","15 1  17.36640167236328\n","15 2  39.98667526245117\n","15 3  17.070377349853516\n","15 4  39.98038101196289\n","16 0 20.70092010498047\n","16 1  37.00416564941406\n","16 2  20.123844146728516\n","16 3  63.220680236816406\n","16 4  25.0192813873291\n","17 0 39.99520492553711\n","17 1  17.13142967224121\n","17 2  18.737306594848633\n","17 3  17.209779739379883\n","17 4  18.48691749572754\n","18 0 40.462181091308594\n","18 1  19.552682876586914\n","18 2  19.024038314819336\n","18 3  21.024717330932617\n","18 4  26.104280471801758\n","19 0 24.492963790893555\n","19 1  40.04132080078125\n","19 2  26.888200759887695\n","19 3  69.80814361572266\n","19 4  25.958494186401367\n","20 0 17.006996154785156\n","20 1  17.46000862121582\n","20 2  17.929210662841797\n","20 3  17.45370101928711\n","20 4  17.479557037353516\n","21 0 17.482738494873047\n","21 1  17.314525604248047\n","21 2  17.45757293701172\n","21 3  17.505613327026367\n","21 4  17.69111442565918\n","22 0 17.029314041137695\n","22 1  18.522470474243164\n","22 2  17.544921875\n","22 3  17.594350814819336\n","22 4  16.90574836730957\n","23 0 17.56166648864746\n","23 1  16.890546798706055\n","23 2  19.342132568359375\n","23 3  17.31808090209961\n","23 4  17.533971786499023\n","24 0 21.488767623901367\n","24 1  21.675119400024414\n","24 2  23.37149429321289\n","24 3  99.14334106445312\n","24 4  22.467914581298828\n","25 0 22.72884178161621\n","25 1  24.885704040527344\n","25 2  24.624202728271484\n","25 3  25.02614402770996\n","25 4  29.434736251831055\n","26 0 20.199880599975586\n","26 1  26.82564353942871\n","26 2  42.10179901123047\n","26 3  23.028533935546875\n","26 4  19.072786331176758\n","27 0 69.95846557617188\n","27 1  26.411481857299805\n","27 2  26.7102108001709\n","27 3  23.077430725097656\n","27 4  41.642486572265625\n","28 0 34.293052673339844\n","28 1  33.98988342285156\n","28 2  29.765811920166016\n","28 3  23.62899398803711\n","28 4  29.871253967285156\n","29 0 17.511577606201172\n","29 1  16.567476272583008\n","29 2  17.80280303955078\n","29 3  18.166057586669922\n","29 4  17.872507095336914\n","30 0 32.185115814208984\n","30 1  28.63973045349121\n","30 2  40.87815856933594\n","30 3  27.412378311157227\n","30 4  22.859214782714844\n","31 0 20.326030731201172\n","31 1  30.366212844848633\n","31 2  20.99091911315918\n","31 3  36.67364501953125\n","31 4  25.74910545349121\n","32 0 17.599260330200195\n","32 1  16.89548683166504\n","32 2  16.80324935913086\n","32 3  17.059829711914062\n","32 4  17.103269577026367\n","33 0 25.2448673248291\n","33 1  21.542957305908203\n","33 2  31.544721603393555\n","33 3  22.203577041625977\n","33 4  27.23777198791504\n","34 0 17.378355026245117\n","34 1  17.309425354003906\n","34 2  18.508079528808594\n","34 3  17.230207443237305\n","34 4  17.258865356445312\n","35 0 21.076128005981445\n","35 1  21.86531639099121\n","35 2  58.95634841918945\n","35 3  20.916276931762695\n","35 4  26.39076042175293\n","36 0 21.094127655029297\n","36 1  38.53733825683594\n","36 2  18.874134063720703\n","36 3  20.335771560668945\n","36 4  19.756467819213867\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WR5y2v2cKa-7"},"source":["for key, value in dic.items():\n","  dic[key]=min(dic[key])\n","  print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjONf8LTL9lS","outputId":"bf31be65-980e-4c2e-e977-dc36fb28457b"},"source":["for key, value in dic.items():\n","  print(dic[key], key)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["17.68967628479004 ('facenet',)\n","17.040260314941406 ('vggface',)\n","18.83814811706543 ('openface',)\n","19.78093719482422 ('deepface',)\n","18.903831481933594 ('deepID',)\n","17.27591896057129 ('arcFace',)\n","16.913318634033203 ('facenet', 'vggface')\n","16.70099639892578 ('facenet', 'openface')\n","20.58898162841797 ('facenet', 'deepface', 'deepID')\n","16.611736297607422 ('facenet', 'vggface', 'deepID', 'arcFace')\n","22.204511642456055 ('facenet', 'openface', 'deepface', 'arcFace')\n","17.40798568725586 ('facenet', 'openface', 'deepID')\n","16.76219940185547 ('facenet', 'openface', 'arcFace')\n","16.51791000366211 ('facenet', 'deepID', 'arcFace')\n","17.014022827148438 ('facenet', 'openface', 'deepID', 'arcFace')\n","16.751989364624023 ('vggface', 'arcFace')\n","20.123844146728516 ('vggface', 'deepface')\n","17.13142967224121 ('facenet', 'vggface', 'openface', 'deepID')\n","19.024038314819336 ('vggface', 'openface', 'deepface', 'arcFace')\n","24.492963790893555 ('vggface', 'openface', 'deepface', 'deepID')\n","17.006996154785156 ('vggface', 'deepID', 'arcFace')\n","17.314525604248047 ('facenet', 'vggface', 'arcFace')\n","16.90574836730957 ('deepID', 'arcFace')\n","16.890546798706055 ('vggface', 'deepID')\n","21.488767623901367 ('openface', 'deepface')\n","22.72884178161621 ('facenet', 'vggface', 'deepface', 'deepID')\n","19.072786331176758 ('facenet', 'vggface', 'deepface', 'deepID', 'arcFace')\n","23.077430725097656 ('facenet', 'vggface', 'openface', 'deepface')\n","23.62899398803711 ('openface', 'deepface', 'deepID')\n","16.567476272583008 ('vggface', 'openface', 'arcFace')\n","22.859214782714844 ('facenet', 'vggface', 'openface', 'deepface', 'arcFace')\n","20.326030731201172 ('openface', 'deepface', 'arcFace')\n","16.80324935913086 ('facenet', 'arcFace')\n","21.542957305908203 ('openface', 'deepface', 'deepID', 'arcFace')\n","17.230207443237305 ('facenet', 'vggface', 'openface', 'deepID', 'arcFace')\n","20.916276931762695 ('facenet', 'vggface', 'deepface')\n","18.874134063720703 ('vggface', 'deepface', 'deepID', 'arcFace')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bPS2BocslaLl","outputId":"8d7f2cf8-0518-45a9-d3c7-d74eebfe12a3"},"source":["doneAlready=list(fullDic.keys())\n","doneAlready"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('facenet',),\n"," ('vggface',),\n"," ('openface',),\n"," ('deepface',),\n"," ('deepID',),\n"," ('arcFace',),\n"," ('facenet', 'vggface'),\n"," ('facenet', 'openface'),\n"," ('facenet', 'deepface'),\n"," ('facenet', 'deepface', 'deepID'),\n"," ('facenet', 'vggface', 'deepID', 'arcFace'),\n"," ('facenet', 'openface', 'deepface', 'arcFace'),\n"," ('facenet', 'openface', 'deepID'),\n"," ('facenet', 'openface', 'arcFace'),\n"," ('facenet', 'deepID', 'arcFace'),\n"," ('vggface', 'openface', 'deepface', 'deepID', 'arcFace'),\n"," ('facenet', 'openface', 'deepID', 'arcFace'),\n"," ('vggface', 'arcFace'),\n"," ('vggface', 'deepface'),\n"," ('facenet', 'vggface', 'openface', 'deepID'),\n"," ('vggface', 'openface', 'deepface', 'arcFace'),\n"," ('vggface', 'openface', 'deepface', 'deepID'),\n"," ('vggface', 'deepID', 'arcFace'),\n"," ('facenet', 'vggface', 'arcFace'),\n"," ('deepID', 'arcFace'),\n"," ('vggface', 'deepID'),\n"," ('vggface', 'deepface', 'arcFace'),\n"," ('openface', 'deepface'),\n"," ('facenet', 'vggface', 'deepface', 'deepID'),\n"," ('facenet', 'vggface', 'deepface', 'deepID', 'arcFace'),\n"," ('facenet', 'vggface', 'openface', 'deepface'),\n"," ('openface', 'deepface', 'deepID'),\n"," ('vggface', 'openface', 'arcFace'),\n"," ('facenet', 'vggface', 'openface', 'deepface', 'arcFace'),\n"," ('openface', 'deepface', 'arcFace'),\n"," ('facenet', 'arcFace'),\n"," ('openface', 'deepface', 'deepID', 'arcFace'),\n"," ('facenet', 'vggface', 'openface', 'deepID', 'arcFace'),\n"," ('facenet', 'vggface', 'deepface'),\n"," ('vggface', 'deepface', 'deepID', 'arcFace'),\n"," ('deepface', 'arcFace')]"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"code","metadata":{"id":"KVvvUPwRWYyD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625135529967,"user_tz":-60,"elapsed":369,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"7d279fec-ad09-4da0-81b9-b88cf58dfe98"},"source":["import time\n","from os import path\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import pickle\n","\n","from deepface.basemodels import VGGFace, OpenFace, Facenet, FbDeepFace, DeepID, DlibWrapper, ArcFace, Boosting\n","from deepface.commons import functions, realtime, distance as dst\n","from keras.preprocessing import image as image_keras_preprocessing\n","\n","#################\n","\n","def postProcesssing(img, grayscale=False,target_size=(160,160)):\n","  #post-processing\n","  if grayscale == True:\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","  img = cv2.resize(img, target_size)\n","  #TODO: resize causes transformation on base image, you should add black pixels to rezie it to target_size\n","\n","  img_pixels = image_keras_preprocessing.img_to_array(img)\n","  img_pixels = np.expand_dims(img_pixels, axis = 0)\n","  img_pixels /= 255 #normalize input in [0, 1]\n","  return img_pixels\n","\n","\n","def getFaceImages(images, listWithFaceDetectors=[\"ssd\", \"opencv\",\"mtcnn\",\"dlib\", \"retinaface\"], grayscale=False, enforce_detection = True, align = True):\n","  #detect face in all images\n","  FRimages=list()\n","  for image in images: \n","    managed_to_find_face=False\n","    for detector_backend in listWithFaceDetectors:\n","      try:\n","          img, region = functions.detect_face(img = image, detector_backend = detector_backend, grayscale = grayscale, enforce_detection = enforce_detection, align = align)\n","          #--------------------------\n","          if img.shape[0] == 0 or img.shape[1] == 0:\n","            if enforce_detection == True:\n","              raise ValueError(\"Detected face shape is \", img.shape,\". Consider to set enforce_detection argument to False.\")\n","            else: #restore base image\n","              img = base_img.copy()\n","          #--------------------------\n","\n","        #   img = functions.preprocess_face(img = image\n","        # , target_size=(input_shape_y, input_shape_x)\n","        # , enforce_detection = enforce_detection\n","        # , detector_backend = detector_backend\n","        # , align = align)\n","          managed_to_find_face=True\n","          FRimages.append(img)\n","          break;\n","      except ValueError: \n","        continue\n","    if(managed_to_find_face==False):\n","      print(\"I COUNT NOT FIND THE FACE!!!!!\")\n","      plt.imshow(image)\n","  return FRimages\n","\n","def getSizeOfEmbedings(tempImage,model_list):\n","  sizeOfFinalinput=0;\n","  tempImage=FRimages[0]\n","  for model in model_list:\n","    input_shape =  input_shape_x, input_shape_y= functions.find_input_shape(model)\n","    img= postProcesssing(tempImage, grayscale=False,target_size=(input_shape_y, input_shape_x))\n","    curentEmbedings = model.predict(img)[0].tolist()\n","    sizeOfFinalinput= sizeOfFinalinput+ len(curentEmbedings)\n","  return sizeOfFinalinput\n","\n","def getEmbedings(FRimages,sizeOfFinalInput):\n","  inputSize=(len(FRimages), sizeOfFinalInput)\n","  embedings=np.empty(inputSize)\n","  for index, image in enumerate(FRimages):\n","    image_embedgins= list()\n","    for model in model_list:\n","      try:\n","        input_shape =  input_shape_x, input_shape_y= functions.find_input_shape(model)\n","        img= postProcesssing(image, grayscale=False,target_size=(input_shape_y, input_shape_x))\n","        curentEmbedings = model.predict(img)[0].tolist()\n","        image_embedgins.extend(curentEmbedings)\n","      except:\n","        print(\"i couldnt get embedings\")\n","        plt.imshow(image)\n","    npEmbedings=np.asarray(image_embedgins)\n","    print(npEmbedings.shape)\n","    embedings[index]=npEmbedings\n","  return embedings\n","\n","\n","def myRepresent(images,model_list , listWithFaceDetectors=[\"ssd\", \"opencv\",\"mtcnn\",\"dlib\", \"retinaface\"], grayscale=False, enforce_detection = True, align = True):\n","  \"\"\"\n","  This function represents facial images as vectors.\n","  Parameters:\n","    img_path: exact image path, numpy array or based64 encoded images could be passed.\n","    model_name (string): VGG-Face, Facenet, OpenFace, DeepFace, DeepID, Dlib, ArcFace.\n","    model: Built deepface model. A face recognition model is built every call of verify function. You can pass pre-built face recognition model optionally if you will call verify function several times. Consider to pass model if you are going to call represent function in a for loop.\n","      model = DeepFace.build_model('VGG-Face')\n","    enforce_detection (boolean): If any face could not be detected in an image, then verify function will return exception. Set this to False not to have this exception. This might be convenient for low resolution images.\n","    detector_backend (string): set face detector backend as retinaface, mtcnn, opencv, ssd or dlib\n","  Returns:\n","    Represent function returns a multidimensional vector. The number of dimensions is changing based on the reference model. E.g. FaceNet returns 128 dimensional vector; VGG-Face returns 2622 dimensional vector.\n","  \"\"\"\n","\n","  #detect face in all images\n","  FRimages=getFaceImages(images=images, listWithFaceDetectors=listWithFaceDetectors, grayscale=grayscale, enforce_detection = enforce_detection, align = align)\n","\n","\n","  tempImage=FRimages[0]\n","  sizeOfFinalinput=getSizeOfEmbedings(tempImage,model_list)\n","  print(\"sizeOfFinalinput \"+str(sizeOfFinalinput) )\n","\n","  embedings=getEmbedings(FRimages,sizeOfFinalInput=sizeOfFinalinput)\n","  return embedings\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Directory  /root /.deepface created\n","Directory  /root /.deepface/weights created\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bmY_HP44EFTm","executionInfo":{"status":"ok","timestamp":1625135533144,"user_tz":-60,"elapsed":3,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["from matplotlib.pyplot import *\n","\n","def plot_loss(history):\n","  plt.plot(history.history['loss'], label='loss')\n","  plt.plot(history.history['val_loss'], label='val_loss')\n","  # plt.ylim([0,2])\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Error [MPG]')\n","  plt.legend()\n","  plt.grid(True)\n","  draw()\n","  show()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgrdT1vrtJiV"},"source":["# numberEpochsPerIteration=30\n","# numberOfIterations=8\n","# for i in range(numberOfIterations+1, 2*numberOfIterations+1):\n","#   learning_rate=init_learning_rate/(i*10)\n","#   adamOptimizerWithCustomLearningRate =tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","#   newModel.compile(\n","#     optimizer=adamOptimizerWithCustomLearningRate,\n","#     # loss=tf.keras.losses.MeanAbsoluteError())\n","#   loss= tf.keras.losses.MeanAbsolutePercentageError())\n","\n","\n","#     # loss=tf.keras.losses.MeanSquaredError())\n","  \n","#   history= newModel.fit(\n","#     training_embedings,\n","#     training_labels,\n","#     validation_data=(test_embedings, test_labels),\n","#   # train_ds,\n","#   # validation_data=val_ds,\n","#   epochs=numberEpochsPerIteration\n","#   )\n","#   plot_loss(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJnlVVtX7biv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MAgXCl8R-Yw9"},"source":["from kerastuner import HyperModel\n","\n","class CNNHyperModel(HyperModel):\n","    def __init__(self, input_shape):\n","        self.input_shape = input_shape\n","\n","    def build(self, hp):\n","      hyperparameters={ \n","      \"rowShape\":160,\n","    \"columnShape\":160,\n","\n","  #   \"rotation_range\":hp.Int(\n","  #     'rotation_range',\n","  #     min_value=0,\n","  #     max_value=15,\n","  #     default=5\n","  # ),\n","  #   \"width_shift_range\":hp.Float(\n","  #     'width_shift_range',\n","  #     min_value=0.00000000001,\n","  #     max_value=0.2,\n","  #     default=0.05\n","  # ),\n","  #   \"height_shift_range\":hp.Float(\n","  #     'height_shift_range',\n","  #     min_value=0.00000000001,\n","  #     max_value=0.2,\n","  #     default=0.05\n","  # ),\n","  #   \"shear_range\":hp.Float(\n","  #     'shear_range',\n","  #     min_value=0.000000000000001,\n","  #     max_value=0.2,\n","  #     default=0.05,\n","  #     sampling='LOG',\n","  # ),\n","  #   \"zoom_range\":hp.Float(\n","  #     'zoom_range',\n","  #     min_value=0.000000000001,\n","  #     max_value=0.35,\n","  #     default=0.10,\n","  # ),\n","  #   \"horizontal_flip\":hp.Choice(\n","  #     'horizontal_flip',\n","  #     values=[True, False],\n","  #     default=True\n","  # ),\n","    \"fill_mode\":'nearest',\n","    \"batch_size\":128,\n","    \n","    \"verbose\": 1,\n","\n","\n","    \"L2_lambda\":hp.Float(\n","      'L2_lambda',\n","      min_value=0.0001,\n","      max_value=0.01,\n","      # default=0.1,\n","      sampling='LOG',\n","  ),\n","    \n","    \"learning_rate\":hp.Float(\n","      'learning_rate',\n","      min_value=0.0001,\n","      max_value=0.1,\n","      # default=0.2,\n","      sampling='LOG',\n","  ),\n","\n","\n","    \"dropout_posibility\":hp.Float(\n","      'dropout_posibility',\n","      min_value=0.001,\n","      max_value=0.3,\n","      sampling='LOG',\n","      # default=0.01,\n","  ),\n","\n","    \"numberOfHiddenLayers\":hp.Choice(\n","      name='numberOfHiddenLayers',\n","      values=[3,4],\n","      # default=128\n","  ),\n","\n","\n","  \"numberOfNodesInFirstDenseLayer\":hp.Choice(\n","      name='numberOfNodesInFirstDenseLayer',\n","      values=[256,128,64],\n","      # default=128\n","  ),\n","\n","    \"numberOfNodesInSecondDenseLayer\":hp.Choice(\n","      name='numberOfNodesInSecondDenseLayer',\n","      values=[64,32,16],\n","      # default=16\n","  ),\n","\n","      \"numberOfNodesInThirdDenseLayer\":hp.Choice(\n","      name='numberOfNodesInThirdDenseLayer',\n","      values=[32,16,8],\n","      # default=6\n","  ),\n","\n","\n","        \"numberOfNodesInThirdDenseLayer\":hp.Choice(\n","      name='numberOfNodesInThirdDenseLayer',\n","      values=[8,4],\n","      # default=6\n","  ),\n","\n","}\n","      Heinitializer = tf.keras.initializers.HeNormal()\n","      adamOptimizerWithCustomLearningRate =tf.keras.optimizers.Adam(learning_rate=hyperparameters[\"learning_rate\"])\n","      model = tf.keras.models.Sequential([                 \n","          pre_trained_model, \n","          # tf.keras.layers.GlobalAveragePooling2D(),\n","          tf.keras.layers.Dense(hyperparameters[\"numberOfNodesInFirstDenseLayer\"], activation='relu', kernel_regularizer=regularizers.l2(hyperparameters[\"L2_lambda\"]),kernel_initializer=Heinitializer),\n","          tf.keras.layers.Dropout(hyperparameters[\"dropout_posibility\"]),\n","          tf.keras.layers.Dense(hyperparameters[\"numberOfNodesInSecondDenseLayer\"], activation='relu', kernel_regularizer=regularizers.l2(hyperparameters[\"L2_lambda\"]),kernel_initializer=Heinitializer),\n","          tf.keras.layers.Dropout(hyperparameters[\"dropout_posibility\"]),\n","          tf.keras.layers.Dense(hyperparameters[\"numberOfNodesInThirdDenseLayer\"], activation='relu', kernel_regularizer=regularizers.l2(hyperparameters[\"L2_lambda\"]),kernel_initializer=Heinitializer),\n","          tf.keras.layers.Dropout(hyperparameters[\"dropout_posibility\"]),\n","          tf.keras.layers.Dense(hyperparameters[\"num_of_classes\"], activation='softmax',kernel_initializer=Heinitializer)\n","      ]) \n","      model.build(input_shape= pre_trained_model.input.shape)\n","      model.compile(loss = tf.losses.SparseCategoricalCrossentropy(), optimizer=adamOptimizerWithCustomLearningRate, metrics=['accuracy'])\n","      # model.summary()\n","\n","\n","      return model\n","\n","hypermodel = CNNHyperModel(input_shape=(hyperparameters[\"rowShape\"], hyperparameters[\"columnShape\"]), num_classes=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7LLeAfnZdAy"},"source":["!rm -rf random_search/\n","!mkdir random_search"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sght4spumO5d"},"source":["from kerastuner.tuners import RandomSearch\n","\n","tuner = RandomSearch(\n","    hypermodel,\n","    objective='val_accuracy',\n","    max_trials=50,\n","    executions_per_trial=1,\n","    directory='random_search',\n","    project_name='Temporary'\n",")\n","tuner.search_space_summary()\n","\n","N_EPOCH_SEARCH = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8zuDaLUOmJD2"},"source":["tuner.search(x_train, y_train, epochs=N_EPOCH_SEARCH, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bB1YCJTSf6C-"},"source":["modelOutput= newModel.predict(x_test_embedings, )\n","num_classes=3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MPXLO5logmUu"},"source":["predictions=list()\n","for preidciton in modelOutput:\n","  predictions.append(np.argmax(preidciton))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFmoH12bhCsU"},"source":["predictions=np.asarray(predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJS7DBSig2zH"},"source":["predictions.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Feoy_CXjEqr"},"source":["modelOutput"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rkXNdAt4hw-9"},"source":["predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HYIoLEb3gGxW"},"source":["confusion = tf.math.confusion_matrix(labels=y_test, predictions=predictions, num_classes=num_classes)\n","confusion"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cX974Q4dn729","executionInfo":{"status":"ok","timestamp":1625135537920,"user_tz":-60,"elapsed":346,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["import os\n","from pathlib import Path\n","import gdown\n","from functools import partial\n","\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.layers import add\n","from tensorflow.keras import backend as K\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Input, concatenate\n","from tensorflow.keras.layers import Dense, Activation, Lambda, Flatten, BatchNormalization\n","from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n","from tensorflow.keras.models import load_model\n","tf_version = int(tf.__version__.split(\".\")[0])\n","from tensorflow.python.keras import backend\n","from tensorflow.python.keras.engine import training\n","from tensorflow.python.keras.utils import data_utils\n","from tensorflow.python.keras.utils import layer_utils\n","from tensorflow.python.lib.io import file_io\n","import tensorflow\n","import zipfile\n","from tensorflow.keras.layers import Convolution2D, LocallyConnected2D, Add, Dropout\n","from tensorflow.keras.layers import Concatenate\n","\n","\n","if tf_version == 1:\n","\tfrom keras.models import Model, Sequential\n","\tfrom keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n","else:\n","\tfrom tensorflow import keras\n","\tfrom tensorflow.keras.models import Model, Sequential\n","\tfrom tensorflow.keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n","\t\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxs0aYyRSFdY","executionInfo":{"status":"ok","timestamp":1625135563156,"user_tz":-60,"elapsed":23046,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["\n","\n","\n","def scaling(x, scale):\n","\treturn x * scale\n","\n","def InceptionResNetV2():\n","\t\n","\tinputs = Input(shape=(160, 160, 3))\n","\tx = Conv2D(32, 3, strides=2, padding='valid', use_bias=False, name= 'Conv2d_1a_3x3') (inputs)\n","\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_1a_3x3_BatchNorm')(x)\n","\tx = Activation('relu', name='Conv2d_1a_3x3_Activation')(x)\n","\tx = Conv2D(32, 3, strides=1, padding='valid', use_bias=False, name= 'Conv2d_2a_3x3') (x)\n","\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2a_3x3_BatchNorm')(x)\n","\tx = Activation('relu', name='Conv2d_2a_3x3_Activation')(x)\n","\tx = Conv2D(64, 3, strides=1, padding='same', use_bias=False, name= 'Conv2d_2b_3x3') (x)\n","\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_2b_3x3_BatchNorm')(x)\n","\tx = Activation('relu', name='Conv2d_2b_3x3_Activation')(x)\n","\tx = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)\n","\tx = Conv2D(80, 1, strides=1, padding='valid', use_bias=False, name= 'Conv2d_3b_1x1') (x)\n","\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_3b_1x1_BatchNorm')(x)\n","\tx = Activation('relu', name='Conv2d_3b_1x1_Activation')(x)\n","\tx = Conv2D(192, 3, strides=1, padding='valid', use_bias=False, name= 'Conv2d_4a_3x3') (x)\n","\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4a_3x3_BatchNorm')(x)\n","\tx = Activation('relu', name='Conv2d_4a_3x3_Activation')(x)\n","\tx = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name= 'Conv2d_4b_3x3') (x)\n","\tx = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Conv2d_4b_3x3_BatchNorm')(x)\n","\tx = Activation('relu', name='Conv2d_4b_3x3_Activation')(x)\n","\t\n","\t# 5x Block35 (Inception-ResNet-A block):\n","\tbranch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block35_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_1_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_1_Conv2d_0b_3x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block35_1_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n","\tbranch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_2_Conv2d_0a_1x1') (x)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n","\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_2_Conv2d_0b_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n","\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_1_Branch_2_Conv2d_0c_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_1_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_1_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n","\tbranches = [branch_0, branch_1, branch_2]\n","\tmixed = Concatenate(axis=3, name='Block35_1_Concatenate')(branches)\n","\tup = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name= 'Block35_1_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block35_1_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block35_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_1_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_1_Conv2d_0b_3x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block35_2_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n","\tbranch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_2_Conv2d_0a_1x1') (x)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n","\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_2_Conv2d_0b_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n","\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_2_Branch_2_Conv2d_0c_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_2_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_2_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n","\tbranches = [branch_0, branch_1, branch_2]\n","\tmixed = Concatenate(axis=3, name='Block35_2_Concatenate')(branches)\n","\tup = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name= 'Block35_2_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block35_2_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block35_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_1_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_1_Conv2d_0b_3x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block35_3_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n","\tbranch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_2_Conv2d_0a_1x1') (x)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n","\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_2_Conv2d_0b_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n","\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_3_Branch_2_Conv2d_0c_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_3_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_3_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n","\tbranches = [branch_0, branch_1, branch_2]\n","\tmixed = Concatenate(axis=3, name='Block35_3_Concatenate')(branches)\n","\tup = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name= 'Block35_3_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block35_3_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block35_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_1_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_1_Conv2d_0b_3x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block35_4_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n","\tbranch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_2_Conv2d_0a_1x1') (x)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n","\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_2_Conv2d_0b_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n","\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_4_Branch_2_Conv2d_0c_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_4_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_4_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n","\tbranches = [branch_0, branch_1, branch_2]\n","\tmixed = Concatenate(axis=3, name='Block35_4_Concatenate')(branches)\n","\tup = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name= 'Block35_4_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block35_4_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block35_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_1_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_1_Conv2d_0b_3x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block35_5_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n","\tbranch_2 = Conv2D(32, 1, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_2_Conv2d_0a_1x1') (x)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n","\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_2_Conv2d_0b_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n","\tbranch_2 = Conv2D(32, 3, strides=1, padding='same', use_bias=False, name= 'Block35_5_Branch_2_Conv2d_0c_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block35_5_Branch_2_Conv2d_0c_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Block35_5_Branch_2_Conv2d_0c_3x3_Activation')(branch_2)\n","\tbranches = [branch_0, branch_1, branch_2]\n","\tmixed = Concatenate(axis=3, name='Block35_5_Concatenate')(branches)\n","\tup = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name= 'Block35_5_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block35_5_Activation')(x)\n","\n","\t# Mixed 6a (Reduction-A block):\n","\tbranch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name= 'Mixed_6a_Branch_0_Conv2d_1a_3x3') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Mixed_6a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n","\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Mixed_6a_Branch_1_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, 3, strides=1, padding='same', use_bias=False, name= 'Mixed_6a_Branch_1_Conv2d_0b_3x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_0b_3x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_0b_3x3_Activation')(branch_1)\n","\tbranch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name= 'Mixed_6a_Branch_1_Conv2d_1a_3x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_6a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Mixed_6a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n","\tbranch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_6a_Branch_2_MaxPool_1a_3x3')(x)\n","\tbranches = [branch_0, branch_1, branch_pool]\n","\tx = Concatenate(axis=3, name='Mixed_6a')(branches)\n","\n","\t# 10x Block17 (Inception-ResNet-B block):\n","\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_1_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block17_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_1_Branch_1_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_1_Branch_1_Conv2d_0b_1x7') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0b_1x7_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0b_1x7_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_1_Branch_1_Conv2d_0c_7x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_1_Branch_1_Conv2d_0c_7x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_1_Branch_1_Conv2d_0c_7x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block17_1_Concatenate')(branches)\n","\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_1_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block17_1_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_2_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block17_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_2_Branch_2_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_2_Branch_2_Conv2d_0b_1x7') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0b_1x7_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0b_1x7_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_2_Branch_2_Conv2d_0c_7x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_2_Branch_2_Conv2d_0c_7x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_2_Branch_2_Conv2d_0c_7x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block17_2_Concatenate')(branches)\n","\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_2_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block17_2_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_3_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block17_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_3_Branch_3_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_3_Branch_3_Conv2d_0b_1x7') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0b_1x7_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0b_1x7_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_3_Branch_3_Conv2d_0c_7x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_3_Branch_3_Conv2d_0c_7x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_3_Branch_3_Conv2d_0c_7x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block17_3_Concatenate')(branches)\n","\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_3_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block17_3_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_4_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block17_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_4_Branch_4_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_4_Branch_4_Conv2d_0b_1x7') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0b_1x7_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0b_1x7_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_4_Branch_4_Conv2d_0c_7x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_4_Branch_4_Conv2d_0c_7x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_4_Branch_4_Conv2d_0c_7x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block17_4_Concatenate')(branches)\n","\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_4_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block17_4_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_5_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block17_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_5_Branch_5_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_5_Branch_5_Conv2d_0b_1x7') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0b_1x7_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0b_1x7_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_5_Branch_5_Conv2d_0c_7x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_5_Branch_5_Conv2d_0c_7x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_5_Branch_5_Conv2d_0c_7x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block17_5_Concatenate')(branches)\n","\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_5_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block17_5_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_6_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block17_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_6_Branch_6_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_6_Branch_6_Conv2d_0b_1x7') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0b_1x7_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0b_1x7_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_6_Branch_6_Conv2d_0c_7x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_6_Branch_6_Conv2d_0c_7x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_6_Branch_6_Conv2d_0c_7x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block17_6_Concatenate')(branches)\n","\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_6_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block17_6_Activation')(x)\t\n","\t\n","\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_7_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block17_7_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_7_Branch_7_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_7_Branch_7_Conv2d_0b_1x7') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0b_1x7_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0b_1x7_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_7_Branch_7_Conv2d_0c_7x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_7_Branch_7_Conv2d_0c_7x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_7_Branch_7_Conv2d_0c_7x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block17_7_Concatenate')(branches)\n","\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_7_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block17_7_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_8_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block17_8_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_8_Branch_8_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_8_Branch_8_Conv2d_0b_1x7') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0b_1x7_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0b_1x7_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_8_Branch_8_Conv2d_0c_7x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_8_Branch_8_Conv2d_0c_7x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_8_Branch_8_Conv2d_0c_7x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block17_8_Concatenate')(branches)\n","\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_8_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block17_8_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_9_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block17_9_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_9_Branch_9_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_9_Branch_9_Conv2d_0b_1x7') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0b_1x7_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0b_1x7_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_9_Branch_9_Conv2d_0c_7x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_9_Branch_9_Conv2d_0c_7x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_9_Branch_9_Conv2d_0c_7x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block17_9_Concatenate')(branches)\n","\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_9_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block17_9_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_10_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block17_10_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(128, 1, strides=1, padding='same', use_bias=False, name= 'Block17_10_Branch_10_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [1, 7], strides=1, padding='same', use_bias=False, name= 'Block17_10_Branch_10_Conv2d_0b_1x7') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0b_1x7_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0b_1x7_Activation')(branch_1)\n","\tbranch_1 = Conv2D(128, [7, 1], strides=1, padding='same', use_bias=False, name= 'Block17_10_Branch_10_Conv2d_0c_7x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block17_10_Branch_10_Conv2d_0c_7x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block17_10_Branch_10_Conv2d_0c_7x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block17_10_Concatenate')(branches)\n","\tup = Conv2D(896, 1, strides=1, padding='same', use_bias=True, name= 'Block17_10_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.1})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block17_10_Activation')(x)\n","\n","\t# Mixed 7a (Reduction-B block): 8 x 8 x 2080\t\n","\tbranch_0 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name= 'Mixed_7a_Branch_0_Conv2d_0a_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_0a_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_0a_1x1_Activation')(branch_0)\n","\tbranch_0 = Conv2D(384, 3, strides=2, padding='valid', use_bias=False, name= 'Mixed_7a_Branch_0_Conv2d_1a_3x3') (branch_0)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_0_Conv2d_1a_3x3_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Mixed_7a_Branch_0_Conv2d_1a_3x3_Activation')(branch_0)\n","\tbranch_1 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name= 'Mixed_7a_Branch_1_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name= 'Mixed_7a_Branch_1_Conv2d_1a_3x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_1_Conv2d_1a_3x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Mixed_7a_Branch_1_Conv2d_1a_3x3_Activation')(branch_1)\n","\tbranch_2 = Conv2D(256, 1, strides=1, padding='same', use_bias=False, name= 'Mixed_7a_Branch_2_Conv2d_0a_1x1') (x)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0a_1x1_Activation')(branch_2)\n","\tbranch_2 = Conv2D(256, 3, strides=1, padding='same', use_bias=False, name= 'Mixed_7a_Branch_2_Conv2d_0b_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_0b_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_0b_3x3_Activation')(branch_2)\n","\tbranch_2 = Conv2D(256, 3, strides=2, padding='valid', use_bias=False, name= 'Mixed_7a_Branch_2_Conv2d_1a_3x3') (branch_2)\n","\tbranch_2 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Mixed_7a_Branch_2_Conv2d_1a_3x3_BatchNorm')(branch_2)\n","\tbranch_2 = Activation('relu', name='Mixed_7a_Branch_2_Conv2d_1a_3x3_Activation')(branch_2)\n","\tbranch_pool = MaxPooling2D(3, strides=2, padding='valid', name='Mixed_7a_Branch_3_MaxPool_1a_3x3')(x)\n","\tbranches = [branch_0, branch_1, branch_2, branch_pool]\n","\tx = Concatenate(axis=3, name='Mixed_7a')(branches)\n","\n","\t# 5x Block8 (Inception-ResNet-C block):\n","\t\n","\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_1_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block8_1_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_1_Branch_1_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_1_Branch_1_Conv2d_0b_1x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_1_Branch_1_Conv2d_0c_3x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_1_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_1_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block8_1_Concatenate')(branches)\n","\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_1_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block8_1_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_2_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block8_2_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_2_Branch_2_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_2_Branch_2_Conv2d_0b_1x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0b_1x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0b_1x3_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_2_Branch_2_Conv2d_0c_3x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_2_Branch_2_Conv2d_0c_3x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_2_Branch_2_Conv2d_0c_3x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block8_2_Concatenate')(branches)\n","\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_2_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block8_2_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_3_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block8_3_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_3_Branch_3_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_3_Branch_3_Conv2d_0b_1x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0b_1x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0b_1x3_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_3_Branch_3_Conv2d_0c_3x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_3_Branch_3_Conv2d_0c_3x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_3_Branch_3_Conv2d_0c_3x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block8_3_Concatenate')(branches)\n","\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_3_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block8_3_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_4_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block8_4_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_4_Branch_4_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_4_Branch_4_Conv2d_0b_1x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0b_1x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0b_1x3_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_4_Branch_4_Conv2d_0c_3x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_4_Branch_4_Conv2d_0c_3x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_4_Branch_4_Conv2d_0c_3x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block8_4_Concatenate')(branches)\n","\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_4_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block8_4_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_5_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block8_5_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_5_Branch_5_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_5_Branch_5_Conv2d_0b_1x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0b_1x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0b_1x3_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_5_Branch_5_Conv2d_0c_3x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_5_Branch_5_Conv2d_0c_3x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_5_Branch_5_Conv2d_0c_3x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block8_5_Concatenate')(branches)\n","\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_5_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.2})(up)\n","\tx = add([x, up])\n","\tx = Activation('relu', name='Block8_5_Activation')(x)\n","\t\n","\tbranch_0 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_6_Branch_0_Conv2d_1x1') (x)\n","\tbranch_0 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_0_Conv2d_1x1_BatchNorm')(branch_0)\n","\tbranch_0 = Activation('relu', name='Block8_6_Branch_0_Conv2d_1x1_Activation')(branch_0)\n","\tbranch_1 = Conv2D(192, 1, strides=1, padding='same', use_bias=False, name= 'Block8_6_Branch_1_Conv2d_0a_1x1') (x)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0a_1x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0a_1x1_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [1, 3], strides=1, padding='same', use_bias=False, name= 'Block8_6_Branch_1_Conv2d_0b_1x3') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0b_1x3_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0b_1x3_Activation')(branch_1)\n","\tbranch_1 = Conv2D(192, [3, 1], strides=1, padding='same', use_bias=False, name= 'Block8_6_Branch_1_Conv2d_0c_3x1') (branch_1)\n","\tbranch_1 = BatchNormalization(axis=3, momentum=0.995, epsilon=0.001, scale=False, name='Block8_6_Branch_1_Conv2d_0c_3x1_BatchNorm')(branch_1)\n","\tbranch_1 = Activation('relu', name='Block8_6_Branch_1_Conv2d_0c_3x1_Activation')(branch_1)\n","\tbranches = [branch_0, branch_1]\n","\tmixed = Concatenate(axis=3, name='Block8_6_Concatenate')(branches)\n","\tup = Conv2D(1792, 1, strides=1, padding='same', use_bias=True, name= 'Block8_6_Conv2d_1x1') (mixed)\n","\tup = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 1})(up)\n","\tx = add([x, up])\n","\t\n","\t# Classification block\n","\tx = GlobalAveragePooling2D(name='AvgPool')(x)\n","\tx = Dropout(1.0 - 0.8, name='Dropout')(x)\n","\t# Bottleneck\n","\tx = Dense(128, use_bias=False, name='Bottleneck')(x)\n","\tx = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False, name='Bottleneck_BatchNorm')(x)\n","\n","\t# Create model\n","\tmodel = Model(inputs, x, name='inception_resnet_v1')\n","\n","\treturn model\n","\n","\n","\n","\n","#---------------------------------------\n","\n","def vggbaseModel():\n","\tmodel = Sequential()\n","\tmodel.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n","\tmodel.add(Convolution2D(64, (3, 3), activation='relu'))\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(64, (3, 3), activation='relu'))\n","\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n","\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(128, (3, 3), activation='relu'))\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(128, (3, 3), activation='relu'))\n","\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n","\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n","\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n","\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n","\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n","\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n","\tmodel.add(ZeroPadding2D((1,1)))\n","\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n","\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n","\n","\tmodel.add(Convolution2D(4096, (7, 7), activation='relu'))\n","\tmodel.add(Dropout(0.5))\n","\tmodel.add(Convolution2D(4096, (1, 1), activation='relu'))\n","\tmodel.add(Dropout(0.5))\n","\tmodel.add(Convolution2D(2622, (1, 1)))\n","\tmodel.add(Flatten())\n","\tmodel.add(Activation('softmax'))\n","\t\n","\treturn model\n","\n","\n","\n","#---------------------------------------\n","\n","def openFaceModel():\n","  myInput = Input(shape=(96, 96, 3))\n","\n","  x = ZeroPadding2D(padding=(3, 3), input_shape=(96, 96, 3))(myInput)\n","  x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\n","  x = BatchNormalization(axis=3, epsilon=0.00001, name='bn1')(x)\n","  x = Activation('relu')(x)\n","  x = ZeroPadding2D(padding=(1, 1))(x)\n","  x = MaxPooling2D(pool_size=3, strides=2)(x)\n","  x = Lambda(lambda x: tf.nn.lrn(x, alpha=1e-4, beta=0.75), name='lrn_1')(x)\n","  x = Conv2D(64, (1, 1), name='conv2')(x)\n","  x = BatchNormalization(axis=3, epsilon=0.00001, name='bn2')(x)\n","  x = Activation('relu')(x)\n","  x = ZeroPadding2D(padding=(1, 1))(x)\n","  x = Conv2D(192, (3, 3), name='conv3')(x)\n","  x = BatchNormalization(axis=3, epsilon=0.00001, name='bn3')(x)\n","  x = Activation('relu')(x)\n","  x = Lambda(lambda x: tf.nn.lrn(x, alpha=1e-4, beta=0.75), name='lrn_2')(x) #x is equal added\n","  x = ZeroPadding2D(padding=(1, 1))(x)\n","  x = MaxPooling2D(pool_size=3, strides=2)(x)\n","\n","  # Inception3a\n","  inception_3a_3x3 = Conv2D(96, (1, 1), name='inception_3a_3x3_conv1')(x)\n","  inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn1')(inception_3a_3x3)\n","  inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n","  inception_3a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3)\n","  inception_3a_3x3 = Conv2D(128, (3, 3), name='inception_3a_3x3_conv2')(inception_3a_3x3)\n","  inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn2')(inception_3a_3x3)\n","  inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n","\n","  inception_3a_5x5 = Conv2D(16, (1, 1), name='inception_3a_5x5_conv1')(x)\n","  inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn1')(inception_3a_5x5)\n","  inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n","  inception_3a_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5)\n","  inception_3a_5x5 = Conv2D(32, (5, 5), name='inception_3a_5x5_conv2')(inception_3a_5x5)\n","  inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn2')(inception_3a_5x5)\n","  inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n","\n","  inception_3a_pool = MaxPooling2D(pool_size=3, strides=2)(x)\n","  inception_3a_pool = Conv2D(32, (1, 1), name='inception_3a_pool_conv')(inception_3a_pool)\n","  inception_3a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_pool_bn')(inception_3a_pool)\n","  inception_3a_pool = Activation('relu')(inception_3a_pool)\n","  inception_3a_pool = ZeroPadding2D(padding=((3, 4), (3, 4)))(inception_3a_pool)\n","\n","  inception_3a_1x1 = Conv2D(64, (1, 1), name='inception_3a_1x1_conv')(x)\n","  inception_3a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_1x1_bn')(inception_3a_1x1)\n","  inception_3a_1x1 = Activation('relu')(inception_3a_1x1)\n","\n","  inception_3a = concatenate([inception_3a_3x3, inception_3a_5x5, inception_3a_pool, inception_3a_1x1], axis=3)\n","\n","  # Inception3b\n","  inception_3b_3x3 = Conv2D(96, (1, 1), name='inception_3b_3x3_conv1')(inception_3a)\n","  inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn1')(inception_3b_3x3)\n","  inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n","  inception_3b_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3)\n","  inception_3b_3x3 = Conv2D(128, (3, 3), name='inception_3b_3x3_conv2')(inception_3b_3x3)\n","  inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn2')(inception_3b_3x3)\n","  inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n","\n","  inception_3b_5x5 = Conv2D(32, (1, 1), name='inception_3b_5x5_conv1')(inception_3a)\n","  inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn1')(inception_3b_5x5)\n","  inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n","  inception_3b_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5)\n","  inception_3b_5x5 = Conv2D(64, (5, 5), name='inception_3b_5x5_conv2')(inception_3b_5x5)\n","  inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn2')(inception_3b_5x5)\n","  inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n","\n","  inception_3b_pool = Lambda(lambda x: x**2, name='power2_3b')(inception_3a)\n","  inception_3b_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3b_pool)\n","  inception_3b_pool = Lambda(lambda x: x*9, name='mult9_3b')(inception_3b_pool)\n","  inception_3b_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_3b')(inception_3b_pool)\n","  inception_3b_pool = Conv2D(64, (1, 1), name='inception_3b_pool_conv')(inception_3b_pool)\n","  inception_3b_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_pool_bn')(inception_3b_pool)\n","  inception_3b_pool = Activation('relu')(inception_3b_pool)\n","  inception_3b_pool = ZeroPadding2D(padding=(4, 4))(inception_3b_pool)\n","\n","  inception_3b_1x1 = Conv2D(64, (1, 1), name='inception_3b_1x1_conv')(inception_3a)\n","  inception_3b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_1x1_bn')(inception_3b_1x1)\n","  inception_3b_1x1 = Activation('relu')(inception_3b_1x1)\n","\n","  inception_3b = concatenate([inception_3b_3x3, inception_3b_5x5, inception_3b_pool, inception_3b_1x1], axis=3)\n","\n","  # Inception3c\n","  inception_3c_3x3 = Conv2D(128, (1, 1), strides=(1, 1), name='inception_3c_3x3_conv1')(inception_3b)\n","  inception_3c_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3c_3x3_bn1')(inception_3c_3x3)\n","  inception_3c_3x3 = Activation('relu')(inception_3c_3x3)\n","  inception_3c_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3c_3x3)\n","  inception_3c_3x3 = Conv2D(256, (3, 3), strides=(2, 2), name='inception_3c_3x3_conv'+'2')(inception_3c_3x3)\n","  inception_3c_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3c_3x3_bn'+'2')(inception_3c_3x3)\n","  inception_3c_3x3 = Activation('relu')(inception_3c_3x3)\n","\n","  inception_3c_5x5 = Conv2D(32, (1, 1), strides=(1, 1), name='inception_3c_5x5_conv1')(inception_3b)\n","  inception_3c_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3c_5x5_bn1')(inception_3c_5x5)\n","  inception_3c_5x5 = Activation('relu')(inception_3c_5x5)\n","  inception_3c_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3c_5x5)\n","  inception_3c_5x5 = Conv2D(64, (5, 5), strides=(2, 2), name='inception_3c_5x5_conv'+'2')(inception_3c_5x5)\n","  inception_3c_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3c_5x5_bn'+'2')(inception_3c_5x5)\n","  inception_3c_5x5 = Activation('relu')(inception_3c_5x5)\n","\n","  inception_3c_pool = MaxPooling2D(pool_size=3, strides=2)(inception_3b)\n","  inception_3c_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_3c_pool)\n","\n","  inception_3c = concatenate([inception_3c_3x3, inception_3c_5x5, inception_3c_pool], axis=3)\n","\n","  #inception 4a\n","  inception_4a_3x3 = Conv2D(96, (1, 1), strides=(1, 1), name='inception_4a_3x3_conv'+'1')(inception_3c)\n","  inception_4a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_3x3_bn'+'1')(inception_4a_3x3)\n","  inception_4a_3x3 = Activation('relu')(inception_4a_3x3)\n","  inception_4a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_4a_3x3)\n","  inception_4a_3x3 = Conv2D(192, (3, 3), strides=(1, 1), name='inception_4a_3x3_conv'+'2')(inception_4a_3x3)\n","  inception_4a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_3x3_bn'+'2')(inception_4a_3x3)\n","  inception_4a_3x3 = Activation('relu')(inception_4a_3x3)\n","\n","  inception_4a_5x5 = Conv2D(32, (1,1), strides=(1,1), name='inception_4a_5x5_conv1')(inception_3c)\n","  inception_4a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_5x5_bn1')(inception_4a_5x5)\n","  inception_4a_5x5 = Activation('relu')(inception_4a_5x5)\n","  inception_4a_5x5 = ZeroPadding2D(padding=(2,2))(inception_4a_5x5)\n","  inception_4a_5x5 = Conv2D(64, (5,5), strides=(1,1), name='inception_4a_5x5_conv'+'2')(inception_4a_5x5)\n","  inception_4a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_5x5_bn'+'2')(inception_4a_5x5)\n","  inception_4a_5x5 = Activation('relu')(inception_4a_5x5)\n","\n","  inception_4a_pool = Lambda(lambda x: x**2, name='power2_4a')(inception_3c)\n","  inception_4a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_4a_pool)\n","  inception_4a_pool = Lambda(lambda x: x*9, name='mult9_4a')(inception_4a_pool)\n","  inception_4a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_4a')(inception_4a_pool)\n","\n","  inception_4a_pool = Conv2D(128, (1,1), strides=(1,1), name='inception_4a_pool_conv'+'')(inception_4a_pool)\n","  inception_4a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_pool_bn'+'')(inception_4a_pool)\n","  inception_4a_pool = Activation('relu')(inception_4a_pool)\n","  inception_4a_pool = ZeroPadding2D(padding=(2, 2))(inception_4a_pool)\n","\n","  inception_4a_1x1 = Conv2D(256, (1, 1), strides=(1, 1), name='inception_4a_1x1_conv'+'')(inception_3c)\n","  inception_4a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4a_1x1_bn'+'')(inception_4a_1x1)\n","  inception_4a_1x1 = Activation('relu')(inception_4a_1x1)\n","\n","  inception_4a = concatenate([inception_4a_3x3, inception_4a_5x5, inception_4a_pool, inception_4a_1x1], axis=3)\n","\n","  #inception4e\n","  inception_4e_3x3 = Conv2D(160, (1,1), strides=(1,1), name='inception_4e_3x3_conv'+'1')(inception_4a)\n","  inception_4e_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4e_3x3_bn'+'1')(inception_4e_3x3)\n","  inception_4e_3x3 = Activation('relu')(inception_4e_3x3)\n","  inception_4e_3x3 = ZeroPadding2D(padding=(1, 1))(inception_4e_3x3)\n","  inception_4e_3x3 = Conv2D(256, (3,3), strides=(2,2), name='inception_4e_3x3_conv'+'2')(inception_4e_3x3)\n","  inception_4e_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4e_3x3_bn'+'2')(inception_4e_3x3)\n","  inception_4e_3x3 = Activation('relu')(inception_4e_3x3)\n","\n","  inception_4e_5x5 = Conv2D(64, (1,1), strides=(1,1), name='inception_4e_5x5_conv'+'1')(inception_4a)\n","  inception_4e_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4e_5x5_bn'+'1')(inception_4e_5x5)\n","  inception_4e_5x5 = Activation('relu')(inception_4e_5x5)\n","  inception_4e_5x5 = ZeroPadding2D(padding=(2, 2))(inception_4e_5x5)\n","  inception_4e_5x5 = Conv2D(128, (5,5), strides=(2,2), name='inception_4e_5x5_conv'+'2')(inception_4e_5x5)\n","  inception_4e_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_4e_5x5_bn'+'2')(inception_4e_5x5)\n","  inception_4e_5x5 = Activation('relu')(inception_4e_5x5)\n","\n","  inception_4e_pool = MaxPooling2D(pool_size=3, strides=2)(inception_4a)\n","  inception_4e_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_4e_pool)\n","\n","  inception_4e = concatenate([inception_4e_3x3, inception_4e_5x5, inception_4e_pool], axis=3)\n","\n","  #inception5a\n","  inception_5a_3x3 = Conv2D(96, (1,1), strides=(1,1), name='inception_5a_3x3_conv'+'1')(inception_4e)\n","  inception_5a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5a_3x3_bn'+'1')(inception_5a_3x3)\n","  inception_5a_3x3 = Activation('relu')(inception_5a_3x3)\n","  inception_5a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_5a_3x3)\n","  inception_5a_3x3 = Conv2D(384, (3,3), strides=(1,1), name='inception_5a_3x3_conv'+'2')(inception_5a_3x3)\n","  inception_5a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5a_3x3_bn'+'2')(inception_5a_3x3)\n","  inception_5a_3x3 = Activation('relu')(inception_5a_3x3)\n","\n","  inception_5a_pool = Lambda(lambda x: x**2, name='power2_5a')(inception_4e)\n","  inception_5a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_5a_pool)\n","  inception_5a_pool = Lambda(lambda x: x*9, name='mult9_5a')(inception_5a_pool)\n","  inception_5a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_5a')(inception_5a_pool)\n","\n","  inception_5a_pool = Conv2D(96, (1,1), strides=(1,1), name='inception_5a_pool_conv'+'')(inception_5a_pool)\n","  inception_5a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5a_pool_bn'+'')(inception_5a_pool)\n","  inception_5a_pool = Activation('relu')(inception_5a_pool)\n","  inception_5a_pool = ZeroPadding2D(padding=(1,1))(inception_5a_pool)\n","\n","  inception_5a_1x1 = Conv2D(256, (1,1), strides=(1,1), name='inception_5a_1x1_conv'+'')(inception_4e)\n","  inception_5a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5a_1x1_bn'+'')(inception_5a_1x1)\n","  inception_5a_1x1 = Activation('relu')(inception_5a_1x1)\n","\n","  inception_5a = concatenate([inception_5a_3x3, inception_5a_pool, inception_5a_1x1], axis=3)\n","\n","  #inception_5b\n","  inception_5b_3x3 = Conv2D(96, (1,1), strides=(1,1), name='inception_5b_3x3_conv'+'1')(inception_5a)\n","  inception_5b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5b_3x3_bn'+'1')(inception_5b_3x3)\n","  inception_5b_3x3 = Activation('relu')(inception_5b_3x3)\n","  inception_5b_3x3 = ZeroPadding2D(padding=(1,1))(inception_5b_3x3)\n","  inception_5b_3x3 = Conv2D(384, (3,3), strides=(1,1), name='inception_5b_3x3_conv'+'2')(inception_5b_3x3)\n","  inception_5b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5b_3x3_bn'+'2')(inception_5b_3x3)\n","  inception_5b_3x3 = Activation('relu')(inception_5b_3x3)\n","\n","  inception_5b_pool = MaxPooling2D(pool_size=3, strides=2)(inception_5a)\n","\n","  inception_5b_pool = Conv2D(96, (1,1), strides=(1,1), name='inception_5b_pool_conv'+'')(inception_5b_pool)\n","  inception_5b_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5b_pool_bn'+'')(inception_5b_pool)\n","  inception_5b_pool = Activation('relu')(inception_5b_pool)\n","\n","  inception_5b_pool = ZeroPadding2D(padding=(1, 1))(inception_5b_pool)\n","\n","  inception_5b_1x1 = Conv2D(256, (1,1), strides=(1,1), name='inception_5b_1x1_conv'+'')(inception_5a)\n","  inception_5b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_5b_1x1_bn'+'')(inception_5b_1x1)\n","  inception_5b_1x1 = Activation('relu')(inception_5b_1x1)\n","\n","  inception_5b = concatenate([inception_5b_3x3, inception_5b_pool, inception_5b_1x1], axis=3)\n","\n","  av_pool = AveragePooling2D(pool_size=(3, 3), strides=(1, 1))(inception_5b)\n","  reshape_layer = Flatten()(av_pool)\n","  dense_layer = Dense(128, name='dense_layer')(reshape_layer)\n","  norm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')(dense_layer)\n","\n","  # Final Model\n","  model = Model(inputs=[myInput], outputs=norm_layer)\n","  return model\n","\n","\n","def deepFaceModel():\n","  base_model = Sequential()\n","  base_model.add(Convolution2D(32, (11, 11), activation='relu', name='C1', input_shape=(152, 152, 3)))\n","  base_model.add(MaxPooling2D(pool_size=3, strides=2, padding='same', name='M2'))\n","  base_model.add(Convolution2D(16, (9, 9), activation='relu', name='C3'))\n","  base_model.add(LocallyConnected2D(16, (9, 9), activation='relu', name='L4'))\n","  base_model.add(LocallyConnected2D(16, (7, 7), strides=2, activation='relu', name='L5') )\n","  base_model.add(LocallyConnected2D(16, (5, 5), activation='relu', name='L6'))\n","  base_model.add(Flatten(name='F0'))\n","  base_model.add(Dense(4096, activation='relu', name='F7'))\n","  base_model.add(Dropout(rate=0.5, name='D0'))\n","  base_model.add(Dense(8631, activation='softmax', name='F8'))\n","  return base_model\n","\t\n","\t#---------------------------------\n","def deepIDModel():\n","  myInput = Input(shape=(55, 47, 3))\n","\n","  x = Conv2D(20, (4, 4), name='Conv1', activation='relu', input_shape=(55, 47, 3))(myInput)\n","  x = MaxPooling2D(pool_size=2, strides=2, name='Pool1')(x)\n","  x = Dropout(rate=0.99, name='D1')(x)\n","\n","  x = Conv2D(40, (3, 3), name='Conv2', activation='relu')(x)\n","  x = MaxPooling2D(pool_size=2, strides=2, name='Pool2')(x)\n","  x = Dropout(rate=0.99, name='D2')(x)\n","\n","  x = Conv2D(60, (3, 3), name='Conv3', activation='relu')(x)\n","  x = MaxPooling2D(pool_size=2, strides=2, name='Pool3')(x)\n","  x = Dropout(rate=0.99, name='D3')(x)\n","\n","  x1 = Flatten()(x)\n","  fc11 = Dense(160, name = 'fc11')(x1)\n","\n","  x2 = Conv2D(80, (2, 2), name='Conv4', activation='relu')(x)\n","  x2 = Flatten()(x2)\n","  fc12 = Dense(160, name = 'fc12')(x2)\n","\n","  y = Add()([fc11, fc12])\n","  y = Activation('relu', name = 'deepid')(y)\n","\n","  model = Model(inputs=[myInput], outputs=y)\n","  return model\n","\n","\t\n","def ResNet34():\n","\n","  img_input = tensorflow.keras.layers.Input(shape=(112, 112, 3))\n","\n","  x = tensorflow.keras.layers.ZeroPadding2D(padding=1, name='conv1_pad')(img_input)\n","  x = tensorflow.keras.layers.Conv2D(64, 3, strides=1, use_bias=False, kernel_initializer='glorot_normal', name='conv1_conv')(x)\n","  x = tensorflow.keras.layers.BatchNormalization(axis=3, epsilon=2e-5, momentum=0.9, name='conv1_bn')(x)\n","  x = tensorflow.keras.layers.PReLU(shared_axes=[1, 2], name='conv1_prelu')(x)\n","  x = stack_fn(x)\n","\n","  model = training.Model(img_input, x, name='ResNet34')\n","\n","  return model\n","\n","\n","\n","def block1(x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None):\n","  bn_axis = 3\n","\n","  if conv_shortcut:\n","    shortcut = tensorflow.keras.layers.Conv2D(filters, 1, strides=stride, use_bias=False, kernel_initializer='glorot_normal', name=name + '_0_conv')(x)\n","    shortcut = tensorflow.keras.layers.BatchNormalization(axis=bn_axis, epsilon=2e-5, momentum=0.9, name=name + '_0_bn')(shortcut)\n","  else:\n","    shortcut = x\n","\n","  x = tensorflow.keras.layers.BatchNormalization(axis=bn_axis, epsilon=2e-5, momentum=0.9, name=name + '_1_bn')(x)\n","  x = tensorflow.keras.layers.ZeroPadding2D(padding=1, name=name + '_1_pad')(x)\n","  x = tensorflow.keras.layers.Conv2D(filters, 3, strides=1, kernel_initializer='glorot_normal', use_bias=False, name=name + '_1_conv')(x)\n","  x = tensorflow.keras.layers.BatchNormalization(axis=bn_axis, epsilon=2e-5, momentum=0.9, name=name + '_2_bn')(x)\n","  x = tensorflow.keras.layers.PReLU(shared_axes=[1, 2], name=name + '_1_prelu')(x)\n","\n","  x = tensorflow.keras.layers.ZeroPadding2D(padding=1, name=name + '_2_pad')(x)\n","  x = tensorflow.keras.layers.Conv2D(filters, kernel_size, strides=stride, kernel_initializer='glorot_normal', use_bias=False, name=name + '_2_conv')(x)\n","  x = tensorflow.keras.layers.BatchNormalization(axis=bn_axis, epsilon=2e-5, momentum=0.9, name=name + '_3_bn')(x)\n","\n","  x = tensorflow.keras.layers.Add(name=name + '_add')([shortcut, x])\n","  return x\n","\n","def stack1(x, filters, blocks, stride1=2, name=None):\n","  x = block1(x, filters, stride=stride1, name=name + '_block1')\n","  for i in range(2, blocks + 1):\n","    x = block1(x, filters, conv_shortcut=False, name=name + '_block' + str(i))\n","  return x\n","\n","def stack_fn(x):\n","  x = stack1(x, 64, 3, name='conv2')\n","  x = stack1(x, 128, 4, name='conv3')\n","  x = stack1(x, 256, 6, name='conv4')\n","  return stack1(x, 512, 3, name='conv5')\n","\n","def arcFaceModel():\n","  base_model = ResNet34()\n","  inputs = base_model.inputs[0]\n","  arcface_model = base_model.outputs[0]\n","  arcface_model = keras.layers.BatchNormalization(momentum=0.9, epsilon=2e-5)(arcface_model)\n","  arcface_model = keras.layers.Dropout(0.4)(arcface_model)\n","  arcface_model = keras.layers.Flatten()(arcface_model)\n","  arcface_model = keras.layers.Dense(512, activation=None, use_bias=True, kernel_initializer=\"glorot_normal\")(arcface_model)\n","  embedding = keras.layers.BatchNormalization(momentum=0.9, epsilon=2e-5, name=\"embedding\", scale=True)(arcface_model)\n","  model = keras.models.Model(inputs, embedding, name=base_model.name)\n","  return model\n","\n","\n","\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"AI7vX74oSFha","executionInfo":{"status":"ok","timestamp":1625135563156,"user_tz":-60,"elapsed":24,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["\n","def putResizeLayerInModel(model, actualInputShape=(160, 160, 3)):\n","  input_size=functions.find_input_shape(model)\n","\n","  toBeModel=tf.keras.Sequential()\n","  toBeModel.add(Input(shape=actualInputShape))\n","  toBeModel.add(tf.keras.layers.experimental.preprocessing.Resizing(height=input_size[0],\n","                                                                width=input_size[1]))\n","  toBeModel.add(model)\n","  return toBeModel\n","\n","def loadAModel(basemodel,nameOfWheights):\n","  home = \"/content/drive/MyDrive/Models_Herodotou/\"\n","  model = basemodel\n","  output = home+nameOfWheights\n","  model.load_weights(output)\n","  return model\n","\n","\n","def loadFaceNetModel(useResizeLayerInModel=False):\n","  model=loadAModel(basemodel=InceptionResNetV2(),nameOfWheights='facenet_weights.h5')\n","  if(useResizeLayerInModel==True):\n","    model=putResizeLayerInModel(model)\n","  return model\n","\n","def loadVGGFaceModel(useResizeLayerInModel=False):\n","  model = loadAModel(basemodel=vggbaseModel(),nameOfWheights='vgg_face_weights.h5')\n","  model= Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\n","  if(useResizeLayerInModel==True):\n","    model=putResizeLayerInModel(model)\n","  return model\n","\n","def loadOpenFaceModel(useResizeLayerInModel=False):\n","  model=loadAModel(basemodel=openFaceModel(),nameOfWheights='openface_weights.h5')\n","  if(useResizeLayerInModel==True):\n","    model=putResizeLayerInModel(model)\n","  return model\n","\n","def loadDeepFaceModel(useResizeLayerInModel=False):\n","  base_model=  loadAModel(basemodel=deepFaceModel(),nameOfWheights='VGGFace2_DeepFace_weights_val-0.9034.h5')\n","  base_model= Model(inputs=base_model.layers[0].input, outputs=base_model.layers[-3].output)\n","  if(useResizeLayerInModel==True):\n","    base_model=putResizeLayerInModel(base_model)\n","  return base_model\n","\n","def loadDeepIDModel(useResizeLayerInModel=False):\n","  model= loadAModel(basemodel=deepIDModel(),nameOfWheights='deepid_keras_weights.h5')\n","  if(useResizeLayerInModel==True):\n","    model=putResizeLayerInModel(model)\n","  return model\n","\n","def loadArcFaceModel(useResizeLayerInModel=False):\n","  model= loadAModel(basemodel=arcFaceModel(),nameOfWheights='arcface_weights.h5')\n","  if(useResizeLayerInModel==True):\n","    model=putResizeLayerInModel(model)\n","  return model"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OTSKKG1UqeQ9","executionInfo":{"status":"ok","timestamp":1625135563157,"user_tz":-60,"elapsed":23,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"c5484f9e-0bd6-4794-d475-3ebe2c55b9d2"},"source":["!ls"],"execution_count":14,"outputs":[{"output_type":"stream","text":["drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_kQC2tl0qeVi","executionInfo":{"status":"ok","timestamp":1625135563157,"user_tz":-60,"elapsed":16,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"ec57c460-ec14-47c3-bf43-fa7eaa7c8bb8"},"source":["%cd ..\n","%cd ..\n","%cd ..\n","%cd ..\n","%cd ..\n","%cd .."],"execution_count":15,"outputs":[{"output_type":"stream","text":["/\n","/\n","/\n","/\n","/\n","/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HWZ73jtR8v3Q","outputId":"ae489998-bd15-47a8-bd1e-770997df648a"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mbin\u001b[0m/      \u001b[01;34mdatalab\u001b[0m/  \u001b[01;34mhome\u001b[0m/   \u001b[01;34mlib64\u001b[0m/  \u001b[01;34mopt\u001b[0m/   \u001b[01;34mrun\u001b[0m/   \u001b[01;34msys\u001b[0m/                \u001b[01;34mtools\u001b[0m/\n","\u001b[01;34mboot\u001b[0m/     \u001b[01;34mdev\u001b[0m/      \u001b[01;34mlib\u001b[0m/    \u001b[01;34mmedia\u001b[0m/  \u001b[01;34mproc\u001b[0m/  \u001b[01;34msbin\u001b[0m/  \u001b[01;34mtensorflow-1.15.2\u001b[0m/  \u001b[01;34musr\u001b[0m/\n","\u001b[01;34mcontent\u001b[0m/  \u001b[01;34metc\u001b[0m/      \u001b[01;34mlib32\u001b[0m/  \u001b[01;34mmnt\u001b[0m/    \u001b[01;34mroot\u001b[0m/  \u001b[01;34msrv\u001b[0m/   \u001b[30;42mtmp\u001b[0m/                \u001b[01;34mvar\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ct8USyHdHHbQ"},"source":["numberOftrainingPictures= !ls /content/drive/MyDrive/AttracivenessRegression/Training | wc -l\n","numberOfTestPictures= !ls /content/drive/MyDrive/AttracivenessRegression/Test | wc -l\n","\n","numberOftrainingPictures= int(numberOftrainingPictures[0])\n","numberOfTestPictures= int(numberOfTestPictures[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98xDDKS9HSLW","outputId":"40783ec6-1f94-40c1-86c3-569fd10f8e83"},"source":["print(numberOftrainingPictures)\n","print(numberOfTestPictures)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["8340\n","180\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GRb7bI7PqkVO","executionInfo":{"status":"ok","timestamp":1625135563574,"user_tz":-60,"elapsed":4,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["base_directory = r\"/content/drive/MyDrive/AttracivenessRegression/\"\n","\n","def readData(numberOfPictures,dirName):\n","  sizeOfInput=(numberOfPictures,160,160,3)\n","  x= np.empty(shape=sizeOfInput,dtype=np.uint8)\n","  y= np.empty(shape=(numberOfPictures,))\n","  aug= list()\n","\n","  directory=base_directory+ dirName+ \"/\"\n","  print(directory)\n","  for index, filename in enumerate(os.listdir(directory)):\n","\n","    img= cv2.imread(directory+filename)\n","    try:\n","      img = img[..., ::-1]\n","      img=img.astype(\"uint8\")\n","    except Exception as e: \n","      print(\"FUUUUCK\\n\"+ filename )\n","      print(e)\n","      continue\n","\n","\n","    rest= filename.split(\"kostis_\")[1]\n","    rest=rest.split(\"_\")\n","    aug_type=rest[0]\n","\n","    label=rest[1] \n","    label= float(label.split(\".jp\")[0])\n","\n","    x[index]=img\n","    y[index]=label\n","    aug.append(aug_type)\n","  \n","  return x,y,aug"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcA9B5BTrjpj","executionInfo":{"status":"ok","timestamp":1625135563575,"user_tz":-60,"elapsed":4,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["import os\n","\n","def findNumberOfItemsInDirectory(directoryPath):\n","\n","  totalFiles = 0\n","  totalDir = 0\n","\n","  for base, dirs, files in os.walk(directoryPath):\n","      print('Searching in : ',base)\n","      for Files in files:\n","          totalFiles += 1\n","\n","  return totalFiles\n","  "],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7Sv8Xu8qlIx","executionInfo":{"status":"ok","timestamp":1625135563575,"user_tz":-60,"elapsed":3,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["def readDataFromSingleFolder(dirPath):\n","  numberOfPictures= findNumberOfItemsInDirectory(dirPath)\n","  sizeOfInput=(numberOfPictures,160,160,3)\n","  x= np.empty(shape=sizeOfInput,dtype=np.uint8)\n","  y= np.empty(shape=(numberOfPictures,))\n","  aug= list()\n","\n","  directory=dirPath+ \"/\"\n","  print(directory)\n","  for index, filename in enumerate(os.listdir(directory)):\n","\n","    img= cv2.imread(directory+filename)\n","    try:\n","      img = img[..., ::-1]\n","      img=img.astype(\"uint8\")\n","    except Exception as e: \n","      print(\"FUUUUCK\\n\"+ filename )\n","      print(e)\n","      continue\n","\n","\n","    rest= filename.split(\"kostis_\")[1]\n","    rest=rest.split(\"_\")\n","    aug_type=rest[0]\n","\n","    label=rest[1] \n","    label= float(label.split(\".jp\")[0])\n","\n","    x[index]=img\n","    y[index]=label\n","    aug.append(aug_type)\n","  \n","  return x,y,aug"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"9iJtEHZTadgK","executionInfo":{"status":"ok","timestamp":1625135585653,"user_tz":-60,"elapsed":323,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["base_directory = r\"/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/\"\n","def readFRImages(listOfDesiredAugmentation, directory_plug_in=\"\"):\n","  totalX=[]\n","  totalY=[]\n","\n","  for directory in listOfDesiredAugmentation:\n","    x,y,aug= readDataFromSingleFolder(base_directory+directory_plug_in+ directory)\n","    totalX.append(x)\n","    totalY.append(y)\n","\n","  images= np.concatenate(totalX, axis=0)\n","  labels= np.concatenate(totalY, axis=0)\n","\n","  return images,labels"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpLkSxOFqkVP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625136640463,"user_tz":-60,"elapsed":1032797,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"b5ed28ec-3f10-4b11-bca3-035a6d103081"},"source":["x_train_all, y_train_all =readFRImages(listOfDesiredAugmentation=listOfDesiredAugmentation\n","                                             ,directory_plug_in=\"Training/\")"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/HF\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/HF/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/None\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/None/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/RO\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/RO/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/HF-RC\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/HF-RC/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/None-RC\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/None-RC/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/None-TR\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/None-TR/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/HF-TR\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Training/HF-TR/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_iwX394qkVQ","executionInfo":{"status":"ok","timestamp":1625136680876,"user_tz":-60,"elapsed":40428,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"f5f4bf6b-599c-4dbb-c75d-8d36978dcfad"},"source":["x_test, y_test=readFRImages(listOfDesiredAugmentation=listOfDesiredAugmentation\n","                                             ,directory_plug_in=\"Test/\")"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/HF\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/HF/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/None\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/None/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/RO\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/RO/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/HF-RC\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/HF-RC/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/None-RC\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/None-RC/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/None-TR\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/None-TR/\n","Searching in :  /content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/HF-TR\n","/content/drive/MyDrive/TrustworthyRegression/FaceRecognisionImages/Test/HF-TR/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3ywb_8UBz6Qu","executionInfo":{"status":"ok","timestamp":1625136680877,"user_tz":-60,"elapsed":7,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["from deepface.commons import functions"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-vdpWOdqUrf"},"source":["# Classifier = tf.keras.models.Sequential([                 \n","#     # pre_trained_model, \n","#     # tf.keras.layers.GlobalAveragePooling2D(),\n","#     tf.keras.Input(shape=(training_embedings[0].shape[0],)),\n","#     normalizer,\n","#     tf.keras.layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","#     tf.keras.layers.Dropout(dropout_posibility),\n","#     tf.keras.layers.Dense(4, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","#     tf.keras.layers.Dropout(dropout_posibility),\n","#     tf.keras.layers.Dense(3, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","#     tf.keras.layers.Dropout(dropout_posibility),\n","#     tf.keras.layers.Dense(1,kernel_initializer=Heinitializer),\n","#     ]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1fOUISUqUpT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lVr5YkFdqUl8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3NXpejGhoFOY","executionInfo":{"status":"ok","timestamp":1625136753400,"user_tz":-60,"elapsed":72529,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["facenet= loadFaceNetModel(useResizeLayerInModel=True)\n","vggface= loadVGGFaceModel(useResizeLayerInModel=True)   #Use this one\n","openface=loadOpenFaceModel(useResizeLayerInModel=True)\n","deepface=loadDeepFaceModel(useResizeLayerInModel=True)\n","deepID=loadDeepIDModel(useResizeLayerInModel=True)      #Use this one\n","arcFace=loadArcFaceModel(useResizeLayerInModel=True)\n","\n"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"xDs4TL1dpo61","executionInfo":{"status":"ok","timestamp":1625136753402,"user_tz":-60,"elapsed":8,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["facenet.trainable=False;\n","vggface.trainable=False;\n","openface.trainable=False;\n","deepface.trainable=False;\n","deepID.trainable=False;\n","arcFace.trainable=False;\n"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mc515GcTluqq"},"source":["# **HERE!**"]},{"cell_type":"code","metadata":{"id":"_cSgH1-xfF6M","executionInfo":{"status":"ok","timestamp":1625135600925,"user_tz":-60,"elapsed":344,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["# listOfDesiredAugmentation=['HF', 'None', 'RO', 'RO-RC', 'HF-RC', 'RO-TR', 'None-RC', 'None-TR', 'HF-TR']\n","# listOfDesiredAugmentation=['HF', 'None', \"RO\"]  # Best results so far\n","listOfDesiredAugmentation=['HF', 'None', 'RO', 'HF-RC', 'None-RC', 'None-TR', 'HF-TR']\n","\n","# listOfDesiredModelNames=[\"facenet\",\"vggface\",\"openface\", \"deepface\",\"deepID\", \"arcFace\"]    #all \n","# listOfDesiredModelNames=[\"facenet\",\"vggface\",\"openface\",\"deepID\", \"deepface\"]                # best performance \n","listOfDesiredModelNames=[ \"facenet\"]                # try minimal stuff  "],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aCU5Q5RFh8Ms","executionInfo":{"status":"ok","timestamp":1625136756155,"user_tz":-60,"elapsed":10,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"fc03b675-943d-49e6-ac34-81be67a285dc"},"source":["%cd /content/drive/MyDrive/TrustworthyRegression/FaceEncodings/Training"],"execution_count":29,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/TrustworthyRegression/FaceEncodings/Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PXYDCPWSWMWN","executionInfo":{"status":"ok","timestamp":1625136756517,"user_tz":-60,"elapsed":10,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"9799525a-02e0-460a-88f0-3c1430e0a4a4"},"source":["%cd .. \n","%cd Training"],"execution_count":30,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/TrustworthyRegression/FaceEncodings\n","/content/drive/MyDrive/TrustworthyRegression/FaceEncodings/Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hZdHa-zZ9QQj","executionInfo":{"status":"ok","timestamp":1625136759633,"user_tz":-60,"elapsed":3118,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["training_embedings, training_labels= readEmbedings(listOfDesiredAugmentation=listOfDesiredAugmentation,\n","                                 listOfDesiredModelNames=listOfDesiredModelNames)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cur8O27ironv","executionInfo":{"status":"ok","timestamp":1625136759634,"user_tz":-60,"elapsed":21,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"02d03ead-0cc2-4a92-f8e9-d728311f715c"},"source":["training_embedings[0].shape[0]"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["128"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdOR7VM-fuX7","executionInfo":{"status":"ok","timestamp":1625136759634,"user_tz":-60,"elapsed":7,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"ee2cc380-431a-4c0d-a80c-097e4a988f99"},"source":["%cd ..\n","%cd Test"],"execution_count":33,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/TrustworthyRegression/FaceEncodings\n","/content/drive/My Drive/TrustworthyRegression/FaceEncodings/Test\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"71nnys9D707G","executionInfo":{"status":"ok","timestamp":1625136759635,"user_tz":-60,"elapsed":5,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["# listOfDesiredAugmentation=['HF', 'None', 'RO', 'RO-RC', 'HF-RC', 'RO-TR', 'None-RC', 'None-TR', 'HF-TR']\n","listOfDesiredAugmentation=['None',]"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"XmgrzOJi7rt6","executionInfo":{"status":"ok","timestamp":1625136760500,"user_tz":-60,"elapsed":869,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["test_embedings, test_labels= readEmbedings(listOfDesiredAugmentation=listOfDesiredAugmentation,\n","                                 listOfDesiredModelNames=listOfDesiredModelNames)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"d5BShMSDSGFi","executionInfo":{"status":"ok","timestamp":1625136760500,"user_tz":-60,"elapsed":3,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["from tensorflow.keras import regularizers\n","Heinitializer = tf.keras.initializers.HeNormal()\n","from tensorflow.keras.layers.experimental import preprocessing\n","\n"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"J91wWBovriQJ","executionInfo":{"status":"ok","timestamp":1625136761026,"user_tz":-60,"elapsed":528,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}}},"source":["from tensorflow.keras.layers.experimental import preprocessing\n","normalizer = preprocessing.Normalization(axis=-1)\n","normalizer.adapt(training_embedings)\n","# print(normalizer.mean.numpy())\n"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bpu62TMX8HFy"},"source":["dropout_posibility=0.06\n","L2_lambda=0.0004\n","init_learning_rate=0.001\n","newModel = tf.keras.models.Sequential([                 \n","    # pre_trained_model, \n","    # tf.keras.layers.GlobalAveragePooling2D(),\n","    tf.keras.Input(shape=(training_embedings[0].shape[0],)),\n","    normalizer,\n","    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","    tf.keras.layers.Dropout(dropout_posibility),\n","    tf.keras.layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","    tf.keras.layers.Dropout(dropout_posibility),\n","    tf.keras.layers.Dense(4, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","    tf.keras.layers.Dropout(dropout_posibility),\n","    tf.keras.layers.Dense(1,kernel_initializer=Heinitializer),\n","    ]) \n","newModel.summary()\n","\n","stathera kato apo 17%\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AO7UF-TIzYnb","executionInfo":{"status":"ok","timestamp":1625137740216,"user_tz":-60,"elapsed":382,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"b417a0f9-7b8a-4c38-c0cd-09b0bc66a6fc"},"source":["# dropout_posibility=0.05\n","# L2_lambda=0.0003\n","\n","dropout_posibility=0.01\n","L2_lambda=0.00001\n","\n","init_learning_rate=0.001\n","newModel = tf.keras.models.Sequential([                 \n","    # pre_trained_model, \n","    # tf.keras.layers.GlobalAveragePooling2D(),\n","    tf.keras.Input(shape=(training_embedings[0].shape[0],)),\n","    normalizer,\n","    # tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","    # tf.keras.layers.Dropout(dropout_posibility),\n","    # tf.keras.layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","    # tf.keras.layers.Dropout(dropout_posibility),\n","    # tf.keras.layers.Dense(4, activation='relu', kernel_regularizer=regularizers.l2(L2_lambda),kernel_initializer=Heinitializer),\n","    # tf.keras.layers.Dropout(dropout_posibility),\n","    tf.keras.layers.Dense(1,kernel_initializer=Heinitializer),\n","    ]) \n","newModel.summary()"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Model: \"sequential_12\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","normalization (Normalization (None, 128)               257       \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 1)                 129       \n","=================================================================\n","Total params: 386\n","Trainable params: 129\n","Non-trainable params: 257\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"o9Op31p9SGFm","executionInfo":{"status":"ok","timestamp":1625138029783,"user_tz":-60,"elapsed":276418,"user":{"displayName":"Stylianos Herodotou","photoUrl":"","userId":"11396829892484379326"}},"outputId":"281996f0-d89b-420a-b09d-79cb4fed9ca5"},"source":["numberEpochsPerIteration=250\n","numberOfIterations=4\n","for i in range(1, numberOfIterations+1):\n","  learning_rate=init_learning_rate/(i*10)\n","  adamOptimizerWithCustomLearningRate =tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","  newModel.compile(\n","    optimizer=adamOptimizerWithCustomLearningRate,\n","    # loss=tf.keras.losses.MeanAbsoluteError())\n","  loss= tf.keras.losses.MeanAbsolutePercentageError())\n","\n","\n","    # loss=tf.keras.losses.MeanSquaredError())\n","  \n","  history= newModel.fit(\n","    training_embedings,\n","    training_labels,\n","    validation_data=(test_embedings, test_labels),\n","  # train_ds,\n","  # validation_data=val_ds,\n","  epochs=numberEpochsPerIteration\n","  )\n","  plot_loss(history)\n","\n","\n","\n","\n"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Epoch 1/250\n","157/157 [==============================] - 1s 2ms/step - loss: 85.5880 - val_loss: 88.5135\n","Epoch 2/250\n","157/157 [==============================] - 0s 1ms/step - loss: 85.1009 - val_loss: 88.0411\n","Epoch 3/250\n","157/157 [==============================] - 0s 1ms/step - loss: 84.6162 - val_loss: 87.6157\n","Epoch 4/250\n","157/157 [==============================] - 0s 1ms/step - loss: 84.1386 - val_loss: 87.1554\n","Epoch 5/250\n","157/157 [==============================] - 0s 1ms/step - loss: 83.6569 - val_loss: 86.7377\n","Epoch 6/250\n","157/157 [==============================] - 0s 1ms/step - loss: 83.1803 - val_loss: 86.2918\n","Epoch 7/250\n","157/157 [==============================] - 0s 1ms/step - loss: 82.6977 - val_loss: 85.8974\n","Epoch 8/250\n","157/157 [==============================] - 0s 1ms/step - loss: 82.2171 - val_loss: 85.4488\n","Epoch 9/250\n","157/157 [==============================] - 0s 1ms/step - loss: 81.7375 - val_loss: 85.0124\n","Epoch 10/250\n","157/157 [==============================] - 0s 2ms/step - loss: 81.2612 - val_loss: 84.5812\n","Epoch 11/250\n","157/157 [==============================] - 0s 2ms/step - loss: 80.7911 - val_loss: 84.1496\n","Epoch 12/250\n","157/157 [==============================] - 0s 2ms/step - loss: 80.3071 - val_loss: 83.7216\n","Epoch 13/250\n","157/157 [==============================] - 0s 2ms/step - loss: 79.8336 - val_loss: 83.2770\n","Epoch 14/250\n","157/157 [==============================] - 0s 1ms/step - loss: 79.3605 - val_loss: 82.8691\n","Epoch 15/250\n","157/157 [==============================] - 0s 1ms/step - loss: 78.8835 - val_loss: 82.3809\n","Epoch 16/250\n","157/157 [==============================] - 0s 2ms/step - loss: 78.4110 - val_loss: 81.9328\n","Epoch 17/250\n","157/157 [==============================] - 0s 2ms/step - loss: 77.9365 - val_loss: 81.4698\n","Epoch 18/250\n","157/157 [==============================] - 0s 2ms/step - loss: 77.4647 - val_loss: 81.0958\n","Epoch 19/250\n","157/157 [==============================] - 0s 2ms/step - loss: 76.9951 - val_loss: 80.6152\n","Epoch 20/250\n","157/157 [==============================] - 0s 2ms/step - loss: 76.5240 - val_loss: 80.2053\n","Epoch 21/250\n","157/157 [==============================] - 0s 1ms/step - loss: 76.0502 - val_loss: 79.7154\n","Epoch 22/250\n","157/157 [==============================] - 0s 1ms/step - loss: 75.5783 - val_loss: 79.2372\n","Epoch 23/250\n","157/157 [==============================] - 0s 2ms/step - loss: 75.1166 - val_loss: 78.7796\n","Epoch 24/250\n","157/157 [==============================] - 0s 2ms/step - loss: 74.6503 - val_loss: 78.3219\n","Epoch 25/250\n","157/157 [==============================] - 0s 1ms/step - loss: 74.1835 - val_loss: 77.8455\n","Epoch 26/250\n","157/157 [==============================] - 0s 2ms/step - loss: 73.7185 - val_loss: 77.3840\n","Epoch 27/250\n","157/157 [==============================] - 0s 1ms/step - loss: 73.2566 - val_loss: 76.8635\n","Epoch 28/250\n","157/157 [==============================] - 0s 2ms/step - loss: 72.7953 - val_loss: 76.4058\n","Epoch 29/250\n","157/157 [==============================] - 0s 1ms/step - loss: 72.3383 - val_loss: 75.9329\n","Epoch 30/250\n","157/157 [==============================] - 0s 1ms/step - loss: 71.8783 - val_loss: 75.4412\n","Epoch 31/250\n","157/157 [==============================] - 0s 1ms/step - loss: 71.4178 - val_loss: 74.9497\n","Epoch 32/250\n","157/157 [==============================] - 0s 1ms/step - loss: 70.9620 - val_loss: 74.4704\n","Epoch 33/250\n","157/157 [==============================] - 0s 1ms/step - loss: 70.5031 - val_loss: 74.0029\n","Epoch 34/250\n","157/157 [==============================] - 0s 1ms/step - loss: 70.0445 - val_loss: 73.5064\n","Epoch 35/250\n","157/157 [==============================] - 0s 2ms/step - loss: 69.5837 - val_loss: 73.0459\n","Epoch 36/250\n","157/157 [==============================] - 0s 2ms/step - loss: 69.1283 - val_loss: 72.5176\n","Epoch 37/250\n","157/157 [==============================] - 0s 2ms/step - loss: 68.6695 - val_loss: 72.0350\n","Epoch 38/250\n","157/157 [==============================] - 0s 1ms/step - loss: 68.2109 - val_loss: 71.5748\n","Epoch 39/250\n","157/157 [==============================] - 0s 1ms/step - loss: 67.7525 - val_loss: 71.0866\n","Epoch 40/250\n","157/157 [==============================] - 0s 2ms/step - loss: 67.2976 - val_loss: 70.5766\n","Epoch 41/250\n","157/157 [==============================] - 0s 1ms/step - loss: 66.8392 - val_loss: 70.1085\n","Epoch 42/250\n","157/157 [==============================] - 0s 1ms/step - loss: 66.3858 - val_loss: 69.6168\n","Epoch 43/250\n","157/157 [==============================] - 0s 2ms/step - loss: 65.9303 - val_loss: 69.1336\n","Epoch 44/250\n","157/157 [==============================] - 0s 2ms/step - loss: 65.4752 - val_loss: 68.6362\n","Epoch 45/250\n","157/157 [==============================] - 0s 2ms/step - loss: 65.0180 - val_loss: 68.1552\n","Epoch 46/250\n","157/157 [==============================] - 0s 1ms/step - loss: 64.5638 - val_loss: 67.6457\n","Epoch 47/250\n","157/157 [==============================] - 0s 2ms/step - loss: 64.1081 - val_loss: 67.2058\n","Epoch 48/250\n","157/157 [==============================] - 0s 2ms/step - loss: 63.6558 - val_loss: 66.6802\n","Epoch 49/250\n","157/157 [==============================] - 0s 2ms/step - loss: 63.1967 - val_loss: 66.2191\n","Epoch 50/250\n","157/157 [==============================] - 0s 1ms/step - loss: 62.7428 - val_loss: 65.7556\n","Epoch 51/250\n","157/157 [==============================] - 0s 2ms/step - loss: 62.2863 - val_loss: 65.2505\n","Epoch 52/250\n","157/157 [==============================] - 0s 1ms/step - loss: 61.8341 - val_loss: 64.7758\n","Epoch 53/250\n","157/157 [==============================] - 0s 2ms/step - loss: 61.3756 - val_loss: 64.2897\n","Epoch 54/250\n","157/157 [==============================] - 0s 1ms/step - loss: 60.9171 - val_loss: 63.8121\n","Epoch 55/250\n","157/157 [==============================] - 0s 1ms/step - loss: 60.4671 - val_loss: 63.3580\n","Epoch 56/250\n","157/157 [==============================] - 0s 2ms/step - loss: 60.0139 - val_loss: 62.8648\n","Epoch 57/250\n","157/157 [==============================] - 0s 2ms/step - loss: 59.5604 - val_loss: 62.3835\n","Epoch 58/250\n","157/157 [==============================] - 0s 1ms/step - loss: 59.1030 - val_loss: 61.8920\n","Epoch 59/250\n","157/157 [==============================] - 0s 1ms/step - loss: 58.6520 - val_loss: 61.4265\n","Epoch 60/250\n","157/157 [==============================] - 0s 2ms/step - loss: 58.2015 - val_loss: 60.9646\n","Epoch 61/250\n","157/157 [==============================] - 0s 2ms/step - loss: 57.7526 - val_loss: 60.4965\n","Epoch 62/250\n","157/157 [==============================] - 0s 2ms/step - loss: 57.2940 - val_loss: 59.9885\n","Epoch 63/250\n","157/157 [==============================] - 0s 2ms/step - loss: 56.8470 - val_loss: 59.5305\n","Epoch 64/250\n","157/157 [==============================] - 0s 2ms/step - loss: 56.3939 - val_loss: 59.0538\n","Epoch 65/250\n","157/157 [==============================] - 0s 3ms/step - loss: 55.9433 - val_loss: 58.5765\n","Epoch 66/250\n","157/157 [==============================] - 0s 2ms/step - loss: 55.4925 - val_loss: 58.1192\n","Epoch 67/250\n","157/157 [==============================] - 1s 5ms/step - loss: 55.0459 - val_loss: 57.6770\n","Epoch 68/250\n","157/157 [==============================] - 1s 3ms/step - loss: 54.5934 - val_loss: 57.2005\n","Epoch 69/250\n","157/157 [==============================] - 0s 2ms/step - loss: 54.1405 - val_loss: 56.7418\n","Epoch 70/250\n","157/157 [==============================] - 0s 2ms/step - loss: 53.6893 - val_loss: 56.2785\n","Epoch 71/250\n","157/157 [==============================] - 0s 2ms/step - loss: 53.2389 - val_loss: 55.8083\n","Epoch 72/250\n","157/157 [==============================] - 0s 2ms/step - loss: 52.7900 - val_loss: 55.3566\n","Epoch 73/250\n","157/157 [==============================] - 0s 2ms/step - loss: 52.3389 - val_loss: 54.8862\n","Epoch 74/250\n","157/157 [==============================] - 0s 2ms/step - loss: 51.8906 - val_loss: 54.4035\n","Epoch 75/250\n","157/157 [==============================] - 0s 2ms/step - loss: 51.4401 - val_loss: 53.9716\n","Epoch 76/250\n","157/157 [==============================] - 0s 2ms/step - loss: 50.9892 - val_loss: 53.4896\n","Epoch 77/250\n","157/157 [==============================] - 0s 3ms/step - loss: 50.5378 - val_loss: 53.0261\n","Epoch 78/250\n","157/157 [==============================] - 0s 3ms/step - loss: 50.0857 - val_loss: 52.5968\n","Epoch 79/250\n","157/157 [==============================] - 0s 2ms/step - loss: 49.6352 - val_loss: 52.1382\n","Epoch 80/250\n","157/157 [==============================] - 0s 2ms/step - loss: 49.1857 - val_loss: 51.6783\n","Epoch 81/250\n","157/157 [==============================] - 0s 2ms/step - loss: 48.7395 - val_loss: 51.2244\n","Epoch 82/250\n","157/157 [==============================] - 0s 2ms/step - loss: 48.2860 - val_loss: 50.7466\n","Epoch 83/250\n","157/157 [==============================] - 0s 2ms/step - loss: 47.8389 - val_loss: 50.2915\n","Epoch 84/250\n","157/157 [==============================] - 0s 2ms/step - loss: 47.3883 - val_loss: 49.8602\n","Epoch 85/250\n","157/157 [==============================] - 0s 2ms/step - loss: 46.9424 - val_loss: 49.3866\n","Epoch 86/250\n","157/157 [==============================] - 0s 1ms/step - loss: 46.4952 - val_loss: 48.9400\n","Epoch 87/250\n","157/157 [==============================] - 0s 2ms/step - loss: 46.0479 - val_loss: 48.4647\n","Epoch 88/250\n","157/157 [==============================] - 0s 2ms/step - loss: 45.5942 - val_loss: 47.9931\n","Epoch 89/250\n","157/157 [==============================] - 0s 1ms/step - loss: 45.1449 - val_loss: 47.5468\n","Epoch 90/250\n","157/157 [==============================] - 0s 1ms/step - loss: 44.6992 - val_loss: 47.1062\n","Epoch 91/250\n","157/157 [==============================] - 0s 2ms/step - loss: 44.2539 - val_loss: 46.6419\n","Epoch 92/250\n","157/157 [==============================] - 0s 2ms/step - loss: 43.8071 - val_loss: 46.1921\n","Epoch 93/250\n","157/157 [==============================] - 0s 1ms/step - loss: 43.3616 - val_loss: 45.7707\n","Epoch 94/250\n","157/157 [==============================] - 0s 2ms/step - loss: 42.9121 - val_loss: 45.2856\n","Epoch 95/250\n","157/157 [==============================] - 0s 2ms/step - loss: 42.4655 - val_loss: 44.8335\n","Epoch 96/250\n","157/157 [==============================] - 0s 2ms/step - loss: 42.0239 - val_loss: 44.4043\n","Epoch 97/250\n","157/157 [==============================] - 0s 1ms/step - loss: 41.5793 - val_loss: 43.9824\n","Epoch 98/250\n","157/157 [==============================] - 0s 2ms/step - loss: 41.1309 - val_loss: 43.5250\n","Epoch 99/250\n","157/157 [==============================] - 0s 2ms/step - loss: 40.6885 - val_loss: 43.0825\n","Epoch 100/250\n","157/157 [==============================] - 0s 2ms/step - loss: 40.2416 - val_loss: 42.6085\n","Epoch 101/250\n","157/157 [==============================] - 0s 1ms/step - loss: 39.7977 - val_loss: 42.1253\n","Epoch 102/250\n","157/157 [==============================] - 0s 1ms/step - loss: 39.3558 - val_loss: 41.6858\n","Epoch 103/250\n","157/157 [==============================] - 0s 1ms/step - loss: 38.9103 - val_loss: 41.2444\n","Epoch 104/250\n","157/157 [==============================] - 0s 1ms/step - loss: 38.4673 - val_loss: 40.7701\n","Epoch 105/250\n","157/157 [==============================] - 0s 2ms/step - loss: 38.0215 - val_loss: 40.3089\n","Epoch 106/250\n","157/157 [==============================] - 0s 1ms/step - loss: 37.5772 - val_loss: 39.8861\n","Epoch 107/250\n","157/157 [==============================] - 0s 1ms/step - loss: 37.1337 - val_loss: 39.3986\n","Epoch 108/250\n","157/157 [==============================] - 0s 1ms/step - loss: 36.6915 - val_loss: 38.9428\n","Epoch 109/250\n","157/157 [==============================] - 0s 1ms/step - loss: 36.2507 - val_loss: 38.4943\n","Epoch 110/250\n","157/157 [==============================] - 0s 1ms/step - loss: 35.8078 - val_loss: 38.0232\n","Epoch 111/250\n","157/157 [==============================] - 0s 2ms/step - loss: 35.3678 - val_loss: 37.5624\n","Epoch 112/250\n","157/157 [==============================] - 0s 2ms/step - loss: 34.9261 - val_loss: 37.1148\n","Epoch 113/250\n","157/157 [==============================] - 0s 1ms/step - loss: 34.4839 - val_loss: 36.6755\n","Epoch 114/250\n","157/157 [==============================] - 0s 1ms/step - loss: 34.0414 - val_loss: 36.1865\n","Epoch 115/250\n","157/157 [==============================] - 0s 1ms/step - loss: 33.6001 - val_loss: 35.7292\n","Epoch 116/250\n","157/157 [==============================] - 0s 1ms/step - loss: 33.1577 - val_loss: 35.2551\n","Epoch 117/250\n","157/157 [==============================] - 0s 1ms/step - loss: 32.7189 - val_loss: 34.8313\n","Epoch 118/250\n","157/157 [==============================] - 0s 1ms/step - loss: 32.2798 - val_loss: 34.3386\n","Epoch 119/250\n","157/157 [==============================] - 0s 2ms/step - loss: 31.8368 - val_loss: 33.8960\n","Epoch 120/250\n","157/157 [==============================] - 0s 1ms/step - loss: 31.3993 - val_loss: 33.3996\n","Epoch 121/250\n","157/157 [==============================] - 0s 1ms/step - loss: 30.9553 - val_loss: 32.9665\n","Epoch 122/250\n","157/157 [==============================] - 0s 2ms/step - loss: 30.5148 - val_loss: 32.5070\n","Epoch 123/250\n","157/157 [==============================] - 0s 2ms/step - loss: 30.0785 - val_loss: 32.0232\n","Epoch 124/250\n","157/157 [==============================] - 0s 1ms/step - loss: 29.6348 - val_loss: 31.5699\n","Epoch 125/250\n","157/157 [==============================] - 0s 2ms/step - loss: 29.1976 - val_loss: 31.0759\n","Epoch 126/250\n","157/157 [==============================] - 0s 1ms/step - loss: 28.7600 - val_loss: 30.5870\n","Epoch 127/250\n","157/157 [==============================] - 0s 1ms/step - loss: 28.3239 - val_loss: 30.1119\n","Epoch 128/250\n","157/157 [==============================] - 0s 1ms/step - loss: 27.8866 - val_loss: 29.6165\n","Epoch 129/250\n","157/157 [==============================] - 0s 1ms/step - loss: 27.4500 - val_loss: 29.1426\n","Epoch 130/250\n","157/157 [==============================] - 0s 1ms/step - loss: 27.0160 - val_loss: 28.6603\n","Epoch 131/250\n","157/157 [==============================] - 0s 1ms/step - loss: 26.5801 - val_loss: 28.1773\n","Epoch 132/250\n","157/157 [==============================] - 0s 2ms/step - loss: 26.1430 - val_loss: 27.7084\n","Epoch 133/250\n","157/157 [==============================] - 0s 1ms/step - loss: 25.7119 - val_loss: 27.1781\n","Epoch 134/250\n","157/157 [==============================] - 0s 1ms/step - loss: 25.2778 - val_loss: 26.7133\n","Epoch 135/250\n","157/157 [==============================] - 0s 2ms/step - loss: 24.8430 - val_loss: 26.2196\n","Epoch 136/250\n","157/157 [==============================] - 0s 1ms/step - loss: 24.4107 - val_loss: 25.7459\n","Epoch 137/250\n","157/157 [==============================] - 0s 1ms/step - loss: 23.9778 - val_loss: 25.2040\n","Epoch 138/250\n","157/157 [==============================] - 0s 1ms/step - loss: 23.5495 - val_loss: 24.7293\n","Epoch 139/250\n","157/157 [==============================] - 0s 1ms/step - loss: 23.1159 - val_loss: 24.2554\n","Epoch 140/250\n","157/157 [==============================] - 0s 1ms/step - loss: 22.6837 - val_loss: 23.7444\n","Epoch 141/250\n","157/157 [==============================] - 0s 2ms/step - loss: 22.2551 - val_loss: 23.2420\n","Epoch 142/250\n","157/157 [==============================] - 0s 1ms/step - loss: 21.8289 - val_loss: 22.7540\n","Epoch 143/250\n","157/157 [==============================] - 0s 1ms/step - loss: 21.3993 - val_loss: 22.2388\n","Epoch 144/250\n","157/157 [==============================] - 0s 2ms/step - loss: 20.9726 - val_loss: 21.7069\n","Epoch 145/250\n","157/157 [==============================] - 0s 2ms/step - loss: 20.5484 - val_loss: 21.1672\n","Epoch 146/250\n","157/157 [==============================] - 0s 2ms/step - loss: 20.1294 - val_loss: 20.6893\n","Epoch 147/250\n","157/157 [==============================] - 0s 2ms/step - loss: 19.7103 - val_loss: 20.1622\n","Epoch 148/250\n","157/157 [==============================] - 0s 2ms/step - loss: 19.2906 - val_loss: 19.6961\n","Epoch 149/250\n","157/157 [==============================] - 0s 2ms/step - loss: 18.8791 - val_loss: 19.2164\n","Epoch 150/250\n","157/157 [==============================] - 0s 1ms/step - loss: 18.4710 - val_loss: 18.7638\n","Epoch 151/250\n","157/157 [==============================] - 0s 1ms/step - loss: 18.0643 - val_loss: 18.3098\n","Epoch 152/250\n","157/157 [==============================] - 0s 1ms/step - loss: 17.6675 - val_loss: 17.8745\n","Epoch 153/250\n","157/157 [==============================] - 0s 1ms/step - loss: 17.2733 - val_loss: 17.4500\n","Epoch 154/250\n","157/157 [==============================] - 0s 2ms/step - loss: 16.8914 - val_loss: 17.0386\n","Epoch 155/250\n","157/157 [==============================] - 0s 1ms/step - loss: 16.5040 - val_loss: 16.6039\n","Epoch 156/250\n","157/157 [==============================] - 0s 2ms/step - loss: 16.1270 - val_loss: 16.1981\n","Epoch 157/250\n","157/157 [==============================] - 0s 1ms/step - loss: 15.7588 - val_loss: 15.8480\n","Epoch 158/250\n","157/157 [==============================] - 0s 1ms/step - loss: 15.3972 - val_loss: 15.4715\n","Epoch 159/250\n","157/157 [==============================] - 0s 2ms/step - loss: 15.0338 - val_loss: 15.1107\n","Epoch 160/250\n","157/157 [==============================] - 0s 2ms/step - loss: 14.6821 - val_loss: 14.7342\n","Epoch 161/250\n","157/157 [==============================] - 0s 2ms/step - loss: 14.3353 - val_loss: 14.4003\n","Epoch 162/250\n","157/157 [==============================] - 0s 2ms/step - loss: 13.9921 - val_loss: 14.1235\n","Epoch 163/250\n","157/157 [==============================] - 0s 2ms/step - loss: 13.6633 - val_loss: 13.8767\n","Epoch 164/250\n","157/157 [==============================] - 0s 2ms/step - loss: 13.3370 - val_loss: 13.6262\n","Epoch 165/250\n","157/157 [==============================] - 0s 1ms/step - loss: 13.0256 - val_loss: 13.3658\n","Epoch 166/250\n","157/157 [==============================] - 0s 1ms/step - loss: 12.7220 - val_loss: 13.1237\n","Epoch 167/250\n","157/157 [==============================] - 0s 1ms/step - loss: 12.4331 - val_loss: 12.9416\n","Epoch 168/250\n","157/157 [==============================] - 0s 2ms/step - loss: 12.1510 - val_loss: 12.7682\n","Epoch 169/250\n","157/157 [==============================] - 0s 2ms/step - loss: 11.8730 - val_loss: 12.5485\n","Epoch 170/250\n","157/157 [==============================] - 0s 2ms/step - loss: 11.6094 - val_loss: 12.3741\n","Epoch 171/250\n","157/157 [==============================] - 0s 2ms/step - loss: 11.3571 - val_loss: 12.2103\n","Epoch 172/250\n","157/157 [==============================] - 0s 2ms/step - loss: 11.1139 - val_loss: 12.0050\n","Epoch 173/250\n","157/157 [==============================] - 0s 2ms/step - loss: 10.8799 - val_loss: 11.8549\n","Epoch 174/250\n","157/157 [==============================] - 0s 2ms/step - loss: 10.6633 - val_loss: 11.7188\n","Epoch 175/250\n","157/157 [==============================] - 0s 1ms/step - loss: 10.4530 - val_loss: 11.5677\n","Epoch 176/250\n","157/157 [==============================] - 0s 2ms/step - loss: 10.2488 - val_loss: 11.3927\n","Epoch 177/250\n","157/157 [==============================] - 0s 2ms/step - loss: 10.0560 - val_loss: 11.2601\n","Epoch 178/250\n","157/157 [==============================] - 0s 2ms/step - loss: 9.8695 - val_loss: 11.1558\n","Epoch 179/250\n","157/157 [==============================] - 0s 1ms/step - loss: 9.6973 - val_loss: 10.9896\n","Epoch 180/250\n","157/157 [==============================] - 0s 2ms/step - loss: 9.5366 - val_loss: 10.8767\n","Epoch 181/250\n","157/157 [==============================] - 0s 1ms/step - loss: 9.3811 - val_loss: 10.7514\n","Epoch 182/250\n","157/157 [==============================] - 0s 1ms/step - loss: 9.2489 - val_loss: 10.6668\n","Epoch 183/250\n","157/157 [==============================] - 0s 2ms/step - loss: 9.1222 - val_loss: 10.5556\n","Epoch 184/250\n","157/157 [==============================] - 0s 2ms/step - loss: 9.0085 - val_loss: 10.4084\n","Epoch 185/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.9038 - val_loss: 10.3582\n","Epoch 186/250\n","157/157 [==============================] - 0s 1ms/step - loss: 8.8111 - val_loss: 10.2171\n","Epoch 187/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.7253 - val_loss: 10.1312\n","Epoch 188/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.6451 - val_loss: 10.0201\n","Epoch 189/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.5733 - val_loss: 9.9492\n","Epoch 190/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.5029 - val_loss: 9.9024\n","Epoch 191/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.4444 - val_loss: 9.8640\n","Epoch 192/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.3841 - val_loss: 9.8978\n","Epoch 193/250\n","157/157 [==============================] - 0s 1ms/step - loss: 8.3342 - val_loss: 9.8665\n","Epoch 194/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.2907 - val_loss: 9.8538\n","Epoch 195/250\n","157/157 [==============================] - 0s 1ms/step - loss: 8.2472 - val_loss: 9.7853\n","Epoch 196/250\n","157/157 [==============================] - 0s 1ms/step - loss: 8.2116 - val_loss: 9.8394\n","Epoch 197/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.1772 - val_loss: 9.8036\n","Epoch 198/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.1512 - val_loss: 9.8578\n","Epoch 199/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.1305 - val_loss: 9.8683\n","Epoch 200/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.1093 - val_loss: 9.8319\n","Epoch 201/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0914 - val_loss: 9.8732\n","Epoch 202/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0743 - val_loss: 9.8842\n","Epoch 203/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0625 - val_loss: 9.9157\n","Epoch 204/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0519 - val_loss: 9.8923\n","Epoch 205/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0453 - val_loss: 9.9138\n","Epoch 206/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0349 - val_loss: 9.9322\n","Epoch 207/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0322 - val_loss: 9.9326\n","Epoch 208/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0261 - val_loss: 9.8888\n","Epoch 209/250\n","157/157 [==============================] - 0s 1ms/step - loss: 8.0200 - val_loss: 9.9261\n","Epoch 210/250\n","157/157 [==============================] - 0s 1ms/step - loss: 8.0164 - val_loss: 9.9259\n","Epoch 211/250\n","157/157 [==============================] - 0s 1ms/step - loss: 8.0193 - val_loss: 9.9089\n","Epoch 212/250\n","157/157 [==============================] - 0s 1ms/step - loss: 8.0091 - val_loss: 9.9610\n","Epoch 213/250\n","157/157 [==============================] - 0s 1ms/step - loss: 8.0082 - val_loss: 9.9198\n","Epoch 214/250\n","157/157 [==============================] - 0s 1ms/step - loss: 8.0094 - val_loss: 9.9169\n","Epoch 215/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0063 - val_loss: 9.9614\n","Epoch 216/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0057 - val_loss: 9.9248\n","Epoch 217/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0003 - val_loss: 9.9379\n","Epoch 218/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0008 - val_loss: 9.9126\n","Epoch 219/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0047 - val_loss: 9.9410\n","Epoch 220/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0022 - val_loss: 9.9376\n","Epoch 221/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9939 - val_loss: 9.9815\n","Epoch 222/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9961 - val_loss: 9.9071\n","Epoch 223/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9976 - val_loss: 9.9071\n","Epoch 224/250\n","157/157 [==============================] - 0s 2ms/step - loss: 8.0013 - val_loss: 9.8988\n","Epoch 225/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9972 - val_loss: 9.9344\n","Epoch 226/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9964 - val_loss: 9.9229\n","Epoch 227/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9954 - val_loss: 9.9032\n","Epoch 228/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9944 - val_loss: 9.8966\n","Epoch 229/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9934 - val_loss: 9.9033\n","Epoch 230/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9926 - val_loss: 9.8925\n","Epoch 231/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9896 - val_loss: 9.9079\n","Epoch 232/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9941 - val_loss: 9.9067\n","Epoch 233/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9902 - val_loss: 9.8921\n","Epoch 234/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9892 - val_loss: 9.8946\n","Epoch 235/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9916 - val_loss: 9.9236\n","Epoch 236/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9877 - val_loss: 9.9175\n","Epoch 237/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9899 - val_loss: 9.8862\n","Epoch 238/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9896 - val_loss: 9.8910\n","Epoch 239/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9868 - val_loss: 9.9378\n","Epoch 240/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9890 - val_loss: 9.8739\n","Epoch 241/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9842 - val_loss: 9.8857\n","Epoch 242/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9849 - val_loss: 9.9252\n","Epoch 243/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9855 - val_loss: 9.9128\n","Epoch 244/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9885 - val_loss: 9.9249\n","Epoch 245/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9858 - val_loss: 9.9047\n","Epoch 246/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9852 - val_loss: 9.8933\n","Epoch 247/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9844 - val_loss: 9.9018\n","Epoch 248/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9849 - val_loss: 9.8985\n","Epoch 249/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9847 - val_loss: 9.8786\n","Epoch 250/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9827 - val_loss: 9.9002\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Epoch 1/250\n","157/157 [==============================] - 1s 2ms/step - loss: 7.9715 - val_loss: 9.8895\n","Epoch 2/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9691 - val_loss: 9.8934\n","Epoch 3/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9692 - val_loss: 9.9146\n","Epoch 4/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9700 - val_loss: 9.8935\n","Epoch 5/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9695 - val_loss: 9.8975\n","Epoch 6/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9683 - val_loss: 9.8804\n","Epoch 7/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9673 - val_loss: 9.8827\n","Epoch 8/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9678 - val_loss: 9.8749\n","Epoch 9/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9681 - val_loss: 9.8916\n","Epoch 10/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9681 - val_loss: 9.8862\n","Epoch 11/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9676 - val_loss: 9.8756\n","Epoch 12/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9674 - val_loss: 9.8797\n","Epoch 13/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9689 - val_loss: 9.8716\n","Epoch 14/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9702 - val_loss: 9.8791\n","Epoch 15/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9678 - val_loss: 9.8668\n","Epoch 16/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9666 - val_loss: 9.8653\n","Epoch 17/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9672 - val_loss: 9.8808\n","Epoch 18/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9670 - val_loss: 9.8690\n","Epoch 19/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9683 - val_loss: 9.8679\n","Epoch 20/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9679 - val_loss: 9.8825\n","Epoch 21/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9662 - val_loss: 9.8843\n","Epoch 22/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9658 - val_loss: 9.8839\n","Epoch 23/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9681 - val_loss: 9.8643\n","Epoch 24/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9644 - val_loss: 9.8699\n","Epoch 25/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9656 - val_loss: 9.8757\n","Epoch 26/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9653 - val_loss: 9.8531\n","Epoch 27/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9667 - val_loss: 9.8513\n","Epoch 28/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9655 - val_loss: 9.8799\n","Epoch 29/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9654 - val_loss: 9.8731\n","Epoch 30/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9639 - val_loss: 9.8594\n","Epoch 31/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9647 - val_loss: 9.8663\n","Epoch 32/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9637 - val_loss: 9.8689\n","Epoch 33/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9639 - val_loss: 9.8715\n","Epoch 34/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9643 - val_loss: 9.8615\n","Epoch 35/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9629 - val_loss: 9.8611\n","Epoch 36/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9629 - val_loss: 9.8469\n","Epoch 37/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9634 - val_loss: 9.8590\n","Epoch 38/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9625 - val_loss: 9.8623\n","Epoch 39/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9637 - val_loss: 9.8726\n","Epoch 40/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9647 - val_loss: 9.8332\n","Epoch 41/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9642 - val_loss: 9.8558\n","Epoch 42/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9624 - val_loss: 9.8752\n","Epoch 43/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9614 - val_loss: 9.8610\n","Epoch 44/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9637 - val_loss: 9.8484\n","Epoch 45/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9623 - val_loss: 9.8736\n","Epoch 46/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9611 - val_loss: 9.8672\n","Epoch 47/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9609 - val_loss: 9.8524\n","Epoch 48/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9629 - val_loss: 9.8623\n","Epoch 49/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9620 - val_loss: 9.8644\n","Epoch 50/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9599 - val_loss: 9.8666\n","Epoch 51/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9615 - val_loss: 9.8613\n","Epoch 52/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9606 - val_loss: 9.8712\n","Epoch 53/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9612 - val_loss: 9.8533\n","Epoch 54/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9611 - val_loss: 9.8539\n","Epoch 55/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9606 - val_loss: 9.8613\n","Epoch 56/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9601 - val_loss: 9.8458\n","Epoch 57/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9607 - val_loss: 9.8574\n","Epoch 58/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9608 - val_loss: 9.8523\n","Epoch 59/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9593 - val_loss: 9.8707\n","Epoch 60/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9600 - val_loss: 9.8540\n","Epoch 61/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9606 - val_loss: 9.8495\n","Epoch 62/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9597 - val_loss: 9.8554\n","Epoch 63/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9590 - val_loss: 9.8473\n","Epoch 64/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9618 - val_loss: 9.8495\n","Epoch 65/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9599 - val_loss: 9.8507\n","Epoch 66/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9595 - val_loss: 9.8409\n","Epoch 67/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9589 - val_loss: 9.8468\n","Epoch 68/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9578 - val_loss: 9.8659\n","Epoch 69/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9573 - val_loss: 9.8501\n","Epoch 70/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9583 - val_loss: 9.8436\n","Epoch 71/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9572 - val_loss: 9.8471\n","Epoch 72/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9580 - val_loss: 9.8647\n","Epoch 73/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9593 - val_loss: 9.8488\n","Epoch 74/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9585 - val_loss: 9.8705\n","Epoch 75/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9587 - val_loss: 9.8623\n","Epoch 76/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9585 - val_loss: 9.8866\n","Epoch 77/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9585 - val_loss: 9.8740\n","Epoch 78/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9576 - val_loss: 9.8560\n","Epoch 79/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9585 - val_loss: 9.8570\n","Epoch 80/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9582 - val_loss: 9.8597\n","Epoch 81/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9557 - val_loss: 9.8776\n","Epoch 82/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9572 - val_loss: 9.8583\n","Epoch 83/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9557 - val_loss: 9.8617\n","Epoch 84/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9561 - val_loss: 9.8589\n","Epoch 85/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9581 - val_loss: 9.8533\n","Epoch 86/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9570 - val_loss: 9.8543\n","Epoch 87/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9564 - val_loss: 9.8616\n","Epoch 88/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9555 - val_loss: 9.8630\n","Epoch 89/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9558 - val_loss: 9.8377\n","Epoch 90/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9564 - val_loss: 9.8600\n","Epoch 91/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9559 - val_loss: 9.8428\n","Epoch 92/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9557 - val_loss: 9.8446\n","Epoch 93/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9560 - val_loss: 9.8568\n","Epoch 94/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9546 - val_loss: 9.8635\n","Epoch 95/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9561 - val_loss: 9.8609\n","Epoch 96/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9549 - val_loss: 9.8616\n","Epoch 97/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9545 - val_loss: 9.8485\n","Epoch 98/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9540 - val_loss: 9.8557\n","Epoch 99/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9553 - val_loss: 9.8576\n","Epoch 100/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9536 - val_loss: 9.8697\n","Epoch 101/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9529 - val_loss: 9.8574\n","Epoch 102/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9551 - val_loss: 9.8472\n","Epoch 103/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9521 - val_loss: 9.8531\n","Epoch 104/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9537 - val_loss: 9.8549\n","Epoch 105/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9533 - val_loss: 9.8615\n","Epoch 106/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9530 - val_loss: 9.8826\n","Epoch 107/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9541 - val_loss: 9.8558\n","Epoch 108/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9542 - val_loss: 9.8438\n","Epoch 109/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9531 - val_loss: 9.8578\n","Epoch 110/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9535 - val_loss: 9.8720\n","Epoch 111/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9521 - val_loss: 9.8683\n","Epoch 112/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9523 - val_loss: 9.8648\n","Epoch 113/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9538 - val_loss: 9.8468\n","Epoch 114/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9534 - val_loss: 9.8547\n","Epoch 115/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9524 - val_loss: 9.8485\n","Epoch 116/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9521 - val_loss: 9.8478\n","Epoch 117/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9521 - val_loss: 9.8611\n","Epoch 118/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9503 - val_loss: 9.8459\n","Epoch 119/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9520 - val_loss: 9.8436\n","Epoch 120/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9528 - val_loss: 9.8576\n","Epoch 121/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9534 - val_loss: 9.8596\n","Epoch 122/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9513 - val_loss: 9.8537\n","Epoch 123/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9499 - val_loss: 9.8372\n","Epoch 124/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9521 - val_loss: 9.8334\n","Epoch 125/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9502 - val_loss: 9.8434\n","Epoch 126/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9509 - val_loss: 9.8475\n","Epoch 127/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9510 - val_loss: 9.8413\n","Epoch 128/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9503 - val_loss: 9.8496\n","Epoch 129/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9516 - val_loss: 9.8320\n","Epoch 130/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9514 - val_loss: 9.8413\n","Epoch 131/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9505 - val_loss: 9.8659\n","Epoch 132/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9507 - val_loss: 9.8536\n","Epoch 133/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9497 - val_loss: 9.8598\n","Epoch 134/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9499 - val_loss: 9.8428\n","Epoch 135/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9507 - val_loss: 9.8439\n","Epoch 136/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9492 - val_loss: 9.8523\n","Epoch 137/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9511 - val_loss: 9.8409\n","Epoch 138/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9480 - val_loss: 9.8512\n","Epoch 139/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9498 - val_loss: 9.8451\n","Epoch 140/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9483 - val_loss: 9.8497\n","Epoch 141/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9498 - val_loss: 9.8495\n","Epoch 142/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9469 - val_loss: 9.8662\n","Epoch 143/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9498 - val_loss: 9.8540\n","Epoch 144/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9476 - val_loss: 9.8608\n","Epoch 145/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9473 - val_loss: 9.8368\n","Epoch 146/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9487 - val_loss: 9.8454\n","Epoch 147/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9485 - val_loss: 9.8427\n","Epoch 148/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9493 - val_loss: 9.8378\n","Epoch 149/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9469 - val_loss: 9.8505\n","Epoch 150/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9487 - val_loss: 9.8446\n","Epoch 151/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9476 - val_loss: 9.8475\n","Epoch 152/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9476 - val_loss: 9.8699\n","Epoch 153/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9480 - val_loss: 9.8671\n","Epoch 154/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9482 - val_loss: 9.8464\n","Epoch 155/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9473 - val_loss: 9.8446\n","Epoch 156/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9470 - val_loss: 9.8481\n","Epoch 157/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9481 - val_loss: 9.8444\n","Epoch 158/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9461 - val_loss: 9.8578\n","Epoch 159/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9470 - val_loss: 9.8613\n","Epoch 160/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9482 - val_loss: 9.8341\n","Epoch 161/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9464 - val_loss: 9.8509\n","Epoch 162/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9473 - val_loss: 9.8354\n","Epoch 163/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9451 - val_loss: 9.8368\n","Epoch 164/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9454 - val_loss: 9.8514\n","Epoch 165/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9461 - val_loss: 9.8487\n","Epoch 166/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9466 - val_loss: 9.8234\n","Epoch 167/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9470 - val_loss: 9.8445\n","Epoch 168/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9462 - val_loss: 9.8260\n","Epoch 169/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9456 - val_loss: 9.8540\n","Epoch 170/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9455 - val_loss: 9.8234\n","Epoch 171/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9450 - val_loss: 9.8544\n","Epoch 172/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9441 - val_loss: 9.8336\n","Epoch 173/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9455 - val_loss: 9.8500\n","Epoch 174/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9451 - val_loss: 9.8430\n","Epoch 175/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9456 - val_loss: 9.8396\n","Epoch 176/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9441 - val_loss: 9.8460\n","Epoch 177/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9438 - val_loss: 9.8501\n","Epoch 178/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9447 - val_loss: 9.8242\n","Epoch 179/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9452 - val_loss: 9.8470\n","Epoch 180/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9447 - val_loss: 9.8405\n","Epoch 181/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9453 - val_loss: 9.8622\n","Epoch 182/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9441 - val_loss: 9.8397\n","Epoch 183/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9429 - val_loss: 9.8430\n","Epoch 184/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9444 - val_loss: 9.8565\n","Epoch 185/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9461 - val_loss: 9.8404\n","Epoch 186/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9442 - val_loss: 9.8425\n","Epoch 187/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9423 - val_loss: 9.8426\n","Epoch 188/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9435 - val_loss: 9.8523\n","Epoch 189/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9438 - val_loss: 9.8427\n","Epoch 190/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9429 - val_loss: 9.8458\n","Epoch 191/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9427 - val_loss: 9.8593\n","Epoch 192/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9430 - val_loss: 9.8425\n","Epoch 193/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9424 - val_loss: 9.8313\n","Epoch 194/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9424 - val_loss: 9.8409\n","Epoch 195/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9421 - val_loss: 9.8184\n","Epoch 196/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9435 - val_loss: 9.8474\n","Epoch 197/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9430 - val_loss: 9.8591\n","Epoch 198/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9425 - val_loss: 9.8354\n","Epoch 199/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9430 - val_loss: 9.8398\n","Epoch 200/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9420 - val_loss: 9.8309\n","Epoch 201/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9421 - val_loss: 9.8495\n","Epoch 202/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9434 - val_loss: 9.8391\n","Epoch 203/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9406 - val_loss: 9.8430\n","Epoch 204/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9425 - val_loss: 9.8353\n","Epoch 205/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9407 - val_loss: 9.8411\n","Epoch 206/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9414 - val_loss: 9.8369\n","Epoch 207/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9403 - val_loss: 9.8348\n","Epoch 208/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9415 - val_loss: 9.8394\n","Epoch 209/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9409 - val_loss: 9.8235\n","Epoch 210/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9425 - val_loss: 9.8427\n","Epoch 211/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9416 - val_loss: 9.8389\n","Epoch 212/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9409 - val_loss: 9.8459\n","Epoch 213/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9406 - val_loss: 9.8411\n","Epoch 214/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9410 - val_loss: 9.8345\n","Epoch 215/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9403 - val_loss: 9.8461\n","Epoch 216/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9420 - val_loss: 9.8380\n","Epoch 217/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9412 - val_loss: 9.8475\n","Epoch 218/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9391 - val_loss: 9.8379\n","Epoch 219/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9393 - val_loss: 9.8280\n","Epoch 220/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9401 - val_loss: 9.8358\n","Epoch 221/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9401 - val_loss: 9.8236\n","Epoch 222/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9388 - val_loss: 9.8485\n","Epoch 223/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9393 - val_loss: 9.8540\n","Epoch 224/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9392 - val_loss: 9.8565\n","Epoch 225/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9405 - val_loss: 9.8552\n","Epoch 226/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9394 - val_loss: 9.8478\n","Epoch 227/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9396 - val_loss: 9.8373\n","Epoch 228/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9384 - val_loss: 9.8472\n","Epoch 229/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9401 - val_loss: 9.8290\n","Epoch 230/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9413 - val_loss: 9.8302\n","Epoch 231/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9387 - val_loss: 9.8273\n","Epoch 232/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9385 - val_loss: 9.8403\n","Epoch 233/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9394 - val_loss: 9.8214\n","Epoch 234/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9392 - val_loss: 9.8361\n","Epoch 235/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9405 - val_loss: 9.8461\n","Epoch 236/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9406 - val_loss: 9.8456\n","Epoch 237/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9395 - val_loss: 9.8528\n","Epoch 238/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9399 - val_loss: 9.8347\n","Epoch 239/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9397 - val_loss: 9.8426\n","Epoch 240/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9386 - val_loss: 9.8316\n","Epoch 241/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9373 - val_loss: 9.8506\n","Epoch 242/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9375 - val_loss: 9.8471\n","Epoch 243/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9369 - val_loss: 9.8267\n","Epoch 244/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9388 - val_loss: 9.8458\n","Epoch 245/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9372 - val_loss: 9.8397\n","Epoch 246/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9369 - val_loss: 9.8398\n","Epoch 247/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9376 - val_loss: 9.8348\n","Epoch 248/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9369 - val_loss: 9.8352\n","Epoch 249/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9404 - val_loss: 9.8192\n","Epoch 250/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9358 - val_loss: 9.8314\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Epoch 1/250\n","157/157 [==============================] - 1s 2ms/step - loss: 7.9335 - val_loss: 9.8385\n","Epoch 2/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9330 - val_loss: 9.8399\n","Epoch 3/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9322 - val_loss: 9.8281\n","Epoch 4/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9330 - val_loss: 9.8431\n","Epoch 5/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9325 - val_loss: 9.8401\n","Epoch 6/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9331 - val_loss: 9.8416\n","Epoch 7/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9319 - val_loss: 9.8219\n","Epoch 8/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9322 - val_loss: 9.8252\n","Epoch 9/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9319 - val_loss: 9.8338\n","Epoch 10/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9319 - val_loss: 9.8372\n","Epoch 11/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9317 - val_loss: 9.8448\n","Epoch 12/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9311 - val_loss: 9.8263\n","Epoch 13/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9326 - val_loss: 9.8346\n","Epoch 14/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9319 - val_loss: 9.8244\n","Epoch 15/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9310 - val_loss: 9.8279\n","Epoch 16/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9324 - val_loss: 9.8315\n","Epoch 17/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9318 - val_loss: 9.8436\n","Epoch 18/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9314 - val_loss: 9.8379\n","Epoch 19/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9317 - val_loss: 9.8290\n","Epoch 20/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9307 - val_loss: 9.8321\n","Epoch 21/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9313 - val_loss: 9.8386\n","Epoch 22/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9317 - val_loss: 9.8353\n","Epoch 23/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9309 - val_loss: 9.8451\n","Epoch 24/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9310 - val_loss: 9.8480\n","Epoch 25/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9316 - val_loss: 9.8393\n","Epoch 26/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9306 - val_loss: 9.8319\n","Epoch 27/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9311 - val_loss: 9.8319\n","Epoch 28/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9304 - val_loss: 9.8320\n","Epoch 29/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9320 - val_loss: 9.8375\n","Epoch 30/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9314 - val_loss: 9.8357\n","Epoch 31/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9300 - val_loss: 9.8385\n","Epoch 32/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9312 - val_loss: 9.8379\n","Epoch 33/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9306 - val_loss: 9.8427\n","Epoch 34/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9307 - val_loss: 9.8362\n","Epoch 35/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9310 - val_loss: 9.8331\n","Epoch 36/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9300 - val_loss: 9.8366\n","Epoch 37/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9320 - val_loss: 9.8372\n","Epoch 38/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9308 - val_loss: 9.8459\n","Epoch 39/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9304 - val_loss: 9.8314\n","Epoch 40/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9305 - val_loss: 9.8197\n","Epoch 41/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9303 - val_loss: 9.8279\n","Epoch 42/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9299 - val_loss: 9.8187\n","Epoch 43/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9300 - val_loss: 9.8294\n","Epoch 44/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9305 - val_loss: 9.8318\n","Epoch 45/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9300 - val_loss: 9.8322\n","Epoch 46/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9307 - val_loss: 9.8312\n","Epoch 47/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9293 - val_loss: 9.8338\n","Epoch 48/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9306 - val_loss: 9.8448\n","Epoch 49/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9303 - val_loss: 9.8375\n","Epoch 50/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9294 - val_loss: 9.8398\n","Epoch 51/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9305 - val_loss: 9.8382\n","Epoch 52/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9304 - val_loss: 9.8257\n","Epoch 53/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9290 - val_loss: 9.8271\n","Epoch 54/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9300 - val_loss: 9.8243\n","Epoch 55/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9292 - val_loss: 9.8289\n","Epoch 56/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9294 - val_loss: 9.8316\n","Epoch 57/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9293 - val_loss: 9.8361\n","Epoch 58/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9290 - val_loss: 9.8396\n","Epoch 59/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9283 - val_loss: 9.8215\n","Epoch 60/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9291 - val_loss: 9.8221\n","Epoch 61/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9287 - val_loss: 9.8424\n","Epoch 62/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9285 - val_loss: 9.8274\n","Epoch 63/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9287 - val_loss: 9.8251\n","Epoch 64/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9287 - val_loss: 9.8348\n","Epoch 65/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9295 - val_loss: 9.8258\n","Epoch 66/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9295 - val_loss: 9.8275\n","Epoch 67/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9296 - val_loss: 9.8186\n","Epoch 68/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9287 - val_loss: 9.8374\n","Epoch 69/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9284 - val_loss: 9.8254\n","Epoch 70/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9281 - val_loss: 9.8344\n","Epoch 71/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9286 - val_loss: 9.8380\n","Epoch 72/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9294 - val_loss: 9.8383\n","Epoch 73/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9283 - val_loss: 9.8356\n","Epoch 74/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9284 - val_loss: 9.8343\n","Epoch 75/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9274 - val_loss: 9.8381\n","Epoch 76/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9288 - val_loss: 9.8376\n","Epoch 77/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9276 - val_loss: 9.8278\n","Epoch 78/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9279 - val_loss: 9.8180\n","Epoch 79/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9278 - val_loss: 9.8228\n","Epoch 80/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9295 - val_loss: 9.8300\n","Epoch 81/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9284 - val_loss: 9.8229\n","Epoch 82/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9278 - val_loss: 9.8404\n","Epoch 83/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9273 - val_loss: 9.8252\n","Epoch 84/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9273 - val_loss: 9.8149\n","Epoch 85/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9276 - val_loss: 9.8240\n","Epoch 86/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9269 - val_loss: 9.8367\n","Epoch 87/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9277 - val_loss: 9.8418\n","Epoch 88/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9271 - val_loss: 9.8362\n","Epoch 89/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9276 - val_loss: 9.8290\n","Epoch 90/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9281 - val_loss: 9.8302\n","Epoch 91/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9283 - val_loss: 9.8285\n","Epoch 92/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9263 - val_loss: 9.8263\n","Epoch 93/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9290 - val_loss: 9.8236\n","Epoch 94/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9266 - val_loss: 9.8222\n","Epoch 95/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9266 - val_loss: 9.8328\n","Epoch 96/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9273 - val_loss: 9.8232\n","Epoch 97/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9267 - val_loss: 9.8343\n","Epoch 98/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9269 - val_loss: 9.8414\n","Epoch 99/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9279 - val_loss: 9.8336\n","Epoch 100/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9276 - val_loss: 9.8282\n","Epoch 101/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9258 - val_loss: 9.8366\n","Epoch 102/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9274 - val_loss: 9.8264\n","Epoch 103/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9260 - val_loss: 9.8304\n","Epoch 104/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9262 - val_loss: 9.8312\n","Epoch 105/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9269 - val_loss: 9.8333\n","Epoch 106/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9263 - val_loss: 9.8343\n","Epoch 107/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9256 - val_loss: 9.8295\n","Epoch 108/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9265 - val_loss: 9.8270\n","Epoch 109/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9269 - val_loss: 9.8315\n","Epoch 110/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9265 - val_loss: 9.8262\n","Epoch 111/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9249 - val_loss: 9.8173\n","Epoch 112/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9262 - val_loss: 9.8292\n","Epoch 113/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9249 - val_loss: 9.8362\n","Epoch 114/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9258 - val_loss: 9.8336\n","Epoch 115/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9259 - val_loss: 9.8247\n","Epoch 116/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9256 - val_loss: 9.8272\n","Epoch 117/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9252 - val_loss: 9.8348\n","Epoch 118/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9252 - val_loss: 9.8334\n","Epoch 119/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9261 - val_loss: 9.8349\n","Epoch 120/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9253 - val_loss: 9.8311\n","Epoch 121/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9256 - val_loss: 9.8326\n","Epoch 122/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9259 - val_loss: 9.8330\n","Epoch 123/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9263 - val_loss: 9.8309\n","Epoch 124/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9258 - val_loss: 9.8347\n","Epoch 125/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9248 - val_loss: 9.8286\n","Epoch 126/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9255 - val_loss: 9.8301\n","Epoch 127/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9253 - val_loss: 9.8244\n","Epoch 128/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9254 - val_loss: 9.8280\n","Epoch 129/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9251 - val_loss: 9.8263\n","Epoch 130/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9241 - val_loss: 9.8228\n","Epoch 131/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9244 - val_loss: 9.8227\n","Epoch 132/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9247 - val_loss: 9.8231\n","Epoch 133/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9250 - val_loss: 9.8306\n","Epoch 134/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9245 - val_loss: 9.8175\n","Epoch 135/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9254 - val_loss: 9.8191\n","Epoch 136/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9247 - val_loss: 9.8259\n","Epoch 137/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9244 - val_loss: 9.8197\n","Epoch 138/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9248 - val_loss: 9.8207\n","Epoch 139/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9241 - val_loss: 9.8214\n","Epoch 140/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9249 - val_loss: 9.8190\n","Epoch 141/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9240 - val_loss: 9.8217\n","Epoch 142/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9249 - val_loss: 9.8327\n","Epoch 143/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9242 - val_loss: 9.8294\n","Epoch 144/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9252 - val_loss: 9.8387\n","Epoch 145/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9235 - val_loss: 9.8287\n","Epoch 146/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9254 - val_loss: 9.8442\n","Epoch 147/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9243 - val_loss: 9.8285\n","Epoch 148/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9239 - val_loss: 9.8319\n","Epoch 149/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9239 - val_loss: 9.8348\n","Epoch 150/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9235 - val_loss: 9.8282\n","Epoch 151/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9245 - val_loss: 9.8299\n","Epoch 152/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9243 - val_loss: 9.8296\n","Epoch 153/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9231 - val_loss: 9.8391\n","Epoch 154/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9236 - val_loss: 9.8275\n","Epoch 155/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9256 - val_loss: 9.8277\n","Epoch 156/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9231 - val_loss: 9.8238\n","Epoch 157/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9238 - val_loss: 9.8265\n","Epoch 158/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9236 - val_loss: 9.8191\n","Epoch 159/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9242 - val_loss: 9.8162\n","Epoch 160/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9236 - val_loss: 9.8284\n","Epoch 161/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9246 - val_loss: 9.8335\n","Epoch 162/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9218 - val_loss: 9.8180\n","Epoch 163/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9232 - val_loss: 9.8211\n","Epoch 164/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9237 - val_loss: 9.8208\n","Epoch 165/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9227 - val_loss: 9.8173\n","Epoch 166/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9236 - val_loss: 9.8367\n","Epoch 167/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9233 - val_loss: 9.8343\n","Epoch 168/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9235 - val_loss: 9.8294\n","Epoch 169/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9227 - val_loss: 9.8244\n","Epoch 170/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9226 - val_loss: 9.8288\n","Epoch 171/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9223 - val_loss: 9.8269\n","Epoch 172/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9231 - val_loss: 9.8250\n","Epoch 173/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9236 - val_loss: 9.8210\n","Epoch 174/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9224 - val_loss: 9.8280\n","Epoch 175/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9233 - val_loss: 9.8273\n","Epoch 176/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9220 - val_loss: 9.8343\n","Epoch 177/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9216 - val_loss: 9.8296\n","Epoch 178/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9228 - val_loss: 9.8376\n","Epoch 179/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9236 - val_loss: 9.8296\n","Epoch 180/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9227 - val_loss: 9.8319\n","Epoch 181/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9213 - val_loss: 9.8296\n","Epoch 182/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9222 - val_loss: 9.8207\n","Epoch 183/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9217 - val_loss: 9.8305\n","Epoch 184/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9219 - val_loss: 9.8341\n","Epoch 185/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9220 - val_loss: 9.8315\n","Epoch 186/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9225 - val_loss: 9.8274\n","Epoch 187/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9216 - val_loss: 9.8325\n","Epoch 188/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9215 - val_loss: 9.8373\n","Epoch 189/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9226 - val_loss: 9.8340\n","Epoch 190/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9214 - val_loss: 9.8288\n","Epoch 191/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9214 - val_loss: 9.8311\n","Epoch 192/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9208 - val_loss: 9.8291\n","Epoch 193/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9222 - val_loss: 9.8262\n","Epoch 194/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9217 - val_loss: 9.8326\n","Epoch 195/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9215 - val_loss: 9.8256\n","Epoch 196/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9206 - val_loss: 9.8175\n","Epoch 197/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9224 - val_loss: 9.8217\n","Epoch 198/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9213 - val_loss: 9.8204\n","Epoch 199/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9219 - val_loss: 9.8250\n","Epoch 200/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9212 - val_loss: 9.8354\n","Epoch 201/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9214 - val_loss: 9.8404\n","Epoch 202/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9215 - val_loss: 9.8269\n","Epoch 203/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9220 - val_loss: 9.8280\n","Epoch 204/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9211 - val_loss: 9.8328\n","Epoch 205/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9205 - val_loss: 9.8239\n","Epoch 206/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9206 - val_loss: 9.8311\n","Epoch 207/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9227 - val_loss: 9.8329\n","Epoch 208/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9208 - val_loss: 9.8365\n","Epoch 209/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9204 - val_loss: 9.8373\n","Epoch 210/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9202 - val_loss: 9.8292\n","Epoch 211/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9202 - val_loss: 9.8381\n","Epoch 212/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9210 - val_loss: 9.8392\n","Epoch 213/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9201 - val_loss: 9.8440\n","Epoch 214/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9195 - val_loss: 9.8380\n","Epoch 215/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9197 - val_loss: 9.8443\n","Epoch 216/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9202 - val_loss: 9.8449\n","Epoch 217/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9209 - val_loss: 9.8366\n","Epoch 218/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9206 - val_loss: 9.8354\n","Epoch 219/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9213 - val_loss: 9.8443\n","Epoch 220/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9203 - val_loss: 9.8324\n","Epoch 221/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9200 - val_loss: 9.8389\n","Epoch 222/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9204 - val_loss: 9.8412\n","Epoch 223/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9204 - val_loss: 9.8309\n","Epoch 224/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9194 - val_loss: 9.8349\n","Epoch 225/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9198 - val_loss: 9.8324\n","Epoch 226/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9195 - val_loss: 9.8345\n","Epoch 227/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9193 - val_loss: 9.8346\n","Epoch 228/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9205 - val_loss: 9.8287\n","Epoch 229/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9197 - val_loss: 9.8266\n","Epoch 230/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9198 - val_loss: 9.8339\n","Epoch 231/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9194 - val_loss: 9.8340\n","Epoch 232/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9191 - val_loss: 9.8351\n","Epoch 233/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9187 - val_loss: 9.8446\n","Epoch 234/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9199 - val_loss: 9.8371\n","Epoch 235/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9190 - val_loss: 9.8326\n","Epoch 236/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9188 - val_loss: 9.8352\n","Epoch 237/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9192 - val_loss: 9.8283\n","Epoch 238/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9189 - val_loss: 9.8402\n","Epoch 239/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9192 - val_loss: 9.8394\n","Epoch 240/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9188 - val_loss: 9.8387\n","Epoch 241/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9191 - val_loss: 9.8396\n","Epoch 242/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9199 - val_loss: 9.8395\n","Epoch 243/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9198 - val_loss: 9.8291\n","Epoch 244/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9190 - val_loss: 9.8308\n","Epoch 245/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9185 - val_loss: 9.8343\n","Epoch 246/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9186 - val_loss: 9.8305\n","Epoch 247/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9194 - val_loss: 9.8365\n","Epoch 248/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9188 - val_loss: 9.8373\n","Epoch 249/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9183 - val_loss: 9.8438\n","Epoch 250/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9193 - val_loss: 9.8451\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Epoch 1/250\n","157/157 [==============================] - 1s 2ms/step - loss: 7.9168 - val_loss: 9.8460\n","Epoch 2/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9162 - val_loss: 9.8408\n","Epoch 3/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9159 - val_loss: 9.8366\n","Epoch 4/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9153 - val_loss: 9.8444\n","Epoch 5/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9158 - val_loss: 9.8443\n","Epoch 6/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9159 - val_loss: 9.8369\n","Epoch 7/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9153 - val_loss: 9.8379\n","Epoch 8/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9162 - val_loss: 9.8331\n","Epoch 9/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9162 - val_loss: 9.8334\n","Epoch 10/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9170 - val_loss: 9.8332\n","Epoch 11/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9147 - val_loss: 9.8379\n","Epoch 12/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9160 - val_loss: 9.8379\n","Epoch 13/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9150 - val_loss: 9.8425\n","Epoch 14/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9157 - val_loss: 9.8355\n","Epoch 15/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9157 - val_loss: 9.8475\n","Epoch 16/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9161 - val_loss: 9.8417\n","Epoch 17/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9155 - val_loss: 9.8328\n","Epoch 18/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9153 - val_loss: 9.8377\n","Epoch 19/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9154 - val_loss: 9.8401\n","Epoch 20/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9160 - val_loss: 9.8320\n","Epoch 21/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9142 - val_loss: 9.8379\n","Epoch 22/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9157 - val_loss: 9.8399\n","Epoch 23/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9149 - val_loss: 9.8423\n","Epoch 24/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9150 - val_loss: 9.8383\n","Epoch 25/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9153 - val_loss: 9.8354\n","Epoch 26/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9156 - val_loss: 9.8289\n","Epoch 27/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9149 - val_loss: 9.8305\n","Epoch 28/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9151 - val_loss: 9.8455\n","Epoch 29/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9151 - val_loss: 9.8521\n","Epoch 30/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9157 - val_loss: 9.8452\n","Epoch 31/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9147 - val_loss: 9.8412\n","Epoch 32/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9146 - val_loss: 9.8417\n","Epoch 33/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9143 - val_loss: 9.8297\n","Epoch 34/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9147 - val_loss: 9.8358\n","Epoch 35/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9150 - val_loss: 9.8354\n","Epoch 36/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9145 - val_loss: 9.8331\n","Epoch 37/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9142 - val_loss: 9.8442\n","Epoch 38/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9151 - val_loss: 9.8384\n","Epoch 39/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9151 - val_loss: 9.8454\n","Epoch 40/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9146 - val_loss: 9.8455\n","Epoch 41/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9148 - val_loss: 9.8323\n","Epoch 42/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9143 - val_loss: 9.8407\n","Epoch 43/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9159 - val_loss: 9.8305\n","Epoch 44/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9149 - val_loss: 9.8341\n","Epoch 45/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9145 - val_loss: 9.8348\n","Epoch 46/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9142 - val_loss: 9.8388\n","Epoch 47/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9148 - val_loss: 9.8415\n","Epoch 48/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9136 - val_loss: 9.8352\n","Epoch 49/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9143 - val_loss: 9.8464\n","Epoch 50/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9146 - val_loss: 9.8312\n","Epoch 51/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9145 - val_loss: 9.8349\n","Epoch 52/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9150 - val_loss: 9.8347\n","Epoch 53/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9149 - val_loss: 9.8306\n","Epoch 54/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9136 - val_loss: 9.8417\n","Epoch 55/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9145 - val_loss: 9.8390\n","Epoch 56/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9143 - val_loss: 9.8329\n","Epoch 57/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9133 - val_loss: 9.8415\n","Epoch 58/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9151 - val_loss: 9.8367\n","Epoch 59/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9132 - val_loss: 9.8388\n","Epoch 60/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9136 - val_loss: 9.8307\n","Epoch 61/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9144 - val_loss: 9.8343\n","Epoch 62/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9138 - val_loss: 9.8362\n","Epoch 63/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9135 - val_loss: 9.8365\n","Epoch 64/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9141 - val_loss: 9.8343\n","Epoch 65/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9142 - val_loss: 9.8343\n","Epoch 66/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9135 - val_loss: 9.8328\n","Epoch 67/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9138 - val_loss: 9.8338\n","Epoch 68/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9133 - val_loss: 9.8277\n","Epoch 69/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9125 - val_loss: 9.8376\n","Epoch 70/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9127 - val_loss: 9.8333\n","Epoch 71/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9130 - val_loss: 9.8446\n","Epoch 72/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9134 - val_loss: 9.8468\n","Epoch 73/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9132 - val_loss: 9.8340\n","Epoch 74/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9131 - val_loss: 9.8314\n","Epoch 75/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9135 - val_loss: 9.8406\n","Epoch 76/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9128 - val_loss: 9.8299\n","Epoch 77/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9136 - val_loss: 9.8325\n","Epoch 78/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9132 - val_loss: 9.8384\n","Epoch 79/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9129 - val_loss: 9.8305\n","Epoch 80/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9129 - val_loss: 9.8317\n","Epoch 81/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9131 - val_loss: 9.8339\n","Epoch 82/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9135 - val_loss: 9.8408\n","Epoch 83/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9128 - val_loss: 9.8381\n","Epoch 84/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9125 - val_loss: 9.8421\n","Epoch 85/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9132 - val_loss: 9.8424\n","Epoch 86/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9128 - val_loss: 9.8407\n","Epoch 87/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9126 - val_loss: 9.8363\n","Epoch 88/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9130 - val_loss: 9.8419\n","Epoch 89/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9129 - val_loss: 9.8463\n","Epoch 90/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9125 - val_loss: 9.8429\n","Epoch 91/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9131 - val_loss: 9.8435\n","Epoch 92/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9124 - val_loss: 9.8450\n","Epoch 93/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9125 - val_loss: 9.8432\n","Epoch 94/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9121 - val_loss: 9.8442\n","Epoch 95/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9131 - val_loss: 9.8425\n","Epoch 96/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9125 - val_loss: 9.8353\n","Epoch 97/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9119 - val_loss: 9.8334\n","Epoch 98/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9128 - val_loss: 9.8336\n","Epoch 99/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9124 - val_loss: 9.8415\n","Epoch 100/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9116 - val_loss: 9.8412\n","Epoch 101/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9122 - val_loss: 9.8297\n","Epoch 102/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9121 - val_loss: 9.8354\n","Epoch 103/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9122 - val_loss: 9.8429\n","Epoch 104/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9115 - val_loss: 9.8413\n","Epoch 105/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9126 - val_loss: 9.8442\n","Epoch 106/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9121 - val_loss: 9.8371\n","Epoch 107/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9118 - val_loss: 9.8310\n","Epoch 108/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9119 - val_loss: 9.8432\n","Epoch 109/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9125 - val_loss: 9.8391\n","Epoch 110/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9120 - val_loss: 9.8352\n","Epoch 111/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9121 - val_loss: 9.8288\n","Epoch 112/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9121 - val_loss: 9.8313\n","Epoch 113/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9115 - val_loss: 9.8367\n","Epoch 114/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9113 - val_loss: 9.8331\n","Epoch 115/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9117 - val_loss: 9.8358\n","Epoch 116/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9115 - val_loss: 9.8304\n","Epoch 117/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9123 - val_loss: 9.8452\n","Epoch 118/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9117 - val_loss: 9.8371\n","Epoch 119/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9120 - val_loss: 9.8433\n","Epoch 120/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9120 - val_loss: 9.8317\n","Epoch 121/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9107 - val_loss: 9.8434\n","Epoch 122/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9119 - val_loss: 9.8444\n","Epoch 123/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9110 - val_loss: 9.8308\n","Epoch 124/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9113 - val_loss: 9.8298\n","Epoch 125/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9108 - val_loss: 9.8373\n","Epoch 126/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9122 - val_loss: 9.8412\n","Epoch 127/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9116 - val_loss: 9.8428\n","Epoch 128/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9117 - val_loss: 9.8408\n","Epoch 129/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9118 - val_loss: 9.8450\n","Epoch 130/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9112 - val_loss: 9.8386\n","Epoch 131/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9108 - val_loss: 9.8334\n","Epoch 132/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9113 - val_loss: 9.8370\n","Epoch 133/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9106 - val_loss: 9.8370\n","Epoch 134/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9117 - val_loss: 9.8364\n","Epoch 135/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9104 - val_loss: 9.8400\n","Epoch 136/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9108 - val_loss: 9.8365\n","Epoch 137/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9113 - val_loss: 9.8423\n","Epoch 138/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9104 - val_loss: 9.8394\n","Epoch 139/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9107 - val_loss: 9.8370\n","Epoch 140/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9109 - val_loss: 9.8398\n","Epoch 141/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9105 - val_loss: 9.8431\n","Epoch 142/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9108 - val_loss: 9.8415\n","Epoch 143/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9104 - val_loss: 9.8353\n","Epoch 144/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9116 - val_loss: 9.8312\n","Epoch 145/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9103 - val_loss: 9.8363\n","Epoch 146/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9112 - val_loss: 9.8373\n","Epoch 147/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9102 - val_loss: 9.8345\n","Epoch 148/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9104 - val_loss: 9.8376\n","Epoch 149/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9104 - val_loss: 9.8294\n","Epoch 150/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9103 - val_loss: 9.8342\n","Epoch 151/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9101 - val_loss: 9.8362\n","Epoch 152/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9102 - val_loss: 9.8438\n","Epoch 153/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9105 - val_loss: 9.8413\n","Epoch 154/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9108 - val_loss: 9.8396\n","Epoch 155/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9103 - val_loss: 9.8365\n","Epoch 156/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9101 - val_loss: 9.8334\n","Epoch 157/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9102 - val_loss: 9.8418\n","Epoch 158/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9108 - val_loss: 9.8374\n","Epoch 159/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9100 - val_loss: 9.8363\n","Epoch 160/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9102 - val_loss: 9.8261\n","Epoch 161/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9103 - val_loss: 9.8312\n","Epoch 162/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9105 - val_loss: 9.8269\n","Epoch 163/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9109 - val_loss: 9.8371\n","Epoch 164/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9101 - val_loss: 9.8355\n","Epoch 165/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9097 - val_loss: 9.8311\n","Epoch 166/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9096 - val_loss: 9.8354\n","Epoch 167/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9089 - val_loss: 9.8311\n","Epoch 168/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9095 - val_loss: 9.8410\n","Epoch 169/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9094 - val_loss: 9.8350\n","Epoch 170/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9098 - val_loss: 9.8356\n","Epoch 171/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9101 - val_loss: 9.8355\n","Epoch 172/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9097 - val_loss: 9.8345\n","Epoch 173/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9097 - val_loss: 9.8349\n","Epoch 174/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9095 - val_loss: 9.8297\n","Epoch 175/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9097 - val_loss: 9.8332\n","Epoch 176/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9093 - val_loss: 9.8357\n","Epoch 177/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9094 - val_loss: 9.8410\n","Epoch 178/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9095 - val_loss: 9.8379\n","Epoch 179/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9092 - val_loss: 9.8257\n","Epoch 180/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9098 - val_loss: 9.8357\n","Epoch 181/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9093 - val_loss: 9.8392\n","Epoch 182/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9092 - val_loss: 9.8322\n","Epoch 183/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9099 - val_loss: 9.8267\n","Epoch 184/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9093 - val_loss: 9.8331\n","Epoch 185/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9097 - val_loss: 9.8304\n","Epoch 186/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9104 - val_loss: 9.8385\n","Epoch 187/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9092 - val_loss: 9.8321\n","Epoch 188/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9090 - val_loss: 9.8339\n","Epoch 189/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9087 - val_loss: 9.8315\n","Epoch 190/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9090 - val_loss: 9.8279\n","Epoch 191/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9092 - val_loss: 9.8276\n","Epoch 192/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9090 - val_loss: 9.8308\n","Epoch 193/250\n","157/157 [==============================] - 0s 1ms/step - loss: 7.9092 - val_loss: 9.8369\n","Epoch 194/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9086 - val_loss: 9.8342\n","Epoch 195/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9090 - val_loss: 9.8342\n","Epoch 196/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9089 - val_loss: 9.8322\n","Epoch 197/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9089 - val_loss: 9.8314\n","Epoch 198/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9080 - val_loss: 9.8334\n","Epoch 199/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9092 - val_loss: 9.8351\n","Epoch 200/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9083 - val_loss: 9.8258\n","Epoch 201/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9086 - val_loss: 9.8304\n","Epoch 202/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9086 - val_loss: 9.8369\n","Epoch 203/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9076 - val_loss: 9.8376\n","Epoch 204/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9088 - val_loss: 9.8330\n","Epoch 205/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9083 - val_loss: 9.8263\n","Epoch 206/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9089 - val_loss: 9.8335\n","Epoch 207/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9082 - val_loss: 9.8323\n","Epoch 208/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9087 - val_loss: 9.8339\n","Epoch 209/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9093 - val_loss: 9.8317\n","Epoch 210/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9086 - val_loss: 9.8321\n","Epoch 211/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9087 - val_loss: 9.8338\n","Epoch 212/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9070 - val_loss: 9.8416\n","Epoch 213/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9088 - val_loss: 9.8398\n","Epoch 214/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9081 - val_loss: 9.8330\n","Epoch 215/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9078 - val_loss: 9.8396\n","Epoch 216/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9081 - val_loss: 9.8349\n","Epoch 217/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9081 - val_loss: 9.8467\n","Epoch 218/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9085 - val_loss: 9.8456\n","Epoch 219/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9084 - val_loss: 9.8372\n","Epoch 220/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9076 - val_loss: 9.8357\n","Epoch 221/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9082 - val_loss: 9.8305\n","Epoch 222/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9079 - val_loss: 9.8302\n","Epoch 223/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9082 - val_loss: 9.8289\n","Epoch 224/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9087 - val_loss: 9.8314\n","Epoch 225/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9078 - val_loss: 9.8199\n","Epoch 226/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9075 - val_loss: 9.8248\n","Epoch 227/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9076 - val_loss: 9.8311\n","Epoch 228/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9075 - val_loss: 9.8309\n","Epoch 229/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9071 - val_loss: 9.8331\n","Epoch 230/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9079 - val_loss: 9.8320\n","Epoch 231/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9082 - val_loss: 9.8307\n","Epoch 232/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9076 - val_loss: 9.8304\n","Epoch 233/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9082 - val_loss: 9.8348\n","Epoch 234/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9079 - val_loss: 9.8379\n","Epoch 235/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9084 - val_loss: 9.8354\n","Epoch 236/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9069 - val_loss: 9.8365\n","Epoch 237/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9082 - val_loss: 9.8367\n","Epoch 238/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9079 - val_loss: 9.8374\n","Epoch 239/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9080 - val_loss: 9.8314\n","Epoch 240/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9077 - val_loss: 9.8351\n","Epoch 241/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9079 - val_loss: 9.8354\n","Epoch 242/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9074 - val_loss: 9.8348\n","Epoch 243/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9075 - val_loss: 9.8296\n","Epoch 244/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9071 - val_loss: 9.8410\n","Epoch 245/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9077 - val_loss: 9.8377\n","Epoch 246/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9070 - val_loss: 9.8292\n","Epoch 247/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9075 - val_loss: 9.8315\n","Epoch 248/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9074 - val_loss: 9.8299\n","Epoch 249/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9070 - val_loss: 9.8204\n","Epoch 250/250\n","157/157 [==============================] - 0s 2ms/step - loss: 7.9073 - val_loss: 9.8255\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwU9Z3/8dene4YZYLiRAQEFvBWCyohoIoJmvZKoMfrDK1E3SmKMZy6y2UTXaC6zSTaJiTGJ0WQ1SNQkbOKqiTKiUYmgI4cosqjIAHLjDMzZ/fn98a2Bnpma0+kZoN/Px6MfXf2tb1V9vlXV9amjq9rcHRERkaYSPR2AiIjsmZQgREQklhKEiIjEUoIQEZFYShAiIhIrr6cD6EpDhw71MWPGdGrYHTt20Ldv364NaA+nNucGtTk3dLbNixYt2uTu+8X126cSxJgxY1i4cGGnhi0tLWXatGldG9AeTm3ODWpzbuhsm83s7Zb66RSTiIjEUoIQEZFYShAiIhJLCUJERGIpQYiISCwlCBERiaUEISIisZQgAFJ1738c7/ex6e7w+mPw2qPvf1wiIl1gn7pRrlPSafjZCRyZ2A9GpSCZDy/8HNIpmHghHHF2qJfMazxMbQVsegPWvAgHngiPfAbMYNosOPIcqKmADa/BfodCQX/Y/g6sXwKbVsDwCTBmKuT1CuOrq4IH/h+8OT983v8YOPJc6NUXjvo49B26e9p11fDUN6H/SKjaCq8/Cr2K4GM/CjH3HggDRu2uX/EurH4+jKNqK8z7Npx7Z5hGg1QdvPpn6LsfHHACVL4L616BcSdD7c5QnujgvsRb/4An/wPeWwsfvgUmnB+mv2UVjJwUzccUWCLMNwiJ8Z1/QiIJ1duhfFGoM3A0jJsehl30G1j7Mux/LOCwYyMk8kP7anfAwAPg6IvDPG6YX8v+CPVVcOiZoWzLm1B2f5hvm1fC4LFw/GfDtP7xIygeD4eeHuZ/S7avgb9+AcafD9XbwrjGnQzz7wgxnfB5OGDK7mXwf0+GaRUOgO3lULkeTrgWRh/XeLx11fDuUhg8Dt7+B1SsD/NryMGh3W//A9L1MLIEDj0jfrm4756nwMCti+F//gT7HQajjgvzJq8AqrbB6/8LBxwfptdea8sg2QuKj2y9Xn0trH4OLAmjJ4dpQlgneg+G/MImba8KdRu+F10hnQrrU3vqvfrnMN8O/FD4Hm19G/J7w4OXQsU6GPMhOOM7jb+PnVVf27ydTZZbI2vLwvd05LHta08XsX3pD4NKSkq8w3dS1+6Ap26nbuFvya+vCGX9RoSVeetb4XPhADjsI9CrD9RXwxt/D1/wTL2KwoZ542th47V+cfgiFxWH8vJFjevn94GDToHjPwOL7oWlD8NZ3w/Tff5nsHF5qDfkkPAFXrMwTKO+Bt5dsns8Y0+GDa+Glad6e4j1+M+Esu3lsPal5m3uPwpGT2Zr+QoGjT4CdmyCVfN296t5L7waHPjBsJFasxC2rYZ0HQwaC70HQUG/8J6qCe15+o4wbMV66Dc8JMcNr8LkmbB8LrxXDh+8AXBYeC/0Kw4b54p1sPoFeOuZ1pdX70EwanJItvmFIXnVV8POLWGeblsdYhk3DfrvHxLOphXRct2fdwYcx+j1j4dhGpZt9XYoGg5F+4XxAuT1hvHnwcSL4J0XoOz3YTlatEHesTFsyBvFNjhs5PJ7h2X/sf+CN5+GRfeF9loCPA3JgrAuVW0NG+zKd8MySPYKcTXEFscSgIGnYNiRIUms/HtIPr2KoGhYWG+P/yzUVoZ5uq4M8gp3jzfZKySJivVheQAUDICjzoGTvgAblof5Ovc62Lk5bJT2Oyws/0FjoOyBMP0RE3fHUjgwbNwGHhh2mFY+CW88sXs9GnAAnH47lH47rA8DRsOp3whtePVPYT35x3+F9f+Is8MO1rAj4KDpITlufSvMx6UPh2TWZzD0GRK6k73CzsPWN+GQ0+Goj7P6wS9zwIYnoWZ7SKZTrg7Lb8lDIfHv3BTWpaLikChX/j3MJwjzse9+YXyJ/DDvDj8r7GhgMOSgsMMzsiSMo3ZHiPfgD4eE/e4yWLcY6naEnZFeRaFen8Fh/Xz1z2E96zcixFJTEeZTOgUTLtj9/Ukkw/o55GB4/qchtqGHwilfD8P0Gx6SVl4BuFP69NOdvZN6kbuXxPbL+QQRefqpv3HyftvDF//Yy8JML3sg7CluXgmrStn1JR9ZAmM+CH2Ghr2oRfeGDcn+x8LT3w0r8aFnhC/Wcz8JC/y4K8OGbchB4Uu7qhQWzw4bJ4DpX4OTv7w7oB2bwgbo9xeHjc3Bp4aV4r21MO2rMPTgEMuIibB+Kdx/Qdgoli+CTa/D4IPCCjju5LDhLl8EG18PG73/Ph8K+7MtOZSBdevDhuqM74YN6nM/CUdRJ1wTNgjJXmHlrKsK7Rk8Lqy4m1eFo6iq7aENngobpD5Do713h/N/E8b18JVhgzFgZNg7f+0vgMFhZ4bYt68ObRk0JsynAaPCdMdNC1/QzW/Aa38NG4SJF4YNcEuqtsL874ejsR0bQzynfj1svGdfhO/YhI3/BPzLreHLWTggJJEn/j0k0/N+GTYOyx4Jy79hozrmpLCxtOjoZvs7MON34QiraBgsngMrHoNL/hA2Or8+DSrWhvqTZ8Ixl8J+h4d5lsgP41x4Dyx+MBz1DDko7FUm88N83rQybJT3PyYsu80rYfgH4MATQiJc9kd4/s6wURv+ARg7Nawzle+GeffG4yERHTCFVYxi3MX/GebNmoVQvjC8p2rh5K+EBLp+KbzyQON52XdY2Oitmhc2WPsdFnaADvtIWMbvLAjxWjIcRXk6jKduR1hWh50Jh380TOd/rg/T7z8KJl8JSx5uvKMDoR15BeHotXBAWH6ZLBGOAndsDBvlirVhnBASTN+hIVFgOGBHnRt2ZF77a/hOQEj86boQX9W2sDNhifB9mTYr1P/Hj0LiPPKcMJ8+eD2MKgkb/rIHdh/JNTX+/NDW5XN3l2Um5gajjw9JdHt5iKWgX4i/ahssmRPWv6GHhISx9a2wrh16Zjib8PdbQrsbjDg6rC+b/4/SA29SgmjN+0kQPfLslp1b4O3nwgZixAfi62x9O6xk/YpbH1fD4WmqLnxpioa1XLf6PehVROn8+Uw76UNhw957YMv1ayoBDytya3WWPRJOBQ0c3bx/fc3uDeOm18MpssL+4ZRKxdqw4ejKUwtxKtbzwrNPM+XMGc37uYcNf+Z8eG9tOLQfPqFxm1L1YUM8YGTjcaTqd5+KrKmAjSvCXuPgsV3flgbV28PGpempifVLQ8LvM7j96/Zrfw0bxDEfChvD8eeFZJ2Okn/hgN1HSC2p2hb2vId/oPGpkHWL4aXfhp2gomHhNO2K/w2Jb+LFIXEOOCDMv4Z1+b114dRc5bsh0R98auPTp+4hrvraMJ8h7Kxte5t/1hzE5I98MpSl02GnrWIdlPwrFBSF8lQ91O0MCTfZwbPtW98KO49VW8N6veaf8Mx/hnl03FXRjkyfsAw8HeZL1ZawU9J//5bHW1cdkmTD8kynop2AiSHGqm0hQQ0YHcr/clNIQBPOZ36/c5l66mkdawdKEO2ih3vlBrU5N/RIm9cvCaeDWkugXa2mYte1x/fxsL4WE4QuUouIdIWGH0Z0p9aO6ruAfuYqIiKxspogzOx6M1tqZsvM7IaY/l8ys7LotdTMUmY2OOr3lpktifp17ryRiIh0WtZOMZnZeOAqYDJQCzxmZn9x95UNddz9DuCOqP7HgBvdfUvGaKa7+6ZsxSgiIi3L5hHEEcACd9/p7vXA08B5rdS/CPh9FuMREZEOyGaCWAqcZGZDzKwPcBYQ8/tHiPqfATycUezAE2a2yMxmZjFOERGJkdWfuZrZp4HPATuAZUCNu8ddi5gBXOruH8soG+nu5WY2DPgbcK27z48ZdiYwE6C4uHjS7NmzOxVrZWUlRUVFnRp2b6U25wa1OTd0ts3Tp09v8WeuuHu3vIBvAZ9rod8fgYtbGfYW4IttTWPSpEneWfPmzev0sHsrtTk3qM25obNtBhZ6C9vUbP+KaVj0fgDh+sMDMXUGACcDf84o62tm/Rq6gdMIp6xERKSbZPtGuYfNbAhQB1zj7tvM7LMA7n5XVOfjwBPuviNjuGLgjxZuN88DHnD3x7Icq4iIZMhqgnD3k2LK7mry+V7g3iZlq4CJ2YxNRERapzupRUQklhKEiIjEUoIQEZFYShAiIhJLCUJERGIpQYiISCwlCBERiaUEISIisZQgREQklhKEiIjEUoIQEZFYShAiIhJLCUJERGIpQYiISCwlCBERiaUEISIisZQgREQkVrb/k/p6M1tqZsvM7IaY/tPMbLuZlUWvb2T0O8PMXjezlWY2K5txiohIc1n7y1EzGw9cBUwGaoHHzOwv7r6ySdVn3P2jTYZNAncC/wKsAV40s7nu/mq24hURkcayeQRxBLDA3Xe6ez3wNHBeO4edDKx091XuXgvMBs7JUpwiIhIja0cQwFLgdjMbAlQBZwELY+qdYGavAGuBL7r7MmAk8E5GnTXA8XETMbOZwEyA4uJiSktLOxVsZWVlp4fdW6nNuUFtzg3ZaHPWEoS7Lzez7wJPADuAMiDVpNpLwIHuXmlmZwF/Ag7p4HTuBu4GKCkp8WnTpnUq3tLSUjo77N5Kbc4NanNuyEabs3qR2t1/7e6T3H0qsBVY0aT/e+5eGXU/CuSb2VCgHBidUXVUVCYiIt0k279iGha9H0C4/vBAk/7Dzcyi7slRPJuBF4FDzGysmfUCLgTmZjNWERFpLJvXIAAejq5B1AHXuPs2M/ssgLvfBZwPXG1m9YTrFBe6uwP1ZvZ54HEgCdwTXZsQEZFuktUE4e4nxZTdldH9U+CnLQz7KPBo9qITEZHW6E5qERGJpQQhIiKxlCBERCSWEoSIiMRSghARkVhKECIiEksJQkREYilBiIhILCUIERGJpQQhIiKxlCBERCSWEoSIiMRSghARkVhKECIiEksJQkREYilBiIhIrGz/5ej1ZrbUzJaZ2Q0x/S8xs8VmtsTMnjOziRn93orKy8xsYTbjFBGR5rL2j3JmNh64CpgM1AKPmdlf3H1lRrU3gZPdfauZnQncDRyf0X+6u2/KVowiItKybB5BHAEscPed7l4PPA2cl1nB3Z9z963RxxeAUVmMR0REOsDcPTsjNjsC+DNwAlAFPAksdPdrW6j/ReBwd78y+vwmsBVw4BfufncLw80EZgIUFxdPmj17dqfirayspKioqFPD7q3U5tygNueGzrZ5+vTpi9y9JLanu2ftBXwaWATMB34O/KiFetOB5cCQjLKR0fsw4BVgalvTmzRpknfWvHnzOj3s3kptzg1qc27obJsJO+6x29SsXqR291+7+yR3n0o4GljRtI6ZfQD4FXCOu2/OGLY8et8A/JFwLUNERLpJtn/FNCx6P4Bw/eGBJv0PAB4BPunuKzLK+5pZv4Zu4DRgaTZjFRGRxrL2K6bIw2Y2BKgDrnH3bWb2WQB3vwv4BjAE+JmZAdR7OBdWDPwxKssDHnD3x7Icq4iIZMhqgnD3k2LK7srovhK4MqbOKmBi03IREek+upNaRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShIiIxGr1Z65mtrgd49jo7qd2UTwiIrKHaOs+iCRwViv9DZjbdeGIiMieoq0E8Rl3f7u1Cmb2uS6MR0RE9hCtXoNw92fbGkF76oiIyN6n1QRhZueY2TUZnxeY2arodUH2wxMRkZ7S1q+YvkzjawwFwHHANOCzWYpJRET2AG1dg+jl7u9kfH42+s+GzdFjuEVEZB/V1hHEoMwP7v75jI/7dX04IiKyp2grQSwws6uaFprZZ4B/ZickERHZE7R1iulG4E9mdjHwUlQ2iXAt4txsBiYi0h51dXWsWbOG6urqXWUDBgxg+fLlPRhV92urzYWFhYwaNYr8/Px2j7PVBBH9H/SJZnYKcFRU/Fd3f6rdUxARyaI1a9bQr18/xowZQ/QvlFRUVNCvX78ejqx7tdZmd2fz5s2sWbOGsWPHtnucbf3MtdDMbiD8n3Qt8POOJAczu97MlprZsmg8Tfubmf3YzFaa2WIzOzaj32Vm9kb0uqzdLRKRnFJdXc2QIUN2JQdpzswYMmRIo6Os9mjrFNN9hP+TfgY4EzgCaLahbyGg8cBVwGRCcnnMzP7i7iszqp0JHBK9jgd+DhxvZoOBm4ESwIFFZjbX3be2t2EikjuUHNrWmXnU1kXqI939Unf/BXA+MLUD4z4CWODuO929HniacCSS6Rzgtx68AAw0sxHA6cDf3H1LlBT+BpzRgWmLiMj71NYRRF1Dh7vXdzADLQVuN7MhQBXhoX8Lm9QZCWTeZ7EmKmupvBkzmwnMBCguLqa0tLQjMe5SWVnZ6WH3VmpzbtjX2zxgwAAqKioalaVSqWZl2TRixAjWrVvXbdOL0542V1dXd2hdaCtBTDSz96JuA3pHnw1wd+/f0oDuvtzMvgs8AewAyoBUuyNrJ3e/G7gboKSkxKdNm9ap8ZSWltLZYfdWanNu2NfbvHz58mYXZ3viInVPXxRvT5sLCws55phj2j3Oth7Wl3T3/tGrn7vnZXS3mBwyhv+1u09y96nAVmBFkyrlwOiMz6OispbKRUT2WO7Ol770JcaPH8+ECRN48MEHAVi3bh1Tp07l6KOPZvz48TzzzDOkUikuv/zyXXV/+MMf9nD0zbX1h0GDW+vv7lvaGH6Yu28wswMI1x+mNKkyF/i8mc0mXKTe7u7rzOxx4Ftm1nAn92nAV1ublojIf/zPMl5d+x6pVIpkMtkl4zxy//7c/LGj2q4IPPLII5SVlfHKK6+wadMmjjvuOKZOncoDDzzA6aefzte+9jVSqRQ7d+6krKyM8vJyli5dCsC2bdu6JN6u1NYppk2E8//10efMixAOjGtj+IejaxB1wDXuvs3MPgvg7ncBjxKuTawEdgJXRP22mNk3gRej8dzaVjISEelpzz77LBdddBHJZJLi4mJOPvlkXnzxRY477jj+9V//lbq6Os4991yOPvpoxo0bx6pVq7j22mv5yEc+wmmnndbT4TfTVoL4MTAd+Afwe8LD+ry9I3f3k2LK7sroduCapnWifvcA97R3WiIiDXv6e9qNclOnTmX+/Pn89a9/5fLLL+emm27iU5/6FK+88gqPP/44d911F3PmzOGee/asTV5b1yBuAI4G/gB8EnjZzL5nZu2/FU9EJEecdNJJPPjgg6RSKTZu3Mj8+fOZPHkyb7/9NsXFxVx11VVceeWVvPTSS2zatIl0Os0nPvEJbrvtNl566aW2J9DN2jqCaNjLn2dmLwMXAt8E3gB+meXYRET2Kh//+Md5/vnnmThxImbG9773PYYPH859993HHXfcQX5+PkVFRfz2t7+lvLycK664gnQ6DcC3v/3tHo6+ubYuUvcl3Mw2g/B470eASe6+uhtiExHZK1RWVgLhbuU77riDO+64o1H/yy67jMsua/7EoD3xqCFTW0cQGwhHC7OjdwdKzKwEwN0fyW54IiLSU9pKEH8gJIXDolcmJxxRiIjIPqitx31f3k1xiIjIHqatx31/tK0RtKeOiIjsfdo6xXSHmZXT+Aa5pr4F/KXrQhIRkT1BWwniXeAHbdR5o4tiERGRPUhb1yCmdVMcIiKyh2nrD4NERKQLFRUVtdjvrbfeYvz48d0YTeuUIEREJFabj9owswQwxd2f64Z4REQ6739nwfol9E7VQ7LNzVv7DJ8AZ36nxd6zZs1i9OjRXHNNeO7oLbfcQl5eHvPmzWPr1q3U1dVx2223cc4553RostXV1Vx99dUsXLiQvLw8fvCDHzB9+nSWLVvGFVdcQW1tLel0mocffpj999+f888/n/Xr15NKpfj617/OjBkz3lezoX3PYkqb2Z1A+/+GSEQkR8yYMYMbbrhhV4KYM2cOjz/+ONdddx39+/dn06ZNTJkyhbPPPpuO/G3znXfeiZmxZMkSXnvtNU477TRWrFjBXXfdxfXXX88ll1xCbW0tqVSKRx99lBEjRvD4448DsH379i5pW3tT7JNm9gngkY487ltEpFtFe/pV3fi472OOOYYNGzawdu1aNm7cyKBBgxg+fDg33ngj8+fPJ5FIUF5ezrvvvsvw4cPbPd5nn32Wa6+9FoDDDz+cAw88kBUrVnDCCSdw++23s2bNGs477zwOOeQQJkyYwE033cRXvvIVPvrRj3LSSc3+aaFT2nsN4jOEx27Umtl7ZlaR8V/VIiI57YILLuChhx7iwQcfZMaMGdx///1s3LiRRYsWUVZWRnFxMdXV1V0yrYsvvpi5c+fSu3dvzjrrLJ566ikOPfRQ5s+fz4QJE/j3f/93br311i6ZVruOINy9U6nYzG4EriQ8t2kJcIW7V2f0/yHhD4kA+gDD3H1g1C8VDQOw2t3P7kwMIiLZNmPGDK666io2bdrE008/zZw5cxg2bBj5+fnMmzePt99+u8PjPOmkk7j//vs55ZRTWLFiBatXr+awww5j1apVjBs3juuuu47Vq1ezePFiDj/8cPr06cOll17KwIED+dWvftUl7Wr3VRwzOxuYGn0sdfdW7542s5HAdcCR7l5lZnMI/ydxb0Mdd78xo/61NL7OUeXuR7c3PhGRnnLUUUdRUVHByJEjGTFiBJdccgkf+9jHmDBhAiUlJRx++OEdHufnPvc5rr76aiZMmEBeXh733nsvBQUFzJkzh9/97nfk5+czfPhw/u3f/o0XX3yRL3zhC+Tl5ZGfn8/Pf/7zLmlXuxKEmX0HOA64Pyq63sw+6O5fbcf4e5tZHeEIYW0rdS8Cbm5PPCIie5olS5bs6h46dCjPP/98bL2G/46IM2bMGJYuXQpAYWEhv/nNb5rVmTVrFrNmzWpUdvrpp3PiiSd2+XUXa881ZzNbDBzt7unocxJ42d0/0MZw1wO3A1XAE+5+SQv1DgReAEa5eyoqqwfKgHrgO+7+pxaGnQnMBCguLp40e/bsNtsTp7KystUbWPZFanNu2NfbPGDAAA4++OBGZalUimQy2UMR9Yz2tHnlypXNfuE0ffr0Re5eEle/Iz8UHghsiboHtFXZzAYR/o1uLLAN+IOZXeru/x1T/ULgoYbkEDnQ3cvNbBzwlJktcff/azqgu98N3A1QUlLi06ZN60CTdistLaWzw+6t1ObcsK+3efny5c32nCu68VdMnbFkyRI++clPNiorKChgwYIFnR5ne9pcWFjIMce0/46F9iaIbwEvm9k8wpNdpwKzWh+EDwNvuvtGADN7BDgRaClBXJNZ4O7l0fsqMyslXJ9oliBERNy9Q/cY9LQJEyZQVlbWrdPszB0Kbf7MNbqTOg1MIfyD3MPACe7+YBuDrgammFkfC0vuVGB5zPgPBwYBz2eUDTKzgqh7KPBB4NV2tUhEckphYSGbN2/u1AYwV7g7mzdvprCwsEPDtfdO6i+7+xxgbgcCWmBmDwEvEa4jvAzcbWa3AgvdvWFcFwKzm9yAdwTwCzNLE5LYd9xdCUJEmhk1ahRr1qxh48aNu8qqq6s7vDHc27XV5sLCQkaNGtWhcbb3FNPfzeyLwIPAjoZCd9/S8iDg7jfT/JdJ32hS55aY4Z4DJrQzNhHJYfn5+YwdO7ZRWWlpaYfOte8LstHm9iaIhqc+ZV4ncGBcl0YjIiJ7jPY+zXVWO645iIjIPqTNi9TRvQ9f6oZYRERkD9Leh/X93cy+aGajzWxwwyurkYmISI/SNQgREYnV3qe5jm27loiI7EtaPcVkZl/O6L6gSb9vZSsoERHpeW1dg7gwo7vpk1vP6OJYRERkD9JWgrAWuuM+i4jIPqStBOEtdMd9FhGRfUhbF6knRv89bYQ//mn4H2oDcutBJyIiOabVBOHuufWPGyIiskt7b5QTEZEcowQhIiKxlCBERCSWEoSIiMTKaoIwsxvNbJmZLTWz35tZYZP+l5vZRjMri15XZvS7zMzeiF6XZTNOERFprr0P6+swMxsJXAcc6e5VZjaHcGf2vU2qPujun28y7GDCP9GVEO63WGRmc919a7biFRGRxrJ9iimPcP9EHtAHWNvO4U4H/ubuW6Kk8Df0aA8RkW6VtQTh7uXA94HVwDpgu7s/EVP1E2a22MweMrPRUdlI4J2MOmuiMhER6Sbmnp0nZpjZIOBhwn9JbAP+ADzk7v+dUWcIUOnuNWb2GWCGu59iZl8ECt39tqje14Eqd/9+zHRmAjMBiouLJ82ePbtT8VZWVlJUVNSpYfdWanNuUJtzQ2fbPH369EXuXhLb092z8gIuAH6d8flTwM9aqZ8kHGUAXAT8IqPfL4CL2prmpEmTvLPmzZvX6WH3VmpzblCbc0Nn2wws9Ba2qdm8BrEamGJmfczMgFOB5ZkVzGxExsezM/o/DpxmZoOiI5HTojIREekmWfsVk7svMLOHgJeAeuBl4G4zu5WQseYC15nZ2VH/LcDl0bBbzOybwIvR6G519y3ZilVERJrLWoIAcPebCT9XzfSNjP5fpfkfETX0uwe4J3vRiYhIa3QntYiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShIiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShIiIxMpqgjCzG81smZktNbPfm1lhk/43mdmrZrbYzJ40swMz+qXMrCx6zc1mnCIi0lzWEoSZjQSuA0rcfTyQBC5sUu3lqP8HgIeA72X0q3L3o6PX2dmKU0RE4mX7FFMe0NvM8oA+wNrMnu4+z913Rh9fAEZlOR4REWknc/fsjdzseuB2oAp4wt0vaaXuT4H17n5b9LkeKAPqge+4+59aGG4mMBOguLh40uzZszsVa2VlJUVFRZ0adm+lNucGtTk3dLbN06dPX+TuJbE93T0rL2AQ8BSwH5AP/Am4tIW6lxKOIAoyykZG7+OAt4CD2prmpEmTvLPmzZvX6WH3VmpzblCbc0Nn2wws9Ba2qdk8xfRh4E133+judcAjwIlNK5nZh4GvAWe7e01DubuXR++rgFLgmOE+zM8AAAyoSURBVCzGKiIiTWQzQawGpphZHzMz4FRgeWYFMzsG+AUhOWzIKB9kZgVR91Dgg8CrWYxVRESayMvWiN19gZk9BLxEuI7wMnC3md1KOKSZC9wBFAF/CDmE1R5+sXQE8AszSxOS2HfcXQlCRKQbZS1BALj7zcDNTYq/kdH/wy0M9xwwIYuhiYhIG3QntYiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShIiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJFZWE4SZ3Whmy8xsqZn93swKm/QvMLMHzWylmS0wszEZ/b4alb9uZqdnM04REWkuawnCzEYC1wEl7j4eSAIXNqn2aWCrux8M/BD4bjTskVHdo4AzgJ+ZWTJbsYqISHPZPsWUB/Q2szygD7C2Sf9zgPui7oeAUy38OfU5wGx3r3H3N4GVwOQsxyoiIhmy9p/U7l5uZt8HVgNVwBPu/kSTaiOBd6L69Wa2HRgSlb+QUW9NVNaMmc0EZgIUFxdTWlraqXgrKys7PezeSm3ODWpzbshGm7OWIMxsEOFIYCywDfiDmV3q7v/dldNx97uBuwFKSkp82rRpnRpPaWkpnR12b6U25wa1OTdko83ZPMX0YeBNd9/o7nXAI8CJTeqUA6MBotNQA4DNmeWRUVGZiIh0k2wmiNXAFDPrE11XOBVY3qTOXOCyqPt84Cl396j8wuhXTmOBQ4B/ZjFWERFpIpvXIBaY2UPAS0A98DJwt5ndCix097nAr4HfmdlKYAvRr5zcfZmZzQFejYa9xt1T2YpVRESay1qCAHD3m4GbmxR/I6N/NXBBC8PeDtyevehERKQ1upNaRERiKUGIiEisrJ5i2lt89CfP8N57Vey3/Dl65ycpyEtQl3bq6tPkJY28hLGjNsXO2np21qQYPqCQooI8dtTW078wn7qUU5tK4+6Yhfr5SaNXXpL8pIXxpZyNFTXkJYxeeQmSCcPMMMCM6N2A0I2BYURFGfWiOkbUL+MzsGVHLZU19fQrzKOoII9kwprVSbuTSjvla2v487tlJMwYWtSLHbX1JM3oU5BH2kO8IwYUUpCXpD6Vpj4dhkulnbSH8SUMEmYkEkbSjGQitD+ZDJ9T7rhDOu2k3EmYUZifoLImBe67hkskwrAN3UYYt5llxE00bQ91ExambbvnU4PMj2bQtyCPhBmL19az9eU1HVo/jLDMCvPD/lRNXZraVJqa+jQGu+IoyEswqG8vtlfVUZCXoFcyQSqa1x7FnnIHID9p5CUS0foV1ofa+jQ7a+sZUlRA2p3q2hSJhFFUEL6mDW1vmPcA23fWMaBPPul0GO+A3vnN5sXayjQrN1SEljRanyyje3dbM4c3210vaUZRYR7uTl3KqUulqUulo3q2a543Gj5j3WuYVmZHi/2bxLcrFgxLROucEa1viV3rSpx02nHC+lSXcvKTtquuR/MTwrg8Wl8Tifhx5RolCOCg/Yp4u3YHhfkJdtTWs2VHOtrAJ6iqC1/wvgVJivsVUjg4yZqtO9lYUUO/wjw2vFdDXjIRNvoGKYdUOk1dfUgatfVhY5I0Y1j/8MWvqUuHjQZhBQ3vIRYnrKANn2mtzq7+UanDgD759C/MZ932aiqr63dtkDLrNGyU62pTrK7eQn3K2VxZS5+CJO6wo6YeMxhaVMCGihpS0Tcor2EjHm0QYXey2f3e/vlu1rid3WbxKz0w0R727PyejiDrGtbNvITh6RQF858gYcb2qjpSad+1viWjHZCGnRcI6+KwfgVsr6qjui4ddhTzE2GdTjv10TpuZvQryMMhSvweDR++D3mJsIPT8F0xg6raNFW19SQSYSeitj69K8EO7JNPXiLB1p21jeJPZuwwtZT4Mg0t6sWN47t+nipBAP914THRTSZTejqUbtXSjTWZe1H10R5iwxFPWzxKFA1fqIY9/IYjjbQ71fVp+uQnSSRs15FF0ySTjkmgDXuMZruHS8ckJadxQSrt7KxN4Q6LFr7ICVOOpyP7h2kPyb66Lhwx9MpLUJCXID+Z2DX+tDvVdWm27KhlYJ98aupT1NY3HOmEtmcm1vq0Nzoqq087eQmjT68kW3aEjUXv/CT1aaeypn73PIzGkY72EPr3zmd7VS15iQRpdyqq65vF/+qrr3LEkUfu2phB852NhrKGeR76sWsvxAkxVlaHWHpF7c+L9rQb5nmjHZuYaTX+3LhC5lLbHVPjfmlv2Ov3XUeUDfMvlU6TSocdtLdWv8P+++9PfdoZ2Cef/GSCdNrplZdgZ234QWTmEWgqnWbt9moG9s6nT0EeVbX11NSnSVjjDXbKwzwwo9nG2333ulyf2r1O9+6VpHd+Ho5TU5+mV7RDmZcwtuyopT7tDO7bq1Eiqk87qdTuI86WhGXm9CvIAza1WrczlCCkmYbTOgB5yY5dpjKzcNqkhUcrJjCKMsaZSBgJjPxuehTjuqIEY4f27Z6J7SH6bV3BtIn793QY3aq0dAPTpmVhl3oPlo1Hi+gitYiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJZ5G3fq7U3MbCPwdicHH0o2bkXcs6nNuUFtzg2dbfOB7r5fXI99KkG8H2a20N1LejqO7qQ25wa1OTdko806xSQiIrGUIEREJJYSxG5393QAPUBtzg1qc27o8jbrGoSIiMTSEYSIiMRSghARkVg5nyDM7Awze93MVprZrJ6OJ1vM7C0zW2JmZWa2MCobbGZ/M7M3ovdBPR3n+2Vm95jZBjNbmlEW204Lfhwt+8VmdmzPRd55LbT5FjMrj5Z3mZmdldHvq1GbXzez03sm6vfHzEab2Twze9XMlpnZ9VH5PrusW2lz9pZ1+HvJ3HwBSeD/gHFAL+AV4MiejitLbX0LGNqk7HvArKh7FvDdno6zC9o5FTgWWNpWO4GzgP8FDJgCLOjp+LuwzbcAX4ype2S0nhcAY6P1P9nTbehEm0cAx0bd/YAVUdv22WXdSpuztqxz/QhiMrDS3Ve5ey0wGzinh2PqTucA90Xd9wHn9mAsXcLd5wNbmhS31M5zgN968AIw0MxGdE+kXaeFNrfkHGC2u9e4+5vASsL3YK/i7uvc/aWouwJYDoxkH17WrbS5Je97Wed6ghgJvJPxeQ2tz/C9mQNPmNkiM5sZlRW7+7qoez1Q3DOhZV1L7dzXl//no9Mp92ScPtzn2mxmY4BjgAXkyLJu0mbI0rLO9QSRSz7k7scCZwLXmNnUzJ4ejkn3+d8850o7gZ8DBwFHA+uA/+zZcLLDzIqAh4Eb3P29zH776rKOaXPWlnWuJ4hyYHTG51FR2T7H3cuj9w3AHwmHmu82HGZH7xt6LsKsaqmd++zyd/d33T3l7mngl+w+tbDPtNnM8gkbyvvd/ZGoeJ9e1nFtzuayzvUE8SJwiJmNNbNewIXA3B6OqcuZWV8z69fQDZwGLCW09bKo2mXAn3smwqxrqZ1zgU9Fv3CZAmzPOD2xV2tyfv3jhOUNoc0XmlmBmY0FDgH+2d3xvV9mZsCvgeXu/oOMXvvssm6pzVld1j19Zb6nX4RfN6wgXOH/Wk/Hk6U2jiP8muEVYFlDO4EhwJPAG8DfgcE9HWsXtPX3hMPsOsI510+31E7CL1rujJb9EqCkp+Pvwjb/LmrT4mhDMSKj/teiNr8OnNnT8XeyzR8inD5aDJRFr7P25WXdSpuztqz1qA0REYmV66eYRESkBUoQIiISSwlCRERiKUGIiEgsJQgREYmlBCHSAWaWynhqZllXPgHYzMZkPpFVpKfl9XQAInuZKnc/uqeDEOkOOoIQ6QLR/218L/rPjX+a2cFR+Rgzeyp6kNqTZnZAVF5sZn80s1ei14nRqJJm9svoef9PmFnvHmuU5DwlCJGO6d3kFNOMjH7b3X0C8FPgR1HZT4D73P0DwP3Aj6PyHwNPu/tEwn85LIvKDwHudPejgG3AJ7LcHpEW6U5qkQ4ws0p3L4opfws4xd1XRQ9UW+/uQ8xsE+HRB3VR+Tp3H2pmG4FR7l6TMY4xwN/c/ZDo81eAfHe/LfstE2lORxAiXcdb6O6ImozuFLpOKD1ICUKk68zIeH8+6n6O8JRggEuAZ6LuJ4GrAcwsaWYDuitIkfbS3olIx/Q2s7KMz4+5e8NPXQeZ2WLCUcBFUdm1wG/M7EvARuCKqPx64G4z+zThSOFqwhNZRfYYugYh0gWiaxAl7r6pp2MR6So6xSQiIrF0BCEiIrF0BCEiIrGUIEREJJYShIiIxFKCEBGRWEoQIiIS6/8Dy42ht7DzT4wAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"BCccECbm1j4m"},"source":["# model_list=[facenet, vggface, openface, deepface, deepID,arcFace ]\n","\n","# listOfDesiredModelNames=[\"facenet\", \"openface\", \"deepID\", \"arcFace\"]                # try minimal stuff  \n","model_list=[facenet, deepID ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EH0BRKeolsqk"},"source":["fine_tune_at=5\n","for model in model_list:\n","  model=model.layers[1]\n","  for layer in model.layers[:fine_tune_at]:\n","    layer.trainable =  False\n","  for layer in model.layers[fine_tune_at:]:\n","    layer.trainable =  True "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJmDiIrh39o4"},"source":["# tempImage=x_test[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o54Lgi67pbfq"},"source":["# inputs = Input(shape=(160, 160, 3))\n","# model1= vggface(inputs)\n","# model2= deepID(inputs)\n","# model3= openface(inputs)\n","# model4= deepface(inputs)\n","# model5= deepID(inputs)\n","# model6= arcFace(inputs)\n","# branches = [model1, model2,model3, model4,model5, model6]\n","# mixed = Concatenate(axis=1, name='models_Concatenate')(branches)\n","# pre_classifier= tf.keras.layers.Dense(newModel.input_shape[1])(mixed)\n","# classifier= newModel(pre_classifier)\n","\n","# concatenatedPreTrainedModels = Model(inputs, classifier, name='concatenatedPreTrainedModels')\n","\n","inputs = Input(shape=(160, 160, 3))\n","models=list()\n","for model in model_list:\n","  models.append(model(inputs))\n","branches = models\n","mixed = Concatenate(axis=1, name='models_Concatenate')(branches)\n","# pre_classifier= tf.keras.layers.Dense(newModel.input_shape[1])(mixed)\n","classifier= newModel(mixed)\n","\n","concatenatedPreTrainedModels = Model(inputs, classifier, name='concatenatedPreTrainedModels')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IwV2VkBmzOim","outputId":"29302aad-bcec-4007-9e3e-d2c02fb674d9"},"source":["concatenatedPreTrainedModels.output.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([None, 1])"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"KqlQDA0-6lqx"},"source":["model=concatenatedPreTrainedModels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QOxn8nulqDEp","outputId":"bb4237f3-e991-447a-e805-8761fe4d40b3"},"source":["learning_rate=0.000011\n","adamOptimizerWithCustomLearningRate =tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","model.compile(\n","  optimizer=adamOptimizerWithCustomLearningRate,\n","  # loss=tf.keras.losses.MeanAbsoluteError())\n","loss= tf.keras.losses.MeanAbsolutePercentageError())\n","\n","\n","  # loss=tf.keras.losses.MeanSquaredError())\n","\n","history= model.fit(\n","    x_train_all,\n","    y_train_all,\n","  validation_data=(x_test, y_test),\n","epochs=2\n",")\n","plot_loss(history)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n"," 35/157 [=====>........................] - ETA: 4:02 - loss: 5173556736.0000"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0BbDddt4qPcv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HRe3Sksp7VB"},"source":["# model..com"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"htW2SwLq8FZ0"},"source":["# def getmixedModel():\n","\n","#   return model;"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":303},"id":"tKuxhJ7znmFH","outputId":"bff843be-9e78-4e07-d2cd-4c02970e1854"},"source":["# mixedModel= getmixedModel()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-214-467f6b74e2bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmixedModel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgetmixedModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-213-9374d8782735>\u001b[0m in \u001b[0;36mgetmixedModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mbranches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mmixed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models_Concatenate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;31m# up = Conv2D(256, 1, strides=1, padding='same', use_bias=True, name= 'Block35_1_Conv2d_1x1') (mixed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# up = Lambda(scaling, output_shape=K.int_shape(up)[1:], arguments={'scale': 0.17})(up)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 970\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1106\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1108\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    876\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2623\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2625\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2626\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2627\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0mshape_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_inputs_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mreduced_inputs_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m       \u001b[0mshape_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_inputs_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"]}]},{"cell_type":"code","metadata":{"id":"jFzjMuxdno7-"},"source":[""],"execution_count":null,"outputs":[]}]}